Lambdas, streams, functional and reactive programming

in action
raoul-gabriel urma
mario fusco
alan mycroft

manning
praise for the previous edition, java 8 in action,
by raoul-gabriel urma, mario fusco, and alan mycroft.

A great and concise guide to whats new in java 8, with plenty of examples to get you
going in a hurry.
Jason lee, oracle
the best guide to java 8 that will ever be written!
William wheeler, prodata computer systems
the new streams api and lambda examples are especially useful.
Steve rogers, cgtek, inc.
A must-have to get functional with java 8.
Mayur s. Patil, mit academy of engineering
helpful as a concise, practice-oriented guide to the exciting new features of java 8.
Functional interfaces and spliterators, oh my!
Will hayworth, developer, atlassian
modern java in action
lambdas, streams, functional
and reactive programming

raoul-gabriel urma, mario fusco, and alan mycroft

No part of this publication may be reproduced, stored in a retrieval system, or transmitted, in
any form or by means electronic, mechanical, photocopying, or otherwise, without prior written
permission of the publisher.

Many of the designations used by manufacturers and sellers to distinguish their products are
claimed as trademarks. Where those designations appear in the book, and manning
publications was aware of a trademark claim, the designations have been printed in initial caps
or all caps.

Recognizing the importance of preserving what has been written, it is mannings policy to have
the books we publish printed on acid-free paper, and we exert our best efforts to that end.
Recognizing also our responsibility to conserve the resources of our planet, manning books
are printed on paper that is at least 15 percent recycled and processed without the use of
elemental chlorine.

Manning publications co. Development editor: kevin harreld
20 baldwin road technical development editor: dennis sellinger
po box 761 review editor: aleksandar dragosavljevic
shelter island, ny 11964 project manager: deirdre hiam
copy editors: heidi ward and kathy simpson
proofreader: carol shields
technical proofreader: jean-francois morin
typesetter: dennis dalinnik
cover designer: marija tudor

isbn: 9781617293566
printed in the united states of america
1 2 3 4 5 6 7 8 9 10  dp  23 22 21 20 19 18
brief contents
part 1 fundamentals ............................................................1
1  java 8, 9, 10, and 11: whats happening? 3
2  passing code with behavior parameterization 26
3  lambda expressions 42

part 2 functional-style data processing with streams ... 79
4  introducing streams 81
5  working with streams 98
6  collecting data with streams 134
7  parallel data processing and performance 172

part 3 effective programming with streams
and lambdas ............................................................199
8  collection api enhancements 201
9  refactoring, testing, and debugging 216
10  domain-specific languages using lambdas 239

v
vi brief contents

part 4 everyday java .........................................................273
11  using optional as a better alternative to null 275
12  new date and time api 297
13  default methods 314
14  the java module system 333

part 5 enhanced java concurrency ................................355
15  concepts behind completablefuture and
reactive programming 357
16  completablefuture: composable asynchronous
programming 387
17  reactive programming 416

part 6 functional programming and future java
evolution ...............................................................443
18  thinking functionally 445
19  functional programming techniques 460
20  blending oop and fp: comparing java and scala 485
21  conclusions and where next for java 500
contents
preface xix
acknowledgments xxi
about this book xxiii
about the authors xxviii
about the cover illustration xxx

part 1 fundamentals ..................................................1

1 java 8, 9, 10, and 11: whats happening? 3
1.1 so, whats the big story? 3
1.2 why is java still changing? 6
javas place in the programming language ecosystem 6
stream processing 8 passing code to methods with behavior


parameterization 9 parallelism and shared mutable data 10


java needs to evolve 11
1.3 functions in java 12
methods and lambdas as first-class citizens 12 passing code: an


example 14 from passing methods to lambdas 16


1.4 streams 17
multithreading is difficult 19

vii
viii contents

1.5 default methods and java modules 21
1.6 other good ideas from functional programming 23

2 passing code with behavior parameterization 26
2.1 coping with changing requirements 27
first attempt: filtering green apples 28 second attempt: 

parameterizing the color 28 third attempt: filtering with


every attribute you can think of 29
2.2 behavior parameterization 30
fourth attempt: filtering by abstract criteria 31
2.3 tackling verbosity 35
anonymous classes 36 fifth attempt: using an anonymous


class 36 sixth attempt: using a lambda expression 37


seventh attempt: abstracting over list type 38
2.4 real-world examples 39
sorting with a comparator 39 executing a block of code 

with runnable 40 returning a result using callable 40


gui event handling 41

3 lambda expressions 42
3.1 lambdas in a nutshell 43
3.2 where and how to use lambdas 46
functional interface 46 
function descriptor 48
3.3 putting lambdas into practice: the execute-around pattern 50
step 1: remember behavior parameterization 51 step 2: use 

a functional interface to pass behaviors 51 step 3: execute 

a behavior! 52 step 4: pass lambdas 52


3.4 using functional interfaces 53
predicate 54 
consumer 54 
function 55
3.5 type checking, type inference, and restrictions 59
type checking 59 same lambda, different functional


interfaces 61 type inference 63 using local variables 63
 

3.6 method references 64
in a nutshell 65 
constructor references 68
3.7 putting lambdas and method references into practice 70
step 1: pass code 71 step 2: use an anonymous class 71


step 3: use lambda expressions 71 step 4: use method 

references 72
contents ix

3.8 useful methods to compose lambda expressions 72
composing comparators 73  composing predicates 73
composing functions 74
3.9 similar ideas from mathematics 76
integration 76  connecting to java 8 lambdas 77

part 2 functional-style data processing
with streams ..................................................79

4 introducing streams 81
4.1 what are streams? 82
4.2 getting started with streams 86
4.3 streams vs. Collections 88
traversable only once 90 
external vs. Internal iteration 91
4.4 stream operations 93
intermediate operations 94 
terminal operations 95
working with streams 95
4.5 road map 96

5 working with streams 98
5.1 filtering 99
filtering with a predicate 99  filtering unique elements 100
5.2 slicing a stream 100
slicing using a predicate 101  truncating a stream 102
skipping elements 103
5.3 mapping 104
applying a function to each element of a stream 104
flattening streams 105
5.4 finding and matching 108
checking to see if a predicate matches at least one element 108
checking to see if a predicate matches all elements 109
finding an element 109 finding the first element 110


5.5 reducing 111
summing the elements 111 
maximum and minimum 113
5.6 putting it all into practice 117
the domain: traders and transactions 117 
solutions 118
x contents

5.7 numeric streams 121
primitive stream specializations 121 numeric ranges 123


putting numerical streams into practice: pythagorean triples 123
5.8 building streams 126
streams from values 126 stream from nullable 126


streams from arrays 127 streams from files 127


streams from functions: creating infinite streams! 128
5.9 overview 132

6 collecting data with streams 134
6.1 collectors in a nutshell 136
collectors as advanced reductions 136 
predefined
collectors 137
6.2 reducing and summarizing 138
finding maximum and minimum in a stream of values 138
summarization 139 joining strings 140 generalized
 

summarization with reduction 141
6.3 grouping 146
manipulating grouped elements 147 
multilevel grouping 149
collecting data in subgroups 150
6.4 partitioning 154
advantages of partitioning 155  partitioning numbers into
prime and nonprime 156
6.5 the collector interface 159
making sense of the methods declared by collector interface 160
putting them all together 163
6.6 developing your own collector for better
performance 165
divide only by prime numbers 166  comparing collectors
performances 170

7 parallel data processing and performance 172
7.1 parallel streams 173
turning a sequential stream into a parallel one 174
measuring stream performance 176 using parallel streams


correctly 180 using parallel streams effectively 182

contents xi

7.2 the fork/join framework 184
working with recursivetask 184 best practices for using the

fork/join framework 188 work stealing 189


7.3 spliterator 190
the splitting process 191  implementing your own
spliterator 192

part 3 effective programming with streams
and lambdas..................................................199

8 collection api enhancements 201
8.1 collection factories 202
list factory 203 
set factory 204 
map factories 204
8.2 working with list and set 205
removeif 205 
replaceall 206
8.3 working with map 207
foreach 207 sorting 208 getordefault 208
 

compute patterns 209 remove patterns 210


replacement patterns 211 merge 211 

8.4 improved concurrenthashmap 213
reduce and search 213  counting 214  set views 214

9 refactoring, testing, and debugging 216
9.1 refactoring for improved readability and flexibility 217
improving code readability 217 from anonymous classes to


lambda expressions 217 from lambda expressions to method


references 219 from imperative data processing to streams 220


improving code flexibility 221
9.2 refactoring object-oriented design patterns
with lambdas 223
strategy 224 template method 225 observer 226
 

chain of responsibility 229 factory 230 

9.3 testing lambdas 232
testing the behavior of a visible lambda 232 focusing on the 

behavior of the method using a lambda 233 pulling complex 

lambdas into separate methods 234 testing high-order 

functions 234
xii contents

9.4 debugging 234
examining the stack trace 235  logging information 236

10 domain-specific languages using lambdas 239
10.1 a specific language for your domain 241
pros and cons of dsls 242 
different dsl solutions available
on the jvm 244
10.2 small dsls in modern java apis 248
the stream api seen as a dsl to manipulate collections 249
collectors as a dsl to aggregate data 250
10.3 patterns and techniques to create dsls in java 252
method chaining 255 using nested functions 257


function sequencing with lambda expressions 259
putting it all together 261 using method references


in a dsl 263
10.4 real world java 8 dsl 266
jooq 266 
cucumber 267 
spring integration 269

part 4 everyday java ...............................................273

11 using optional as a better alternative to null 275
11.1 how do you model the absence of a value? 276
reducing nullpointerexceptions with defensive checking 277
problems with null 278 what are the alternatives to null in


other languages? 279
11.2 introducing the optional class 280
11.3 patterns for adopting optionals 281
creating optional objects 281 extracting and transforming


values from optionals with map 282 chaining optional objects


with flatmap 283 manipulating a stream of optionals 287


default actions and unwrapping an optional 288 combining 

two optionals 289 rejecting certain values with filter 290


11.4 practical examples of using optional 292
wrapping a potentially null value in an optional 292
exceptions vs. Optional 293 primitive optionals and why


you shouldnt use them 294 putting it all together 294

contents xiii

12 new date and time api 297
12.1 localdate, localtime, localdatetime, instant, duration,
and period 298
working with localdate and localtime 299 combining a date 

and a time 300 instant: a date and time for machines 301


defining a duration or a period 301
12.2 manipulating, parsing, and formatting dates 303
working with temporaladjusters 305 
printing and parsing
date-time objects 308
12.3 working with different time zones and calendars 310
using time zones 310 fixed offset from utc/greenwich
 311
using alternative calendar systems 311

13 default methods 314
13.1 evolving apis 317
api version 1 317 
api version 2 318
13.2 default methods in a nutshell 320
13.3 usage patterns for default methods 322
optional methods 322 
multiple inheritance of behavior 323
13.4 resolution rules 326
three resolution rules to know 327 most specific default-


providing interface wins 327 conflicts and explicit


disambiguation 329 diamond problem 330


14 the java module system 333
14.1 the driving force: reasoning about software 334
separation of concerns 334 
information hiding 334
java software 335
14.2 why the java module system was designed 336
modularity limitations 336 
monolithic jdk 337
comparison with osgi 338
14.3 java modules: the big picture 339
14.4 developing an application with the java module
system 340
setting up an application 340 fine-grained and coarse-grained


modularization 342 java module system basics 342

xiv contents

14.5 working with several modules 343
the exports clause 344  the requires clause 344
naming 345
14.6 compiling and packaging 345
14.7 automatic modules 349
14.8 module declaration and clauses 350
requires 350 exports 350 requires transitive 351
 

exports to 351 open and opens 351 uses and
 

provides 352
14.9 a bigger example and where to learn more 352

part 5 enhanced java concurrency ......................355

15 concepts behind completablefuture and
reactive programming 357
15.1 evolving java support for expressing concurrency 360
threads and higher-level abstractions 361 executors and thread


pools 362 other abstractions of threads: non-nested with method


calls 364 what do you want from threads? 366


15.2 synchronous and asynchronous apis 366
future-style api 368 reactive-style api 369 sleeping
 

(and other blocking operations) considered harmful 370
reality check 372 how do exceptions work with


asynchronous apis? 372
15.3 the box-and-channel model 373
15.4 completablefuture and combinators for
concurrency 375
15.5 publish-subscribe and reactive programming 378
example use for summing two flows 380  backpressure 384
a simple form of real backpressure 384
15.6 reactive systems vs. Reactive programming 385
15.7 road map 386

16 completablefuture: composable asynchronous
programming 387
16.1 simple use of futures 388
understanding futures and their limitations 389 using 

completablefutures to build an asynchronous application 390
contents xv

16.2 implementing an asynchronous api 391
converting a synchronous method into an asynchronous one 392
dealing with errors 394
16.3 making your code nonblocking 396
parallelizing requests using a parallel stream 397
making asynchronous requests with completablefutures 397
looking for the solution that scales better 399 using a custom


executor 400
16.4 pipelining asynchronous tasks 402
implementing a discount service 403 using the discount


service 404 composing synchronous and asynchronous


operations 405 combining two completablefutures:


dependent and independent 408 reflecting on future vs.


Completablefuture 409 using timeouts effectively 410


16.5 reacting to a completablefuture completion 411
refactoring the best-price-finder application 412
putting it all together 414
16.6 road map 414

17 reactive programming 416
17.1 the reactive manifesto 417
reactive at application level 418 
reactive at system level 420
17.2 reactive streams and the flow api 421
introducing the flow class 421 creating your first reactive


application 424 transforming data with a processor 429


why doesnt java provide an implementation of the flow api? 431
17.3 using the reactive library rxjava 431
creating and using an observable 433  transforming and
combining observables 437

part 6 functional programming and future
java evolution .............................................443

18 thinking functionally 445
18.1 implementing and maintaining systems 446
shared mutable data 446 declarative programming
 447
why functional programming? 448
xvi contents

18.2 whats functional programming? 449
functional-style java 450 referential transparency 452


object-oriented vs. Functional-style programming 452
functional style in practice 453
18.3 recursion vs. Iteration 455

19 functional programming techniques 460
19.1 functions everywhere 461
higher-order functions 461 
currying 463
19.2 persistent data structures 464
destructive updates vs. Functional 464 another example with

trees 467 using a functional approach 468


19.3 lazy evaluation with streams 469
self-defining stream 470 
your own lazy list 472
19.4 pattern matching 476
visitor design pattern 477 
pattern matching to the rescue 478
19.5 miscellany 481
caching or memoization 481 what does return the same


object mean? 482 combinators 483


20 blending oop and fp: comparing java and scala 485
20.1 introduction to scala 486
hello beer 486 basic data structures: list, set, map, tuple,


stream, option 488
20.2 functions 493
first-class functions in scala 493 anonymous functions


and closures 494 currying 496


20.3 classes and traits 497
less verbosity with scala classes 497  scala traits vs. Java
interfaces 498

21 conclusions and where next for java 500
21.1 review of java 8 features 501
behavior parameterization (lambdas and method references) 501
streams 502 completablefuture 502 optional 503
 

flow api 503 default methods 504


21.2 the java 9 module system 504
21.3 java 10 local variable type inference 505
contents xvii

21.4 whats ahead for java? 507
declaration-site variance 507 pattern matching
 507
richer forms of generics 508 deeper support for


immutability 510 value types 511


21.5 moving java forward faster 514
21.6 the final word 515

appendix a miscellaneous language updates 517
appendix b miscellaneous library updates 521
appendix c performing multiple operations in parallel on a stream 529
appendix d lambdas and jvm bytecode 538
index 543
preface
back in 1998 when i was eight years old, i picked up my first book on computingon
javascript and html. Little did i know that opening that book would transform my
life by exposing me to programming languages and the amazing things i could do
with them. I was hooked. Every so often, i still find a new programming language fea-
ture that revives this excitement because it enables me to write clearer, more concise
code in half the time. I hope that the new ideas in java 8, java 9, and java 10, incorpo-
rated from functional programming and explored in this book, inspire you in the
same way.
So, you may wonder, how did this bookand its second editioncome about?
Well, back in 2011, brian goetz (the java language architect at oracle) was shar-
ing various proposals to add lambda expressions to java, with the aim of getting the
community involved. These rekindled my excitement, and i started to evangelize the
ideas, organizing java 8 workshops at various developer conferences and giving lec-
tures to students at the university of cambridge.
By april 2013, word had spread, and our publisher at manning emailed asking
whether i was interested in writing a book about lambdas in java 8. At the time i was a
humble second-year phd candidate, and that seemed to be a bad idea because it
would interfere with my thesis submission schedule. On the other hand, carpe diem. I
thought writing a short book shouldnt be too much work, right? (it was only later that
i realized i was utterly wrong! ) so, i sought advice from my phd supervisor, professor
alan mycroft, who, it turned out, was ready to support me in this adventure (even
offering to help in such non-phd workim forever in his debt). A few days later, we

xix
xx preface

met fellow java 8 evangelist mario fusco, who had vast professional experience and
had become well known at major developer conferences for his talks on functional
programming.
We quickly realized that by combining our energy and diverse backgrounds we
could deliver, not just a short book on java 8 lambdas, but instead a book that, we
hope, the java community will still be reading in five or ten years. We had a unique
opportunity to discuss many topics in depth that will benefit java programmers and
open doors to a new universe of ideas: functional programming.
Now, its 2018, and we find that the first edition amazingly sold 20,000 copies, java 9
has just been released, java 10 is about to be released, and time has dulled the mem-
ory of many long nights of editing. So, here it isthe second edition modern java in
action, covering java 8, java 9, and java 10. We hope you will enjoy it!

Raoul-gabriel urma
cambridge spark
acknowledgments
this book would not have been possible without the support of many amazing people:
 personal friends and people who provided valuable reviews and suggestions on
a volunteer basis: richard walker, jan saganowski, brian goetz, stuart marks,
cem redif, paul sandoz, stephen colebourne, inigo mediavilla, allahbaksh
asadullah, tomasz nurkiewicz, and michael muller
 our manning early access program (meap) readers who posted comments in
the author online forum
 the reviewers from the development process who provided helpful feedback:
antonio magnaghi, brent stains, franziska meyer, furkan kamachi, jason lee,
jorn dinkla, lochana menikarachchi, mayur patil, nikolaos kaintantzis, sim-
one bordet, steve rogers, will hayworth, and william wheeler
 kevin harreld, our development editor at manning, who was very patient in
answering all our questions and concerns, provided detailed feedback for each
of our draft chapters, and supported us in all possible ways
 dennis selinger and jean-francois morin, who provided a thorough technical
proofread of the manuscript shortly before it went to production; and al
scherer, who provided technical expertise during development
raoul-gabriel urma
first and foremost, id like to thank my parents for their endless love and support in
my life. This little dream of writing a book has now come true! Next, i would like to
express my eternal gratitude to alan mycroft, my phd supervisor and coauthor, for his

xxi
xxii acknowledgments

trust and support. Id also like to thank my coauthor mario fusco for sharing this fun
journey. Finally, id like to thank friends who have provided mentorship, useful
advice, and encouragement in my life: sophia drossopoulou, aidan roche, alex
buckley, haadi jabado, and jaspar robertson. You guys rock!
Mario fusco
id like to especially thank my wife, marilena, whose boundless patience allowed me to
stay focused on the book, and our daughter, sofia, because the infinite chaos she can
produce allowed me to get creatively distracted from the book. As youll discover read-
ing the book, sofia also taught us, like only a two-year-old baby girl can, the difference
between internal and external iteration. Id like to also thank raoul-gabriel urma
and alan mycroft, with whom i shared the (big) joys and the (small) pains of this writ-
ing experience.
Alan mycroft
id like to thank my wife, hilary, and the rest of my family for enduring the many
hours that just a bit more work to do on the book consumed. I also thank my col-
leagues and students over the years for teaching me how to teach, mario and raoul
for being such efficient coauthors, and particularly raoul for his skill at being so pleas-
antly demanding when requiring the next bit of text by friday.
About this book
put simply, the new features in java 8 along with the (less-obvious) changes in java 9
are the biggest change to java in the 21 years since java 1.0 was released. Nothing has
been taken away, so all your existing java code continues to workbut the new fea-
tures provide powerful new idioms and design patterns to help you write clearer, more
concise code. At first you might think (as with all new features), why are they chang-
ing my language again? But then, after a bit of practice, comes the revelation that
youve just used the features to write shorter, clearer code in half the time you
expectedand you realize you could never go back to old java again.
The second edition of this book, modern java in action: lambdas, streams, functional
and reactive programming, is written to get you over that initial hump of sounds good
in principle, but its all a bit new and unfamiliar and into coding like a native.
Perhaps, you might think, but lambdas, functional programmingarent those
the sort of things that bearded sandal-wearing academics talk about in their ivory tow-
ers? They might be, but java 8 has incorporated just the right balance of ideas into
java to gain many of their advantages in a way thats comprehensible to ordinary java
programmers. And this book tells the story from the ordinary-programmer viewpoint,
with an occasional how this arose for perspective.
Lambdasthat sounds greek to me! Yes, it does, but its a great idea for enabling
you to write concise java programs. Many of you are familiar with event handlers and
callbacks, where you register an object containing a method to be used when some
event happens. Lambdas make this sort of idea much more widely usable in java. Put
simply, lambdas and their friends, method references, provide the ability to concisely

xxiii
xxiv about this book

pass code or methods as arguments to be executed in the middle of doing something
else. Youll see in this book how this idea occurs more frequently than you might
think: from simply parameterizing a sort method with code to do the comparison to
expressing complex queries on collections of data using the new streams api.
Streamswhat are they? Theyre a great new java 8 addition. They behave like
collections but have several advantages that enable new styles of programming. First,
if youve ever programmed using a database-query language such as sql, youll rec-
ognize that it enables queries to be written in a few lines that would take many lines
in java. Java 8 streams support this concise database-queries style of programming
but with java syntax and none of the need to know about databases! Second, streams
are designed so that not all their data needs to be in memory (or even computed) at
once. Thus, you can process streams that are too big to fit in your computer mem-
ory. But java 8 can optimize operations on streams in a way that java cant do for col-
lectionsfor example, it can group together several operations on the same stream
so that the data is traversed only once instead of expensively traversing it multiple
times. Even better, java can automatically parallelize stream operations for you
(unlike collections).
And functional-style programming, whats that? Its another style of program-
ming, just like object-oriented programming, but centered on using functions as val-
ues, just as we mentioned previously when discussing lambdas.
Whats great about java 8 is that it incorporates many of the best ideas from func-
tional programming into the familiar java syntax. The fine design choices enable you
to see functional-style programming in java 8 as an additional set of design patterns
and idioms to enable you to write clearer, more concise code in less time. Think of it
as having a wider range of weapons in your programming armory.
Oh yes, in addition to these features that lean on big conceptual additions to java, we
also explain the many other useful java 8 features and updates such as default methods,
the new optional class, completablefuture, and the new date and time api.
And there are the java 9 additions: a new module system, support for reactive pro-
gramming via the flow api, and various other enhancements.
But hey, this is an overview, and its time now for us to leave you to read the book.

How this book is organized: a roadmap
modern java in action is divided into six parts: fundamentals, functional-style data
processing with streams, effective programming with streams and lambdas, every-
day java, enhanced java concurrency, and functional programming and future
java evolution. While we strongly recommend that you read the chapters in the first
two parts first (and in order because many of the concepts presented build on previ-
ous chapters), the remaining four parts can be read reasonably independently. Most
chapters include several quizzes to help you work through the material.
The first part of the book provides the fundamentals to help you get started with
the new java ideas introduced in java 8. By the end of this first part, youll have a full
about this book xxv

understanding of what lambda expressions are, and youll be able to write code thats
both concise and flexible enough to easily adapt to changing requirements.
 In chapter 1, we summarize the main changes to java (lambda expressions,
method references, streams, and default methods) and set the scene for the
book.
 In chapter 2, youll learn about behavior parameterization, a software-
development pattern that java 8 relies heavily on and is the motivation for
lambda expressions.
 Chapter 3 gives a full explanation, with code examples and quizzes at every
step, of the concepts of lambda expressions and method references.
The second part of this book is a deep exploration of the new streams api, which
lets you write powerful code that processes a collection of data in a declarative way.
By the end of this second part, youll have a full understanding of what streams are
and how you can use them in your codebase to process a collection of data concisely
and efficiently.
 Chapter 4 introduces the concept of a stream and explains how it compares
with a collection.
 Chapter 5 investigates in detail the stream operations available to express
sophisticated data-processing queries. Youll look at many patterns such as fil-
tering, slicing, finding, matching, mapping, and reducing.
 Chapter 6 covers collectorsa feature of the streams api that lets you express
even more complex data-processing queries.
 In chapter 7, youll learn about how streams can automatically run in parallel
and leverage your multicore architectures. In addition, youll learn about vari-
ous pitfalls to avoid when using parallel streams correctly and effectively.
The third part of this book explores various java 8 and java 9 topics that will make you
more effective at using java and will enhance your codebase with modern idioms.
Because it is oriented toward more-advanced programming ideas we have arranged,
nothing later in the book depends on the techniques described here.
 Chapter 8 is a new chapter for the second edition and explores the collection
api enhancements of java 8 and java 9. It covers using collection factories and
learning new idiomatic patterns to work with list and set collections along with
idiomatic patterns involving map.
 Chapter 9 explores how you can improve your existing code using new java 8
features and a few recipes. In addition, it explores vital software-development
techniques such as design patterns, refactoring, testing, and debugging.
 Chapter 10 is also new for the second edition. It explores the idea of basing an api
on a domain-specific language (dsl). This is not only a powerful way of design-
ing apis but one which is both becoming increasingly popular and is already
appearing in the java classes such as comparators, stream, and collectors.
Xxvi about this book

the fourth part of this book explores various new features in java 8 and java 9 cen-
tered around making it easier and more reliable to code your projects. We start with
two apis introduced in java 8.
 Chapter 11 covers the java. Util. Optional class, which allows you to both design
better apis and reduce null pointer exceptions.
 Chapter 12 explores the date and time api, which greatly improves the previ-
ous error-prone apis for working with dates and time.
 In chapter 13, youll learn what default methods are, how you can use them to
evolve apis in a compatible way, some practical usage patterns, and rules for
using default methods effectively.
 Chapter 14 is new for this second edition and explores the java module sys-
tema major enhancement in java 9 that enables huge systems to be modular-
ized in a documented and enforceable way, rather than being just a haphazard
collection of packages.
The fifth part of this book explores the more advanced ways of structuring concurrent
programs in javabeyond the ideas of easy-to-use parallel processing for streams
introduced in chapters 6 and 7. Chapter 15 is new to this second edition and covers
the big-picture idea of asynchronous apisincluding the ideas of futures and the
publish-subscribe protocol behind reactive programming and encapsulated in the
java 9 flow api.
 Chapter 16 explores completablefuture, which lets you express complex asyn-
chronous computations in a declarative wayparalleling the design of the
streams api.
 Chapter 17 is again new to this second edition and explores the java 9 flow api
in detail, focusing on practical reactive programming code.
In the sixth and final part of this book, we draw back a little with a tutorial introduc-
tion to writing effective functional-style programs in java, along with a comparison of
java 8 features with those of scala.
 Chapter 18 gives a full tutorial on functional programming, introduces some of
its terminology, and explains how to write functional-style programs in java.
 Chapter 19 covers more advanced functional programming techniques includ-
ing higher-order functions, currying persistent data structures, lazy lists, and
pattern matching. You can view this chapter as a mix of practical techniques to
apply in your codebase as well as academic information that will make you a
more knowledgeable programmer.
 Chapter 20 follows by discussing how java 8 features compare to features in the
scala languagea language that, like java, is implemented on top of the jvm
and that has evolved quickly to threaten some aspects of javas niche in the pro-
gramming language ecosystem.
About this book xxvii

 in chapter 21, we review the journey of learning about java 8 and the gentle
push toward functional-style programming. In addition, we speculate on what
future enhancements and great new features may be in javas pipeline beyond
java 8, java 9, and the small additions in java 10.
Finally, there are four appendixes, which cover a number of other topics related to
java 8. Appendix a summarizes minor java 8 language features that we didnt discuss
in the book. Appendix b gives an overview of other main additions to the java library
that you may find useful. Appendix c is a continuation of part 2 and looks at advanced
uses of streams. Appendix d explores how the java compiler implements lambda
expressions behind the scenes.

About the code
all source code in listings or in text is in a fixed-width font like this to separate it
from ordinary text. Code annotations accompany many of the listings, highlighting
important concepts.
Source code for all the working examples in the book and instructions to run
them are available on a github repository and as a download via the books website.
Both links to the source code may be found at www. Manning. Com/books/modern-
java-in-action.

Book forum
purchase of modern java in action includes free access to a private web forum run by
manning publications where you can make comments about the book, ask technical
questions, and receive help from the authors and from other users. To access the
forum, go to https: //forums. Manning. Com/forums/modern-java-in-action. You can
also learn more about mannings forums and the rules of conduct at https: //forums
. Manning. Com/forums/about.
Mannings commitment to our readers is to provide a venue where a meaningful
dialogue between individual readers and between readers and the authors can take
place. It is not a commitment to any specific amount of participation on the part of
the authors, whose contribution to the forum remains voluntary (and unpaid). We
suggest you try asking the authors some challenging questions lest their interest stray!
The forum and the archives of previous discussions will be accessible from the pub-
lishers website as long as the book is in print.
About the authors
raoul-gabriel urma is ceo and co-founder of cambridge
spark, a leading learning community for data scientists and
developers in the uk. Raoul was nominated a java champion in
2017. He has worked for google, ebay, oracle, and goldman
sachs. Raoul completed a phd in computer science at the uni-
versity of cambridge. In addition, he holds a meng in com-
puter science from imperial college london and graduated
with first-class honors, having won several prizes for technical
innovation. Raoul has delivered over 100 technical talks at inter-
national conferences.

Mario fusco is a senior software engineer at red hat working
on the core development of drools, the jboss rule engine. He
has vast experience as a java developer, having been involved in
(and often leading) many enterprise-level projects in several
industries ranging from media companies to the financial sec-
tor. Among his interests are functional programming and
domain-specific languages. By leveraging these two passions, he
created the open source library lambdaj with the goal of provid-
ing an internal java dsl for manipulating collections and allow-
ing a bit of functional programming in java.

Xxviii
about the authors xxix

alan mycroft is professor of computing in the computer lab-
oratory of cambridge university, where he has been a faculty
member since 1984. Hes also a fellow at robinson college, a
co-founder of the european association for programming
languages and systems, and a co-founder and trustee of the
raspberry pi foundation. He has degrees in mathematics (cam-
bridge) and computer science (edinburgh). Hes the author
of about 100 research papers and has supervised more than 20
phd theses. His research centers on programming languages
and their semantics, optimization, and implementation. He
maintains strong industrial links, having worked at at&t laboratories and intel
research during academic leave, as well as spinning out codemist ltd. , which built
the original arm c compiler under the norcroft name.
About the cover illustration
the figure on the cover of java in action is captioned habit of a mandarin of war
in chinese tartary in 1700. The mandarins habit is ornately decorated, and he is
carrying a sword and a bow and quiver on his back. If you look carefully at his belt,
you will find a lambda buckle (added by our designer as a wink at one of the topics
of this book). The illustration is taken from thomas jefferys a collection of the
dresses of different nations, ancient and modern, london, published between 1757 and
1772. The title page states that these are hand-colored copperplate engravings,
heightened with gum arabic. Thomas jefferys (17191771) was called geographer
to king george iii. He was an english cartographer who was the leading map sup-
plier of his day. He engraved and printed maps for government and other official
bodies and produced a wide range of commercial maps and atlases, especially of
north america. His work as a mapmaker sparked an interest in local dress customs
of the lands he surveyed and mapped; they are brilliantly displayed in this four-vol-
ume collection.
Fascination with faraway lands and travel for pleasure were relatively new phe-
nomena in the eighteenth century, and collections such as this one were popular,
introducing both the tourist as well as the armchair traveler to the inhabitants of
other countries. The diversity of the drawings in jefferys volumes speaks vividly of
the uniqueness and individuality of the worlds nations centuries ago. Dress codes
have changed, and the diversity by region and country, so rich at one time, has
faded away. It is now often hard to tell the inhabitant of one continent from
another. Perhaps, trying to view it optimistically, we have traded a cultural and visual

xxx
about the cover illustration xxxi

diversity for a more varied personal lifeor a more varied and interesting intellec-
tual and technical life.
At a time when it is hard to tell one computer book from another, manning cele-
brates the inventiveness and initiative of the computer business with book covers
based on the rich diversity of national costumes three centuries ago, brought back to
life by jefferys pictures.
Part 1

fundamentals

t his first part of the book provides the fundamentals to help you get started
with the new java ideas introduced in java 8. By the end of this first part, youll
have a full understanding of what lambda expressions are, and youll be able to
write code thats both concise and flexible enough to easily adapt to changing
requirements.
In chapter 1, we summarize the main changes to java (lambda expressions,
method references, streams, and default methods) and set the scene for the book.
In chapter 2, youll learn about behavior parameterization, a software devel-
opment pattern that java 8 relies heavily on and is the motivation for lambda
expressions.
Chapter 3 gives a full explanation, with code examples and quizzes at every
step, of the concepts of lambda expressions and method references.
Java 8, 9, 10, and 11:
whats happening?

This chapter covers
 why java keeps changing
 changing computing background
 pressures for java to evolve
 introducing new core features of java 8 and 9

since the release of java development kit (jdk 1.0) in 1996, java has won a large
following of students, project managers, and programmers who are active users. Its
an expressive language and continues to be used for projects both large and small.
Its evolution (via the addition of new features) from java 1.1 (1997) to java 7
(2011) has been well managed. Java 8 was released in march 2014, java 9 in sep-
tember 2017, java 10 in march 2018, and java 11 planned for september 2018. The
question is this: why should you care about these changes?

1.1 so, whats the big story?
We argue that the changes to java 8 were in many ways more profound than any
other changes to java in its history (java 9 adds important, but less-profound,
productivity changes, as youll see later in this chapter, while java 10 makes much

3
4 chapter 1 java 8, 9, 10, and 11: whats happening?

Smaller adjustments to type inference). The good news is that the changes enable you
to write programs more easily. For example, instead of writing verbose code (to sort a
list of apples in inventory based on their weight) like
collections. Sort(inventory, new comparator<apple>() {
public int compare(apple a1, apple a2){
return a1. Getweight(). Compareto(a2. Getweight());
}
});

in java 8 you can write more concise code that reads a lot closer to the problem state-
ment, like the following:
the first java 8 code
inventory. Sort(comparing(apple: : getweight)); of the book!

It reads sort inventory comparing apple weight. Dont worry about this code for
now. This book will explain what it does and how you can write similar code.
Theres also a hardware influence: commodity cpus have become multicorethe
processor in your laptop or desktop machine probably contains four or more cpu
cores. But the vast majority of existing java programs use only one of these cores and
leave the other three idle (or spend a small fraction of their processing power run-
ning part of the operating system or a virus checker).
Prior to java 8, experts might tell you that you have to use threads to use these
cores. The problem is that working with threads is difficult and error-prone. Java has
followed an evolutionary path of continually trying to make concurrency easier and
less error-prone. Java 1.0 had threads and locks and even a memory modelthe best
practice at the timebut these primitives proved too difficult to use reliably in non-
specialist project teams. Java 5 added industrial-strength building blocks like thread
pools and concurrent collections. Java 7 added the fork/join framework, making par-
allelism more practical but still difficult. Java 8 gave us a new, simpler way of thinking
about parallelism. But you still have to follow some rules, which youll learn in this
book.
As youll see later in this book, java 9 adds a further structuring method for con-
currencyreactive programming. Although this has more-specialist use, it standard-
izes a means of exploiting the rxjava and akka reactive streams toolkits that are
becoming popular for highly concurrent systems.
From the previous two desiderata (more concise code and simpler use of multi-
core processors) springs the whole consistent edifice captured by java 8. We start by
giving you a quick taste of these ideas (hopefully enough to intrigue you, but short
enough to summarize them):
 the streams api
 techniques for passing code to methods
 default methods in interfaces
so, whats the big story? 5

java 8 provides a new api (called streams) that supports many parallel operations to
process data and resembles the way you might think in database query languages
you express what you want in a higher-level manner, and the implementation (here
the streams library) chooses the best low-level execution mechanism. As a result, it
avoids the need for you to write code that uses synchronized, which is not only highly
error-prone but also more expensive than you may realize on multicore cpus.1
from a slightly revisionist viewpoint, the addition of streams in java 8 can be seen
as a direct cause of the two other additions to java 8: concise techniques to pass code to
methods (method references, lambdas) and default methods in interfaces.
But thinking of passing code to methods as a mere consequence of streams down-
plays its range of uses within java 8. It gives you a new concise way to express behavior
parameterization. Suppose you want to write two methods that differ in only a few lines
of code. You can now simply pass the code of the parts that differ as an argument (this
programming technique is shorter, clearer, and less error-prone than the common
tendency to use copy and paste). Experts will here note that behavior parameteriza-
tion could, prior to java 8, be encoded using anonymous classesbut well let the
example at the beginning of this chapter, which shows increased code conciseness
with java 8, speak for itself in terms of clarity.
The java 8 feature of passing code to methods (and being able to return it and
incorporate it into data structures) also provides access to a range of additional tech-
niques that are commonly referred to as functional-style programming. In a nutshell,
such code, called functions in the functional programming community, can be passed
around and combined in a way to produce powerful programming idioms that youll
see in java guise throughout this book.
The meat of this chapter begins with a high-level discussion on why languages
evolve, continues with sections on the core features of java 8, and then introduces the
ideas of functional-style programming that the new features simplify using and that
new computer architectures favor. In essence, section 1.2 discusses the evolution pro-
cess and the concepts, which java was previously lacking, to exploit multicore parallel-
ism in an easy way. Section 1.3 explains why passing code to methods in java 8 is such
a powerful new programming idiom, and section 1.4 does the same for streamsthe
new java 8 way of representing sequenced data and indicating whether these can be
processed in parallel. Section 1.5 explains how the new java 8 feature of default meth-
ods enables interfaces and their libraries to evolve with less fuss and less recompila-
tion; it also explains the modules addition to java 9, which enables components of large
java systems to be specified more clearly than just a jar file of packages. Finally, sec-
tion 1.6 looks ahead at the ideas of functional-style programming in java and other
languages sharing the jvm. In summary, this chapter introduces ideas that are succes-
sively elaborated in the rest of the book. Enjoy the ride!

1
multicore cpus have separate caches (fast memory) attached to each processor core. Locking requires these
to be synchronized, requiring relatively slow cache-coherency-protocol inter-core communication.
6 chapter 1 java 8, 9, 10, and 11: whats happening?

1.2 why is java still changing?
With the 1960s came the quest for the perfect programming language. Peter landin,
a famous computer scientist of his day, noted in 1966 in a landmark article2 that there
had already been 700 programming languages and speculated on what the next 700
would be likeincluding arguments for functional-style programming similar to that
in java 8.
Many thousands of programming languages later, academics have concluded that
programming languages behave like ecosystems: new languages appear, and old lan-
guages are supplanted unless they evolve. We all hope for a perfect universal language,
but in reality certain languages are better fitted for certain niches. For example, c and
c++ remain popular for building operating systems and various other embedded sys-
tems because of their small runtime footprint and in spite of their lack of program-
ming safety. This lack of safety can lead to programs crashing unpredictably and
exposing security holes for viruses and the like; indeed, type-safe languages such as
java and c# have supplanted c and c++ in various applications when the additional
runtime footprint is acceptable.
Prior occupancy of a niche tends to discourage competitors. Changing to a new
language and tool chain is often too painful for just a single feature, but newcomers
will eventually displace existing languages, unless they evolve fast enough to keep up.
(older readers are often able to quote a range of such languages in which theyve pre-
viously coded but whose popularity has since wanedada, algol, cobol, pascal,
delphi, and snobol, to name but a few. )
youre a java programmer, and java has been successful at colonizing (and displac-
ing competitor languages in) a large ecosystem niche of programming tasks for nearly
20 years. Lets examine some reasons for that.

1.2.1 javas place in the programming language ecosystem
java started well. Right from the start, it was a well-designed object-oriented language
with many useful libraries. It also supported small-scale concurrency from day one
with its integrated support for threads and locks (and with its early prescient acknowl-
edgment, in the form of a hardware-neutral memory model, that concurrent threads
on multicore processors can have unexpected behaviors in addition to those that hap-
pen on single-core processors). Also, the decision to compile java to jvm bytecode (a
virtual machine code that soon every browser supported) meant that it became the
language of choice for internet applet programs (do you remember applets? ).
Indeed, theres a danger that the java virtual machine (jvm) and its bytecode will be
seen as more important than the java language itself and that, for certain applications,
java might be replaced by one of its competing languages such as scala, groovy, or
kotlin, which also run on the jvm. Various recent updates to the jvm (for example,
the new invokedynamic bytecode in jdk7) aim to help such competitor languages

2
p. J. Landin, the next 700 programming languages, cacm 9(3):15765, march 1966.
Why is java still changing? 7

run smoothly on the jvmand to interoperate with java. Java has also been successful
at colonizing various aspects of embedded computing (everything from smart cards,
toasters, and set-top boxes to car-braking systems).

How did java get into a general programming niche?
Object orientation became fashionable in the 1990s for two reasons: its encapsulation
discipline resulted in fewer software engineering issues than those of c; and as a
mental model it easily captured the wimp programming model of windows 95 and
up. This can be summarized as follows: everything is an object; and a mouse click
sends an event message to a handler (invokes the clicked method in a mouse
object). The write-once, run-anywhere model of java and the ability of early browsers
to (safely) execute java code applets gave it a niche in universities, whose
graduates then populated industry. There was initial resistance to the additional run
cost of java over c/c++, but machines got faster, and programmer time became
more and more important. Microsofts c# further validated the java-style object-
oriented model.

But the climate is changing for the programming language ecosystem; programmers
are increasingly dealing with so-called big data (data sets of terabytes and up) and wish-
ing to exploit multicore computers or computing clusters effectively to process it. And
this means using parallel processingsomething java wasnt previously friendly to.
You may have come across ideas from other programming niches (for example, goo-
gles map-reduce or the relative ease of data manipulation using database query lan-
guages such as sql) that help you work with large volumes of data and multicore
cpus. Figure 1.1 summarizes the language ecosystem pictorially: think of the land-
scape as the space of programming problems and the dominant vegetation for a par-
ticular bit of ground as the favorite language for that program. Climate change is the
idea that new hardware or new programming influences (for example, why cant i
program in an sql-like style? ) mean that different languages become the language

climate change (multicore processors,
new programmer influences)

scala c#/f# figure 1.1 programming-
language ecosystem and
javascript etc.
Java c/c++ climate change
8 chapter 1 java 8, 9, 10, and 11: whats happening?

Of choice for new projects, just like increasing regional temperatures mean grapes
now thrive in higher latitudes. But theres hysteresismany an old farmer will keep
raising traditional crops. In summary, new languages are appearing and becoming
increasingly popular because theyve adapted quickly to the climate change.
The main benefit of the java 8 additions for a programmer is that they provide
more programming tools and concepts to solve new or existing programming prob-
lems more quickly or, more importantly, in a more concise, more easily maintainable
way. Although the concepts are new to java, theyve proved powerful in niche research-
like languages. In the following sections, well highlight and develop the ideas behind
three such programming concepts that have driven the development of the java 8 fea-
tures to exploit parallelism and write more concise code in general. Well introduce
them in a slightly different order from the rest of the book to enable a unix-based
analogy and to expose the need this because of that dependencies in java 8s new
parallelism for multicore.

Another climate-change factor for java
one climate-change factor involves how large systems are designed. Nowadays, its
common for a large system to incorporate large component subsystems from else-
where, and perhaps these are built on top of other components from other vendors.
Worse still, these components and their interfaces also tend to evolve. Java 8 and
java 9 have addressed these aspects by providing default methods and modules to
facilitate this design style.

The next three sections examine the three programming concepts that drove the
design of java 8.

1.2.2 stream processing
the first programming concept is stream processing. For introductory purposes, a stream
is a sequence of data items that are conceptually produced one at a time. A program
might read items from an input stream one by one and similarly write items to an out-
put stream. The output stream of one program could well be the input stream of
another.
One practical example is in unix or linux, where many programs operate by read-
ing data from standard input (stdin in unix and c, system. In in java), operating on
it, and then writing their results to standard output (stdout in unix and c, system. Out
in java). First, a little background: unix cat creates a stream by concatenating two
files, tr translates the characters in a stream, sort sorts lines in a stream, and tail -3
gives the last three lines in a stream. The unix command line allows such programs to
be linked together with pipes (|), giving examples such as

cat file1 file2 | tr "[a-z]" "[a-z]" | sort | tail -3
why is java still changing? 9

which (supposing file1 and file2 contain a single word per line) prints the three
words from the files that appear latest in dictionary order, after first translating them
to lowercase. We say that sort takes a stream of lines3 as input and produces another
stream of lines as output (the latter being sorted), as illustrated in figure 1.2. Note
that in unix these commands (cat, tr, sort, and tail) are executed concurrently, so
that sort can be processing the first few lines before cat or tr has finished. A more
mechanical analogy is a car-manufacturing assembly line where a stream of cars is
queued between processing stations that each take a car, modify it, and pass it on to
the next station for further processing; processing at separate stations is typically con-
current even though the assembly line is physically a sequence.

File 1 file 2 "[a-z]" "[a-z]" -3

cat tr sort tail

figure 1.2 unix commands operating on streams

java 8 adds a streams api (note the uppercase s) in java. Util. Stream based on this
idea; stream<t> is a sequence of items of type t. You can think of it as a fancy iterator
for now. The streams api has many methods that can be chained to form a complex
pipeline just like unix commands were chained in the previous example.
The key motivation for this is that you can now program in java 8 at a higher level
of abstraction, structuring your thoughts of turning a stream of this into a stream of
that (similar to how you think when writing database queries) rather than one item at
a time. Another advantage is that java 8 can transparently run your pipeline of stream
operations on several cpu cores on disjoint parts of the inputthis is parallelism
almost for free instead of hard work using threads. We cover the java 8 streams api in
detail in chapters 47.

1.2.3 passing code to methods with behavior parameterization
the second programming concept added to java 8 is the ability to pass a piece of code
to an api. This sounds awfully abstract. In the unix example, you might want to tell
the sort command to use a custom ordering. Although the sort command supports
command-line parameters to perform various predefined kinds of sorting such as
reverse order, these are limited.
For example, lets say you have a collection of invoice ids with a format similar to
2013uk0001, 2014us0002, and so on. The first four digits represent the year, the next
two letters a country code, and last four digits the id of a client. You may want to sort

3
purists will say a stream of characters, but its conceptually simpler to think that sort reorders lines.
10 chapter 1 java 8, 9, 10, and 11: whats happening?

These invoice ids by year or perhaps using the customer id or even the country code.
What you want is the ability to tell the sort command to take as an argument an
ordering defined by the user: a separate piece of code passed to the sort command.
Now, as a direct parallel in java, you want to tell a sort method to compare using a
customized order. You could write a method compareusingcustomerid to compare
two invoice ids, but, prior to java 8, you couldnt pass this method to another method!
You could create a comparator object to pass to the sort method as we showed at the
start of this chapter, but this is verbose and obfuscates the idea of simply reusing an
existing piece of behavior. Java 8 adds the ability to pass methods (your code) as argu-
ments to other methods. Figure 1.3, based on figure 1.2, illustrates this idea. We also
refer to this conceptually as behavior parameterization. Why is this important? The
streams api is built on the idea of passing code to parameterize the behavior of its
operations, just as you passed compareusingcustomerid to parameterize the behavior
of sort.

Public int compareusingcustomerid(string inv1, string inv2){
....
}

sort

figure 1.3 passing method compareusingcustomerid as an argument to sort

we summarize how this works in section 1.3 of this chapter, but leave full details to
chapters 2 and 3. Chapters 18 and 19 look at more advanced things you can do using
this feature, with techniques from the functional programming community.

1.2.4 parallelism and shared mutable data
the third programming concept is rather more implicit and arises from the phrase
parallelism almost for free in our previous discussion on stream processing. What do
you have to give up? You may have to make some small changes in the way you code
the behavior passed to stream methods. At first, these changes might feel a little
uncomfortable, but once you get used to them, youll love them. You must provide
behavior that is safe to execute concurrently on different pieces of the input. Typically
this means writing code that doesnt access shared mutable data to do its job. Some-
times these are referred to as pure functions or side-effect-free functions or stateless
functions, and well discuss these in detail in chapters 18 and 19. The previous paral-
lelism arises only by assuming that multiple copies of your piece of code can work
independently. If theres a shared variable or object, which is written to, then things
no longer work. What if two processes want to modify the shared variable at the same
why is java still changing? 11

time? (section 1.4 gives a more detailed explanation with a diagram. ) youll find more
about this style throughout the book.
Java 8 streams exploit parallelism more easily than javas existing threads api, so
although its possible to use synchronized to break the no-shared-mutable-data rule,
its fighting the system in that its abusing an abstraction optimized around that rule.
Using synchronized across multiple processing cores is often far more expensive than
you expect, because synchronization forces code to execute sequentially, which works
against the goal of parallelism.
Two of these points (no shared mutable data and the ability to pass methods and
functionscodeto other methods) are the cornerstones of whats generally described
as the paradigm of functional programming, which youll see in detail in chapters 18 and
19. In contrast, in the imperative programming paradigm you typically describe a pro-
gram in terms of a sequence of statements that mutate state. The no-shared-mutable-
data requirement means that a method is perfectly described solely by the way it trans-
forms arguments to results; in other words, it behaves as a mathematical function and
has no (visible) side effects.

1.2.5 java needs to evolve
youve seen evolution in java before. For example, the introduction of generics and
using list<string> instead of just list may initially have been irritating. But
youre now familiar with this style and the benefits it brings (catching more errors at
compile time and making code easier to read, because you now know what some-
thing is a list of).
Other changes have made common things easier to express (for example, using
a for-each loop instead of exposing the boilerplate use of an iterator). The main
changes in java 8 reflect a move away from classical object orientation, which often
focuses on mutating existing values, and toward the functional-style programming
spectrum in which what you want to do in broad-brush terms (for example, create a
value representing all transport routes from a to b for less than a given price) is con-
sidered prime and separated from how you can achieve this (for example, scan a data
structure modifying certain components). Note that classical object-oriented pro-
gramming and functional programming, as extremes, might appear to be in con-
flict. But the idea is to get the best from both programming paradigms, so you have
a better chance of having the right tool for the job. We discuss this in detail in sec-
tions 1.3 and 1.4.
A takeaway line might be this: languages need to evolve to track changing hard-
ware or programmer expectations (if you need convincing, consider that cobol
was once one of the most important languages commercially). To endure, java has to
evolve by adding new features. This evolution will be pointless unless the new features
are used, so in using java 8 youre protecting your way of life as a java programmer.
On top of that, we have a feeling youll love using java 8s new features. Ask anyone
whos used java 8 whether theyre willing to go back! Additionally, the new java 8
12 chapter 1 java 8, 9, 10, and 11: whats happening?

Features might, in the ecosystem analogy, enable java to conquer programming-task
territory currently occupied by other languages, so java 8 programmers will be even
more in demand.
We now introduce the new concepts in java 8, one by one, pointing out the chap-
ters that cover these concepts in more detail.

1.3 functions in java
the word function in programming languages is commonly used as a synonym for
method, particularly a static method; this is in addition to it being used for mathematical
function, one without side effects. Fortunately, as youll see, when java 8 refers to func-
tions these usages nearly coincide.
Java 8 adds functions as new forms of value. These facilitate the use of streams, cov-
ered in section 1.4, which java 8 provides to exploit parallel programming on multi-
core processors. We start by showing that functions as values are useful in themselves.
Think about the possible values manipulated by java programs. First, there are
primitive values such as 42 (of type int) and 3.14 (of type double). Second, values can
be objects (more strictly, references to objects). The only way to get one of these is by
using new, perhaps via a factory method or a library function; object references point
to instances of a class. Examples include "abc" (of type string), new integer(1111)
(of type integer), and the result new hashmap<integer, string>(100) of explicitly
calling a constructor for hashmap. Even arrays are objects. Whats the problem?
To help answer this, well note that the whole point of a programming language is
to manipulate values, which, following historical programming-language tradition, are
therefore called first-class values (or citizens, in the terminology borrowed from the
1960s civil rights movement in the united states). Other structures in our program-
ming languages, which perhaps help us express the structure of values but which cant
be passed around during program execution, are second-class citizens. Values as listed
previously are first-class java citizens, but various other java concepts, such as methods
and classes, exemplify second-class citizens. Methods are fine when used to define
classes, which in turn may be instantiated to produce values, but neither are values
themselves. Does this matter? Yes, it turns out that being able to pass methods around
at runtime, and hence making them first-class citizens, is useful in programming, so
the java 8 designers added the ability to express this directly in java. Incidentally, you
might wonder whether making other second-class citizens such as classes into first-
class-citizen values might also be a good idea. Various languages such as smalltalk and
javascript have explored this route.

1.3.1 methods and lambdas as first-class citizens
experiments in other languages, such as scala and groovy, have determined that
allowing concepts like methods to be used as first-class values made programming
easier by adding to the toolset available to programmers. And once programmers
become familiar with a powerful feature, they become reluctant to use languages
functions in java 13

without it! The designers of java 8 decided to allow methods to be valuesto make
it easier for you to program. Moreover, the java 8 feature of methods as values forms
the basis of various other java 8 features (such as streams).
The first new java 8 feature we introduce is that of method references. Suppose you
want to filter all the hidden files in a directory. You need to start writing a method
that, given a file, will tell you whether its hidden. Fortunately, theres such a method
in the file class called ishidden. It can be viewed as a function that takes a file and
returns a boolean. But to use it for filtering, you need to wrap it into a filefilter
object that you then pass to the file. Listfiles method, as follows:
file[] hiddenfiles = new file(". "). Listfiles(new filefilter() {
public boolean accept(file file) {
return file. Ishidden(); filtering hidden files!
}
});

yuck! Thats horrible. Although its only three significant lines, its three opaque
lineswe all remember saying do i really have to do it this way? On first encounter.
You already have the method ishidden that you could use. Why do you have to wrap it
up in a verbose filefilter class and then instantiate it? Because thats what you had
to do prior to java 8.
Now, you can rewrite that code as follows:

file[] hiddenfiles = new file(". "). Listfiles(file: : ishidden);

wow! Isnt that cool? You already have the function ishidden available, so you pass it
to the listfiles method using the java 8 method reference : : syntax (meaning use this
method as a value); note that weve also slipped into using the word function for
methods. Well explain later how the mechanics work. One advantage is that your
code now reads closer to the problem statement.
Heres a taste of whats coming: methods are no longer second-class values. Analo-
gous to using an object reference when you pass an object around (and object references
are created by new), in java 8 when you write file: : ishidden, you create a method ref-
erence, which can similarly be passed around. This concept is discussed in detail in
chapter 3. Given that methods contain code (the executable body of a method), using
method references enables passing code around as in figure 1.3. Figure 1.4 illustrates
the concept. Youll also see a concrete example (selecting apples from an inventory)
in the next section.
Lambdas: anonymous functions
as well as allowing (named) methods to be first-class values, java 8 allows a richer idea
of functions as values, including lambdas 4 (or anonymous functions). For example, you
can now write (int x) -> x + 1 to mean the function that, when called with argument

4
originally named after the greek letter  (lambda). Although the symbol isnt used in java, its name lives on.
14 chapter 1 java 8, 9, 10, and 11: whats happening?

Old way of filtering hidden files

file[] hiddenfiles = new file(". "). Listfiles(new filefilter() {
public boolean accept(file file) {
return file. Ishidden();
}
filtering files with
}); the ishidden method
requires wrapping the
method inside a filefilter
filefilter object object before passing it to
the file. Listfiles method.
Ishidden method

file -> boolean file. Listfiles

java 8 style in java 8 you can
pass the ishidden
function to the listfiles
file[] hiddenfiles = new file(". "). Listfiles(file: : ishidden) method using the method
reference : : syntax.

File: : ishidden syntax
file: : ishidden file. Listfiles

figure 1.4 passing the method reference file: : ishidden to the method listfiles

x, returns the value x + 1. You might wonder why this is necessary, because you could
define a method add1 inside a class mymathsutils and then write mymathsutils: : add1!
Yes, you could, but the new lambda syntax is more concise for cases where you dont
have a convenient method and class available. Chapter 3 explores lambdas in detail.
Programs using these concepts are said to be written in functional-programming
style; this phrase means writing programs that pass functions around as first-class
values.

1.3.2 passing code: an example
lets look at an example of how this helps you write programs (discussed in more
detail in chapter 2). All the code for the examples is available on a github repository
and as a download via the books website. Both links may be found at www. Manning
. Com/books/modern-java-in-action. Suppose you have a class apple with a method
getcolor and a variable inventory holding a list of apples; then you might wish to
select all the green apples (here using a color enum type that includes values green
functions in java 15

and red) and return them in a list. The word filter is commonly used to express this
concept. Before java 8, you might write a method filtergreenapples:

public static list<apple> filtergreenapples(list<apple> inventory) {
list<apple> result = new arraylist<>();
for (apple apple: inventory){ the result list
if (green. Equals(apple. Getcolor())) { accumulates the result;
result. Add(apple); it starts as empty, and
} the highlighted then green apples are
} text selects only added one by one.
Return result; green apples.
}

but next, somebody would like the list of heavy apples (say over 150 g), and so, with a
heavy heart, youd write the following method to achieve this (perhaps even using
copy and paste):
public static list<apple> filterheavyapples(list<apple> inventory) {
list<apple> result = new arraylist<>();
for (apple apple: inventory){
if (apple. Getweight() > 150) {
here the highlighted
result. Add(apple); text selects only
} heavy apples.
}
return result;
}

we all know the dangers of copy and paste for software engineering (updates and bug
fixes to one variant but not the other), and hey, these two methods vary only in one
line: the highlighted condition inside the if construct. If the difference between the
two method calls in the highlighted code had been what weight range was acceptable,
then you could have passed lower and upper acceptable weights as arguments to
filterperhaps (150, 1000) to select heavy apples (over 150 g) or (0, 80) to select
light apples (under 80 g).
But as we mentioned previously, java 8 makes it possible to pass the code of the
condition as an argument, avoiding code duplication of the filter method. You can
now write this:
public static boolean isgreenapple(apple apple) {
return green. Equals(apple. Getcolor());
}
public static boolean isheavyapple(apple apple) {
return apple. Getweight() > 150; included for clarity
} (normally imported
public interface predicate<t>{ from java. Util. Function)
boolean test(t t);
} a method is passed as
static list<apple> filterapples(list<apple> inventory, a predicate parameter
predicate<apple> p) { named p (see the
list<apple> result = new arraylist<>();
sidebar whats a
predicate? ).
For (apple apple: inventory){
16 chapter 1 java 8, 9, 10, and 11: whats happening?

If (p. Test(apple)) {
does the apple match
result. Add(apple); the condition
} represented by p?
}
return result;
}

and to use this, you call either

filterapples(inventory, apple: : isgreenapple);

or

filterapples(inventory, apple: : isheavyapple);

we explain how this works in detail in the next two chapters. The key idea to take away
for now is that you can pass around a method in java 8.

Whats a predicate?
The previous code passed a method apple: : isgreenapple (which takes an apple
for argument and returns a boolean) to filterapples, which expected a predicate
<apple> parameter. The word predicate is often used in mathematics to mean some-
thing function-like that takes a value for an argument and returns true or false. As
youll see later, java 8 would also allow you to write function<apple, boolean>
more familiar to readers who learned about functions but not predicates at school
but using predicate<apple> is more standard (and slightly more efficient because
it avoids boxing a boolean into a boolean).

1.3.3 from passing methods to lambdas
passing methods as values is clearly useful, but its annoying having to write a defini-
tion for short methods such as isheavyapple and isgreenapple when theyre used
perhaps only once or twice. But java 8 has solved this, too. It introduces a new nota-
tion (anonymous functions, or lambdas) that enables you to write just

filterapples(inventory, (apple a) -> green. Equals(a. Getcolor()) );

or

filterapples(inventory, (apple a) -> a. Getweight() > 150 );

or even
filterapples(inventory, (apple a) -> a. Getweight() < 80 ||
red. Equals(a. Getcolor()) );

you dont even need to write a method definition thats used only once; the code is
crisper and clearer because you dont need to search to find the code youre passing.
Streams 17

but if such a lambda exceeds a few lines in length (so that its behavior isnt instantly
clear), you should instead use a method reference to a method with a descriptive
name instead of using an anonymous lambda. Code clarity should be your guide.
The java 8 designers could almost have stopped here, and perhaps they would
have done so before multicore cpus. Functional-style programming as presented so
far turns out to be powerful, as youll see. Java might then have been rounded off by
adding filter and a few friends as generic library methods, such as

static <t> collection<t> filter(collection<t> c, predicate<t> p);

you wouldnt even have to write methods like filterapples because, for example, the
previous call

filterapples(inventory, (apple a) -> a. Getweight() > 150 );

could be written as a call to the library method filter:

filter(inventory, (apple a) -> a. Getweight() > 150 );

but, for reasons centered on better exploiting parallelism, the designers didnt do
this. Java 8 instead contains a new collection-like api called stream, containing a
comprehensive set of operations similar to the filter operation that functional pro-
grammers may be familiar with (for example, map and reduce), along with methods to
convert between collections and streams, which we now investigate.

1.4 streams
nearly every java application makes and processes collections. But working with collec-
tions isnt always ideal. For example, lets say you need to filter expensive transactions
from a list and then group them by currency. Youd need to write a lot of boilerplate
code to implement this data-processing query, as shown here:
creates the map where the grouped
transaction will be accumulated
map<currency, list<transaction>> transactionsbycurrencies =
filters new hashmap<>();
expensive for (transaction transaction : transactions) {
transactions iterates the list
if(transaction. Getprice() > 1000){ of transactions
currency currency = transaction. Getcurrency();
list<transaction> transactionsforcurrency =
transactionsbycurrencies. Get(currency); extracts the
if (transactionsforcurrency == null) { transactions
if there isnt currency
an entry in transactionsforcurrency = new arraylist<>();
the grouping transactionsbycurrencies. Put(currency,
map for this transactionsforcurrency);
currency, }
transactionsforcurrency. Add(transaction); adds the currently
create it.
} traversed transaction to
}
the list of transactions
with the same currency
18 chapter 1 java 8, 9, 10, and 11: whats happening?

In addition, its difficult to understand at a glance what the code does because of the
multiple nested control-flow statements.
Using the streams api, you can solve this problem as follows:

import static java. Util. Stream. Collectors. Groupingby; filters expensive
map<currency, list<transaction>> transactionsbycurrencies = transactions
transactions. Stream()
. Filter((transaction t) -> t. Getprice() > 1000) groups them
. Collect(groupingby(transaction: : getcurrency)); by currency

dont worry about this code for now because it may look like a bit of magic. Chap-
ters 47 are dedicated to explaining how to make sense of the streams api. For now,
its worth noticing that the streams api provides a different way to process data in
comparison to the collections api. Using a collection, youre managing the iteration
process yourself. You need to iterate through the elements one by one using a for-
each loop processing them in turn. We call this way of iterating over data external itera-
tion. In contrast, using the streams api, you dont need to think in terms of loops. The
data processing happens internally inside the library. We call this idea internal iteration.
We come back to these ideas in chapter 4.
As a second pain point of working with collections, think for a second about how
you would process the list of transactions if you had a vast number of them; how can
you process this huge list? A single cpu wouldnt be able to process this large amount
of data, but you probably have a multicore computer on your desk. Ideally, youd like
to share the work among the different cpu cores available on your machine to reduce
the processing time. In theory, if you have eight cores, they should be able to process
your data eight times as fast as using one core, because they work in parallel.5

multicore computers
all new desktop and laptop computers are multicore computers. Instead of a single
cpu, they have four or eight or more cpus (usually called cores5). The problem is that
a classic java program uses just a single one of these cores, and the power of the oth-
ers is wasted. Similarly, many companies use computing clusters (computers con-
nected together with fast networks) to be able to process vast amounts of data
efficiently. Java 8 facilitates new programming styles to better exploit such computers.
Googles search engine is an example of a piece of code thats too big to run on a
single computer. It reads every page on the internet and creates an index, mapping
every word appearing on any internet page back to every url containing that word.
Then, when you do a google search involving several words, software can quickly use
this index to give you a set of web pages containing those words. Try to imagine how
you might code this algorithm in java (even for a smaller index than googles, youd
need to exploit all the cores in your computer).

5
this naming is unfortunate in some ways. Each of the cores in a multicore chip is a full-fledged cpu. But the
phrase multicore cpu has become common, so core is used to refer to the individual cpus.
Streams 19

1.4.1 multithreading is difficult
the problem is that exploiting parallelism by writing multithreaded code (using the
threads api from previous versions of java) is difficult. You have to think differently:
threads can access and update shared variables at the same time. As a result, data
could change unexpectedly if not coordinated6 properly. This model is harder to
think about7 than a step-by-step sequential model. For example, figure 1.5 shows a
possible problem with two threads trying to add a number to a shared variable sum if
theyre not synchronized properly.

Execution

1 2 3 4 5 6

thread 1 100 103 103
add (3)
read write

sum 100 100 100 100 103 105

read
write
add (5)
thread 2 100 105 105

thread 1: sum = sum + 3;
thread 2: sum = sum + 5;

figure 1.5 a possible problem with two threads trying to add to a shared sum variable. The result is
105 instead of an expected result of 108.

Java 8 also addresses both problems (boilerplate and obscurity involving processing
collections and difficulty exploiting multicore) with the streams api (java. Util
. Stream). The first design motivator is that there are many data-processing patterns
(similar to filterapples in the previous section or operations familiar from database
query languages such as sql) that occur over and over again and that would benefit
from forming part of a library: filtering data based on a criterion (for example, heavy
apples), extracting data (for example, extracting the weight field from each apple in a
list), or grouping data (for example, grouping a list of numbers into separate lists of
even and odd numbers), and so on. The second motivator is that such operations can

6
traditionally via the keyword synchronized, but many subtle bugs arise from its misplacement. Java 8s
stream-based parallelism encourages a functional programming style where synchronized is rarely used; it
focuses on partitioning the data rather than coordinating access to it.
7
ahaa source of pressure for the language to evolve!
20 chapter 1 java 8, 9, 10, and 11: whats happening?

Often be parallelized. For instance, as illustrated in figure 1.6, filtering a list on two
cpus could be done by asking one cpu to process the first half of a list and the sec-
ond cpu to process the other half of the list. This is called the forking step (1). The
cpus then filter their respective half-lists (2). Finally (3), one cpu would join the two
results. (this is closely related to how google searches work so quickly, using many
more than two processors. )

1
fork list of
a b c d e
5 apples

cpu 1 cpu 2
2
filter
b c e

3
joint
b c e
results

figure 1.6 forking filter onto two cpus and joining the result

for now, well just say that the new streams api behaves similarly to javas existing
collections api: both provide access to sequences of data items. But its useful for
now to keep in mind that collections is mostly about storing and accessing data,
whereas streams is mostly about describing computations on data. The key point
here is that the streams api allows and encourages the elements within a stream to be
processed in parallel. Although it may seem odd at first, often the fastest way to fil-
ter a collection (for example, to use filterapples in the previous section on a list)
is to convert it to a stream, process it in parallel, and then convert it back to a list.
Again, well just say parallelism almost for free and provide a taste of how you can
filter heavy apples from a list sequentially or in parallel using streams and a lambda
expression.
Heres an example of sequential processing:

import static java. Util. Stream. Collectors. Tolist;
list<apple> heavyapples =
inventory. Stream(). Filter((apple a) -> a. Getweight() > 150)
. Collect(tolist());
default methods and java modules 21

and here it is using parallel processing:
import static java. Util. Stream. Collectors. Tolist;
list<apple> heavyapples =
inventory. Parallelstream(). Filter((apple a) -> a. Getweight() > 150)
. Collect(tolist());

parallelism in java and no shared mutable state
people have always said parallelism in java is difficult, and all this stuff about
synchronized is error-prone. Wheres the magic bullet in java 8?
There are two magic bullets. First, the library handles partitioningbreaking down
a big stream into several smaller streams to be processed in parallel for you. Sec-
ond, this parallelism almost for free from streams, works only if the methods passed
to library methods like filter dont interact (for example, by having mutable shared
objects). But it turns out that this restriction feels natural to a coder (see, by way
of example, our apple: : isgreenapple example). Although the primary meaning of
functional in functional programming means using functions as first-class values,
it often has a secondary nuance of no interaction during execution between com-
ponents.

Chapter 7 explores parallel data processing in java 8 and its performance in more
detail. One of the practical issues the java 8 developers found in evolving java with all
these new goodies was that of evolving existing interfaces. For example, the method
collections. Sort belongs to the list interface but was never included. Ideally, youd
like to do list. Sort(comparator) instead of collections. Sort(list, comparator).
This may seem trivial but, prior to java 8 you can update an interface only if you
update all the classes that implement ita logistical nightmare! This issue is resolved
in java 8 by default methods.

1.5 default methods and java modules
as we mentioned earlier, modern systems tend to be built from componentsperhaps
bought-in from elsewhere. Historically, java had little support for this, apart from a
jar file containing a set of java packages with no particular structure. Moreover, evolv-
ing interfaces to such packages was hardchanging a java interface meant changing
every class that implements it. Java 8 and 9 have started to address this.
First, java 9 provides a module system that provide you with syntax to define mod-
ules containing collections of packagesand keep much better control over visibility
and namespaces. Modules enrich a simple jar-like component with structure, both
as user documentation and for machine checking; we explain them in detail in
chapter 14. Second, java 8 added default methods to support evolvable interfaces. We
cover these in detail in chapter 13. Theyre important because youll increasingly
encounter them in interfaces, but because relatively few programmers will need to
write default methods themselves and because they facilitate program evolution
22 chapter 1 java 8, 9, 10, and 11: whats happening?

Rather than helping write any particular program, we keep the explanation here
short and example-based.
In section 1.4, we gave the following example java 8 code:
list<apple> heavyapples1 =
inventory. Stream(). Filter((apple a) -> a. Getweight() > 150)
. Collect(tolist());
list<apple> heavyapples2 =
inventory. Parallelstream(). Filter((apple a) -> a. Getweight() > 150)
. Collect(tolist());

but theres a problem here: a list<t> prior to java 8 doesnt have stream or parallel-
stream methodsand neither does the collection<t> interface that it implements
because these methods hadnt been conceived of. And without these methods, this code
wont compile. The simplest solution, which you might employ for your own interfaces,
would have been for the java 8 designers to add the stream method to the collection
interface and add the implementation in the arraylist class.
But doing this would have been a nightmare for users. Many alternative collection
frameworks implement interfaces from the collections api. Adding a new method to
an interface means all concrete classes must provide an implementation for it. Lan-
guage designers have no control over existing implementations of collection, so you
have a dilemma: how can you evolve published interfaces without disrupting existing
implementations?
The java 8 solution is to break the last link: an interface can now contain method
signatures for which an implementing class doesnt provide an implementation. Then
who implements them? The missing method bodies are given as part of the interface
(hence default implementations) rather than in the implementing class.
This provides a way for an interface designer to enlarge an interface beyond those
methods that were originally plannedwithout breaking existing code. Java 8 allows
the existing default keyword to be used in interface specifications to achieve this.
For example, in java 8, you can call the sort method directly on a list. This is made
possible with the following default method in the java 8 list interface, which calls the
static method collections. Sort:
default void sort(comparator<? Super e> c) {
collections. Sort(this, c);
}

this means any concrete classes of list dont have to explicitly implement sort,
whereas in previous java versions such concrete classes would fail to recompile unless
they provided an implementation for sort.
But wait a second. A single class can implement multiple interfaces, right? If you
have multiple default implementations in several interfaces, does that mean you have
a form of multiple inheritance in java? Yes, to some extent. We show in chapter 13 that
there are some rules that prevent issues such as the infamous diamond inheritance prob-
lem in c++.
Other good ideas from functional programming 23

1.6 other good ideas from functional programming
the previous sections introduced two core ideas from functional programming that
are now part of java: using methods and lambdas as first-class values, and the idea that
calls to functions or methods can be efficiently and safely executed in parallel in the
absence of mutable shared state. Both of these ideas are exploited by the new streams
api we described earlier.
Common functional languages (sml, ocaml, haskell) also provide further con-
structs to help programmers. One of these is avoiding null by explicit use of more
descriptive data types. Tony hoare, one of the giants of computer science, said this in
a presentation at qcon london 2009:
i call it my billion-dollar mistake. It was the invention of the null reference in 1965. . . . I
couldnt resist the temptation to put in a null reference, simply because it was so easy to
implement.

Java 8 introduced the optional<t> class that, if used consistently, can help you avoid
null-pointer exceptions. Its a container object that may or may not contain a value.
Optional<t> includes methods to explicitly deal with the case where a value is absent,
and as a result you can avoid null-pointer exceptions. It uses the type system to allow
you to indicate when a variable is anticipated to potentially have a missing value. We
discuss optional<t> in detail in chapter 11.
A second idea is that of (structural) pattern matching.8 this is used in mathematics.
For example:
f(0) = 1
f(n) = n*f(n-1) otherwise

in java, you would write an if-then-else or a switch statement. Other languages
have shown that, for more complex data types, pattern matching can express pro-
gramming ideas more concisely compared to using if-then-else. For such data
types, you might also use polymorphism and method overriding as an alternative to
if-then-else, but theres ongoing language-design discussion as to which is more
appropriate.9 wed say that both are useful tools and that you should have both in
your armory. Unfortunately, java 8 doesnt have full support for pattern matching,
although we show how it can be expressed in chapter 19. A java enhancement pro-
posal is also being discussed to support pattern matching in a future version of java
(see http: //openjdk. Java. Net/jeps/305). In the meantime, lets illustrate with an exam-
ple expressed in the scala programming language (another java-like language using
the jvm that has inspired some aspects of java evolution; see chapter 20). Suppose you

8
this phrase has two uses. Here we mean the one familiar from mathematics and functional programming
whereby a function is defined by cases, rather than using if-then-else. The other meaning concerns phrases
like find all files of the form img*. Jpg in a given directory associated with so-called regular expressions.
9
the wikipedia article on the expression problem (a term coined by phil wadler) provides an entry to the
discussion.
24 chapter 1 java 8, 9, 10, and 11: whats happening?

Want to write a program that does basic simplifications on a tree representing an arith-
metic expression. Given a data type expr representing such expressions, in scala you
can write the following code to decompose an expr into its parts and then return
another expr:
def simplifyexpression(expr: expr): expr = expr match { adds 0
case binop("+", e, number(0)) => e
case binop("-", e, number(0)) => e
subtracts 0
case binop("*", e, number(1)) => e
case binop("/", e, number(1)) => e multiplies by 1
case _ => expr
} cant be simplified with divides by 1
these cases, so leave alone

here scalas syntax expr match corresponds to javas switch (expr). Dont worry
about this code for nowyoull read more on pattern matching in chapter 19. For
now, you can think of pattern matching as an extended form of switch that can
decompose a data type into its components at the same time.
Why should the switch statement in java be limited to primitive values and strings?
Functional languages tend to allow switch to be used on many more data types,
including allowing pattern matching (in the scala code, this is achieved using a match
operation). In object-oriented design, the visitor pattern is a common pattern used to
walk through a family of classes (such as the different components of a car: wheel,
engine, chassis, and so on) and apply an operation to each object visited. One advan-
tage of pattern matching is that a compiler can report common errors such as, class
brakes is part of the family of classes used to represent components of class car. You
forgot to explicitly deal with it.
Chapters 18 and 19 give a full tutorial introduction to functional programming
and how to write functional-style programs in java 8including the toolkit of func-
tions provided in its library. Chapter 20 follows by discussing how java 8 features com-
pare to those in scalaa language that, like java, is implemented on top of the jvm
and that has evolved quickly to threaten some aspects of javas niche in the program-
ming language ecosystem. This material is positioned toward the end of the book to
provide additional insight into why the new java 8 and java 9 features were added.

Java 8, 9, 10, and 11 features: where do you start?
Java 8 and java 9 both provided significant updates to java. But as a java program-
mer, its likely to be the java 8 additions that affect you most on a daily small-scale-
coding basisthe idea of passing a method or a lambda is rapidly becoming vital
java knowledge. In contrast, the java 9 enhancements enrich our ability to define and
use larger-scale components, be it structuring a system using modules or importing
a reactive-programming toolkit. Finally, java 10 is a much smaller increment com-
pared to previous upgrades and consists of allowing type inference for local vari-
ables, which we discuss briefly in chapter 21, where we also mention the related
richer syntax for arguments of lambda expressions due to be introduced in java 11.
Summary 25

at the time of writing, java 11 is scheduled to be released in september 2018. Java 11
also brings a new asynchronous http client library (http: //openjdk. Java. Net/jeps/321)
that leverages the java 8 and java 9 developments (details in chapters 15, 16, and
17) of completablefuture and reactive programming.

Summary
 keep in mind the idea of the language ecosystem and the consequent evolve-
or-wither pressure on languages. Although java may be supremely healthy at
the moment, we can recall other healthy languages such as cobol that
failed to evolve.
 The core additions to java 8 provide exciting new concepts and functionality to
ease the writing of programs that are both effective and concise.
 Multicore processors arent fully served by pre-java-8 programming practice.
 Functions are first-class values; remember how methods can be passed as func-
tional values and how anonymous functions (lambdas) are written.
 The java 8 concept of streams generalizes many aspects of collections, but the
former often enables more readable code and allows elements of a stream to be
processed in parallel.
 Large-scale component-based programming, and evolving a systems interfaces,
werent historically well served by java. You can now specify modules to struc-
ture systems in java 9 and use default methods to allow an interface to be
enhanced without changing all the classes that implement it.
 Other interesting ideas from functional programming include dealing with
null and using pattern matching.
Passing code with behavior
parameterization

this chapter covers
 coping with changing requirements
 behavior parameterization
 anonymous classes
 preview of lambda expressions
 real-world examples: comparator, runnable,
and gui

a well-known problem in software engineering is that no matter what you do,
user requirements will change. For example, imagine an application to help a
farmer understand his inventory. The farmer might want a functionality to find
all green apples in his inventory. But the next day he might tell you, actually, i
also want to find all apples heavier than 150 g. Two days later, the farmer comes
back and adds, it would be really nice if i could find all apples that are green
and heavier than 150 g. How can you cope with these changing requirements?
Ideally, youd like to minimize your engineering effort. In addition, similar new
functionalities ought to be straightforward to implement and maintainable in
the long term.

26
coping with changing requirements 27

behavior parameterization is a software development pattern that lets you handle fre-
quent requirement changes. In a nutshell, it means taking a block of code and making
it available without executing it. This block of code can be called later by other parts
of your programs, which means that you can defer the execution of that block of
code. For instance, you could pass the block of code as an argument to another
method that will execute it later. As a result, the methods behavior is parameterized
based on that block of code. For example, if you process a collection, you may want to
write a method that
 can do something for every element of a list
 can do something else when you finish processing the list
 can do yet something else if you encounter an error

this is what behavior parameterization refers to. Heres an analogy: your roommate
knows how to drive to the supermarket and back home. You can tell him to buy a list
of things such as bread, cheese, and wine. This is equivalent to calling a method
goandbuy passing a list of products as its argument. But one day youre at the office,
and you need him to do something hes never done beforepick up a package from
the post office. You need to pass him a list of instructions: go to the post office, use
this reference number, talk to the manager, and pick up the parcel. You could pass
him the list of instructions by email, and when he receives it, he can follow the instruc-
tions. Youve now done something a bit more advanced thats equivalent to a method
goanddo, which can execute various new behaviors as arguments.
Well start this chapter by walking you through an example of how you can evolve
your code to be more flexible for changing requirements. Building on this knowl-
edge, we show how to use behavior parameterization for several real-world examples.
For example, you may have used the behavior parameterization pattern already, using
existing classes and interfaces in the java api to sort a list, to filter names of files, or
to tell a thread to execute a block of code or even perform gui event handling. Youll
soon realize that this pattern is historically verbose in java. Lambda expressions in java
8 onward tackle the problem of verbosity. Well show in chapter 3 how to construct
lambda expressions, where to use them, and how you can make your code more con-
cise by adopting them.

2.1 coping with changing requirements
writing code that can cope with changing requirements is difficult. Lets walk through
an example that well gradually improve, showing some best practices for making your
code more flexible. In the context of a farm-inventory application, you have to imple-
ment a functionality to filter green apples from a list. Sounds easy, right?
28 chapter 2 passing code with behavior parameterization

2.1.1 first attempt: filtering green apples
assume, as in chapter 1, you have a color enum available to represent different colors
of an apple:

enum color { red, green }

a first solution might be as follows:

public static list<apple> filtergreenapples(list<apple> inventory) {
list<apple> result = new arraylist<>();
for(apple apple: inventory){ an accumulator
if( green. Equals(apple. Getcolor() ) { list for apples
result. Add(apple);
} selects only green apples
}
return result;
}

the highlighted line shows the condition required to select green apples. You can
assume that you have a color enum with a set of colors, such as green, available. But
now the farmer changes his mind and wants to also filter red apples. What can you do?
A naive solution would be to duplicate your method, rename it as filterredapples,
and change the if condition to match red apples. However, this approach doesnt
cope well with changes if the farmer wants multiple colors. A good principle is this:
when you find yourself writing nearly repeated code, try to abstract instead.

2.1.2 second attempt: parameterizing the color
how do we avoid duplicating most of the code in filtergreenapples to make filter-
redapples? To parameterize the color and be more flexible to such changes, what you
could do is add a parameter to your method:

public static list<apple> filterapplesbycolor(list<apple> inventory,
color color) {
list<apple> result = new arraylist<>();
for (apple apple: inventory) {
if ( apple. Getcolor(). Equals(color) ) {
result. Add(apple);
}
}
return result;
}

you can now make the farmer happy and invoke your method as follows:

list<apple> greenapples = filterapplesbycolor(inventory, green);
list<apple> redapples = filterapplesbycolor(inventory, red);
...
Coping with changing requirements 29

too easy, right? Lets complicate the example a bit. The farmer comes back to you and
says, it would be really cool to differentiate between light apples and heavy apples.
Heavy apples typically have a weight greater than 150 g.
Wearing your software engineering hat, you realize in advance that the farmer may
want to vary the weight. So you create the following method to cope with various
weights through an additional parameter:
public static list<apple> filterapplesbyweight(list<apple> inventory,
int weight) {
list<apple> result = new arraylist<>();
for (apple apple: inventory){
if ( apple. Getweight() > weight ) {
result. Add(apple);
}
}
return result;
}

this is a good solution, but notice how you have to duplicate most of the implementa-
tion for traversing the inventory and applying the filtering criteria on each apple. This
is somewhat disappointing because it breaks the dry (dont repeat yourself) principle
of software engineering. What if you want to alter the filter traversing to enhance per-
formance? You now have to modify the implementation of all of your methods instead
of only a single one. This is expensive from an engineering-effort perspective.
You could combine the color and weight into one method, called filter. But then
youd still need a way to differentiate what attribute you want to filter on. You could
add a flag to differentiate between color and weight queries. (but never do this! Well
explain why shortly. )

2.1.3 third attempt: filtering with every attribute you can think of
an ugly attempt to merge all attributes might be as follows:
public static list<apple> filterapples(list<apple> inventory, color color,
int weight, boolean flag) {
list<apple> result = new arraylist<>();
for (apple apple: inventory) {
if ( (flag && apple. Getcolor(). Equals(color)) ||
(! Flag && apple. Getweight() > weight) ){
an ugly way to
result. Add(apple); select color or
} weight
}
return result;
}

you could use this as follows (but its ugly):
list<apple> greenapples = filterapples(inventory, green, 0, true);
list<apple> heavyapples = filterapples(inventory, null, 150, false);
...
30 chapter 2 passing code with behavior parameterization

this solution is extremely bad. First, the client code looks terrible. What do true and
false mean? In addition, this solution doesnt cope well with changing requirements.
What if the farmer asks you to filter with different attributes of an apple, for example,
its size, its shape, its origin, and so on? Furthermore, what if the farmer asks you for
more complicated queries that combine attributes, such as green apples that are also
heavy? Youd either have multiple duplicated filter methods or one hugely complex
method. So far, youve parameterized the filterapples method with values such as a
string, an integer, an enum type, or a boolean. This can be fine for certain well-
defined problems. But in this case, what you need is a better way to tell your filter-
apples method the selection criteria for apples. In the next section, we describe how
to make use of behavior parameterization to attain that flexibility.

2.2 behavior parameterization
you saw in the previous section that you need a better way than adding lots of parame-
ters to cope with changing requirements. Lets step back and find a better level of
abstraction. One possible solution is to model your selection criteria: youre working
with apples and returning a boolean based on some attributes of apple. For example,
is it green? Is it heavier than 150 g? We call this a predicate (a function that returns a
boolean). Lets therefore define an interface to model the selection criteria:
public interface applepredicate{
boolean test (apple apple);
}

you can now declare multiple implementations of applepredicate to represent dif-
ferent selection criteria, as shown in the following (and illustrated in figure 2.1):
public class appleheavyweightpredicate implements applepredicate {
public boolean test(apple apple) {
return apple. Getweight() > 150; selects only heavy apples
}
}
public class applegreencolorpredicate implements applepredicate {
public boolean test(apple apple) {
return green. Equals(apple. Getcolor()); selects only green apples
}
}

applepredicate
encapsulates a strategy
applepredicate
for selecting an apple
+ boolean test(apple apple)

applegreencolorpredicate appleheavyweightpredicate

figure 2.1 different strategies for selecting an apple
behavior parameterization 31

you can see these criteria as different behaviors for the filter method. What you just
did is related to the strategy design pattern (see http: //en. Wikipedia. Org/wiki/strategy_
pattern), which lets you define a family of algorithms, encapsulate each algorithm
(called a strategy), and select an algorithm at run time. In this case the family of algo-
rithms is applepredicate and the different strategies are appleheavyweightpredicate
and applegreencolorpredicate.
But how can you make use of the different implementations of applepredicate?
You need your filterapples method to accept applepredicate objects to test a con-
dition on an apple. This is what behavior parameterization means: the ability to tell a
method to take multiple behaviors (or strategies) as parameters and use them inter-
nally to accomplish different behaviors.
To achieve this in the running example, you add a parameter to the filterapples
method to take an applepredicate object. This has a great software engineering ben-
efit: you can now separate the logic of iterating the collection inside the filter-
apples method with the behavior you want to apply to each element of the collection
(in this case a predicate).

2.2.1 fourth attempt: filtering by abstract criteria
our modified filter method, which uses an applepredicate, looks like this:
public static list<apple> filterapples(list<apple> inventory,
applepredicate p) {
list<apple> result = new arraylist<>();
for(apple apple: inventory) {
if(p. Test(apple)) {
predicate p encapsulates
result. Add(apple); the condition to test on
} an apple.
}
return result;
}

passing code/behavior
its worth pausing for a moment for a small celebration. This code is much more flex-
ible than our first attempt, but at the same time its easy to read and to use! You can
now create different applepredicate objects and pass them to the filterapples
method. Free flexibility! For example, if the farmer asks you to find all red apples that
are heavier than 150 g, all you need to do is create a class that implements the
applepredicate accordingly. Your code is now flexible enough for any change of
requirements involving the attributes of apple:
public class appleredandheavypredicate implements applepredicate {
public boolean test(apple apple){
return red. Equals(apple. Getcolor())
&& apple. Getweight() > 150;
}
}
list<apple> redandheavyapples =
filterapples(inventory, new appleredandheavypredicate());
32 chapter 2 passing code with behavior parameterization

youve achieved something cool; the behavior of the filterapples method depends
on the code you pass to it via the applepredicate object. Youve parameterized the
behavior of the filterapples method!
Note that in the previous example, the only code that matters is the implementa-
tion of the test method, as illustrated in figure 2.2; this is what defines the new behav-
iors for the filterapples method. Unfortunately, because the filterapples method
can only take objects, you have to wrap that code inside an applepredicate object.
What youre doing is similar to passing code inline, because youre passing a boolean
expression through an object that implements the test method. Youll see in sec-
tion 2.3 (and in more detail in chapter 3) that by using lambdas, you can directly pass
the expression red. Equals(apple. Getcolor()) && apple. Getweight() > 150 to the
filterapples method without having to define multiple applepredicate classes.
This removes unnecessary verbosity.

Applepredicate object

public class appleredandheavypredicate implements applepredicate {
public boolean test(apple apple){

return red. Equals(apple. Getcolor())
&& apple. Getweight() > 150;

}
}

pass as
argument

filterapples(inventory, );

pass a strategy to the filter method: filter
the apples by using the boolean expression
encapsulated within the applepredicate object.
To encapsulate this piece of code, it is wrapped
with a lot of boilerplate code (in bold).

Figure 2.2 parameterizing the behavior of filterapples and passing different filter
strategies

multiple behaviors, one parameter
as we explained earlier, behavior parameterization is great because it enables you to
separate the logic of iterating the collection to filter and the behavior to apply on
each element of that collection. As a consequence, you can reuse the same method
and give it different behaviors to achieve different things, as illustrated in figure 2.3.
This is why behavior parameterization is a useful concept you should have in your toolset
for creating flexible apis.
Behavior parameterization 33

applepredicate applepredicate

new
return apple. Getweight() > 150; return green. Equals(apple. Getcolor());
behavior

public static list<apple> filterapples(list<apple> inventory, applepredicate p){
list<apple> result= new arraylist<>();
behavior
for(apple apple: inventory){
parameterization
if(p. Test(apple)){
result. Add(apple);
}
}
return result;
}

output heavy green
apples apples

figure 2.3 parameterizing the behavior of filterapples and passing different filter strategies

to make sure you feel comfortable with the idea of behavior parameterization, try to
do quiz 2.1!

Quiz 2.1: write a flexible prettyprintapple method
write a prettyprintapple method that takes a list of apples and that can be
parameterized with multiple ways to generate a string output from an apple (a bit
like multiple customized tostring methods). For example, you could tell your
prettyprintapple method to print only the weight of each apple. In addition, you
could tell your prettyprintapple method to print each apple individually and men-
tion whether its heavy or light. The solution is similar to the filtering examples weve
explored so far. To help you get started, we provide a rough skeleton of the pretty-
printapple method:
public static void prettyprintapple(list<apple> inventory, ? ? ? ) {
for(apple apple: inventory) {
string output = ? ? ?. ? ? ? (apple);
system. Out. Println(output);
}
}

answer:
first, you need a way to represent a behavior that takes an apple and returns a
formatted string result. You did something similar when you created an apple-
predicate interface:
34 chapter 2 passing code with behavior parameterization

(continued)
public interface appleformatter {
string accept(apple a);
}

you can now represent multiple formatting behaviors by implementing the apple-
formatter interface:
public class applefancyformatter implements appleformatter {
public string accept(apple apple) {
string characteristic = apple. Getweight() > 150 ? "heavy" :
"light";
return "a " + characteristic +
" " + apple. Getcolor() +" apple";
}
}
public class applesimpleformatter implements appleformatter {
public string accept(apple apple) {
return "an apple of " + apple. Getweight() + "g";
}
}

finally, you need to tell your prettyprintapple method to take appleformatter
objects and use them internally. You can do this by adding a parameter to pretty-
printapple:
public static void prettyprintapple(list<apple> inventory,
appleformatter formatter) {
for(apple apple: inventory) {
string output = formatter. Accept(apple);
system. Out. Println(output);
}
}

bingo! Youre now able to pass multiple behaviors to your prettyprintapple
method. You do this by instantiating implementations of appleformatter and giving
them as arguments to prettyprintapple:
prettyprintapple(inventory, new applefancyformatter());

this will produce an output along the lines of
a light green apple
a heavy red apple
...

Or try this:
prettyprintapple(inventory, new applesimpleformatter());

this will produce an output along the lines of
an apple of 80g
an apple of 155g
...
Tackling verbosity 35

youve seen that you can abstract over behavior and make your code adapt to require-
ment changes, but the process is verbose because you need to declare multiple classes
that you instantiate only once. Lets see how to improve that.

2.3 tackling verbosity
we all know that a feature or concept thats cumbersome to use will be avoided. At the
moment, when you want to pass new behavior to your filterapples method, youre
forced to declare several classes that implement the applepredicate interface and
then instantiate several applepredicate objects that you allocate only once, as shown
in the following listing that summarizes what youve seen so far. Theres a lot of ver-
bosity involved and its a time-consuming process!

Listing 2.1 behavior parameterization: filtering apples with predicates

public class appleheavyweightpredicate implements applepredicate {
public boolean test(apple apple) {
return apple. Getweight() > 150; selects heavy apples
}
}
public class applegreencolorpredicate implements applepredicate {
public boolean test(apple apple) {
return green. Equals(apple. Getcolor()); selects green apples
}
}
public class filteringapples {
public static void main(string... Args) {
list<apple> inventory = arrays. Aslist(new apple(80, green),
new apple(155, green),
results in a list new apple(120, red));
containing one list<apple> heavyapples =
apple of 155 g
filterapples(inventory, new appleheavyweightpredicate());
list<apple> greenapples =
filterapples(inventory, new applegreencolorpredicate()); results
} in a list
public static list<apple> filterapples(list<apple> inventory, containing
applepredicate p) { two green
list<apple> result = new arraylist<>(); apples
for (apple apple : inventory) {
if (p. Test(apple)){
result. Add(apple);
}
}
return result;
}
}

this is unnecessary overhead. Can you do better? Java has mechanisms called anony-
mous classes, which let you declare and instantiate a class at the same time. They enable
you to improve your code one step further by making it a little more concise. But
36 chapter 2 passing code with behavior parameterization

theyre not entirely satisfactory. Section 2.3.3 anticipates the next chapter with a short
preview of how lambda expressions can make your code more readable.

2.3.1 anonymous classes
anonymous classes are like the local classes (a class defined in a block) that youre
already familiar with in java. But anonymous classes dont have a name. They allow
you to declare and instantiate a class at the same time. In short, they allow you to cre-
ate ad hoc implementations.

2.3.2 fifth attempt: using an anonymous class
the following code shows how to rewrite the filtering example by creating an object
that implements applepredicate using an anonymous class:

list<apple> redapples = filterapples(inventory, new applepredicate() {
public boolean test(apple apple){
return red. Equals(apple. Getcolor()); parameterizes the
} behavior of the method
}); filterapples with an
anonymous class.

Anonymous classes are often used in the context of gui applications to create event-
handler objects. We dont want to bring back painful memories of swing, but the fol-
lowing is a common pattern that you see in practice (here using the javafx api, a
modern ui platform for java):
button. Setonaction(new eventhandler<actionevent>() {
public void handle(actionevent event) {
system. Out. Println("whoooo a click! ! ");
}
});

but anonymous classes are still not good enough. First, they tend to be bulky because
they take a lot of space, as shown in the boldface code here using the same two exam-
ples used previously:

list<apple> redapples = filterapples(inventory, new applepredicate() {
public boolean test(apple a){
return red. Equals(a. Getcolor()); lots of
} boilerplate code
});
button. Setonaction(new eventhandler<actionevent>() {
public void handle(actionevent event) {
system. Out. Println("whoooo a click! ! ");
}

second, many programmers find them confusing to use. For example, quiz 2.2 shows
a classic java puzzler that catches most programmers off guard! Try your hand at it.
Tackling verbosity 37

quiz 2.2: anonymous class puzzler
what will the output be when this code is executed: 4, 5, 6, or 42?
Public class meaningofthis {
public final int value = 4;
public void doit() {
int value = 6;
runnable r = new runnable() {
public final int value = 5;
public void run(){
int value = 10;
system. Out. Println(this. Value);
}
};
r. Run();
}
public static void main(string... Args) {
meaningofthis m = new meaningofthis();
m. Doit();
} whats the output
} of this line?

Answer:
the answer is 5, because this refers to the enclosing runnable, not the enclosing
class meaningofthis.

Verbosity in general is bad; it discourages the use of a language feature because it
takes a long time to write and maintain verbose code, and its not pleasant to read!
Good code should be easy to comprehend at a glance. Even though anonymous
classes somewhat tackle the verbosity associated with declaring multiple concrete
classes for an interface, theyre still unsatisfactory. In the context of passing a simple
piece of code (for example, a boolean expression representing a selection criterion),
you still have to create an object and explicitly implement a method to define a new
behavior (for example, the method test for predicate or the method handle for
eventhandler).
Ideally wed like to encourage programmers to use the behavior parameterization
pattern, because as youve just seen, it makes your code more adaptive to requirement
changes. In chapter 3, youll see that the java 8 language designers solved this prob-
lem by introducing lambda expressions, a more concise way to pass code. Enough sus-
pense; heres a short preview of how lambda expressions can help you in your quest
for clean code.

2.3.3 sixth attempt: using a lambda expression
the previous code can be rewritten as follows in java 8 using a lambda expression:
list<apple> result =
filterapples(inventory, (apple apple) -> red. Equals(apple. Getcolor()));
38 chapter 2 passing code with behavior parameterization

you have to admit this code looks a lot cleaner than our previous attempts! Its great
because its starting to look a lot closer to the problem statement. Weve now tackled
the verbosity issue. Figure 2.4 summarizes our journey so far.

Behavior parameterization

flexible anonymous
classes lambdas
classes

value parameterization

rigid

verbose concise

figure 2.4 behavior parameterization versus value parameterization

2.3.4 seventh attempt: abstracting over list type
theres one more step that you can do in your journey toward abstraction. At the
moment, the filterapples method works only for apple. But you can also abstract
on the list type to go beyond the problem domain youre thinking of, as shown:
public interface predicate<t> {
boolean test(t t);
}
public static <t> list<t> filter(list<t> list, predicate<t> p) {
list<t> result = new arraylist<>();
for(t e: list) { introduces a type
if(p. Test(e)) { parameter t
result. Add(e);
}
}
return result;
}

you can now use the method filter with a list of bananas, oranges, integers, or
strings! Heres an example, using lambda expressions:
list<apple> redapples =
filter(inventory, (apple apple) -> red. Equals(apple. Getcolor()));
list<integer> evennumbers =
filter(numbers, (integer i) -> i % 2 == 0);
real-world examples 39

isnt it cool? Youve managed to find the sweet spot between flexibility and concise-
ness, which wasnt possible prior to java 8!

2.4 real-world examples
youve now seen that behavior parameterization is a useful pattern to easily adapt to
changing requirements. This pattern lets you encapsulate a behavior (a piece of code)
and parameterize the behavior of methods by passing and using these behaviors you
create (for example, different predicates for an apple). We mentioned earlier that
this approach is similar to the strategy design pattern. You may have already used this
pattern in practice. Many methods in the java api can be parameterized with different
behaviors. These methods are often used together with anonymous classes. We show
four examples, which should solidify the idea of passing code for you: sorting with a
comparator, executing a block of code with runnable, returning a result from a task
using callable, and gui event handling.

2.4.1 sorting with a comparator
sorting a collection is a recurring programming task. For example, say your farmer
wants you to sort the inventory of apples based on their weight. Or perhaps he
changes his mind and wants you to sort the apples by color. Sound familiar? Yes, you
need a way to represent and use different sorting behaviors to easily adapt to changing
requirements.
From java 8, a list comes with a sort method (you could also use collections
. Sort). The behavior of sort can be parameterized using a java. Util. Comparator
object, which has the following interface:
// java. Util. Comparator
public interface comparator<t> {
int compare(t o1, t o2);
}

you can therefore create different behaviors for the sort method by creating an ad
hoc implementation of comparator. For example, you can use it to sort the inventory
by increasing weight using an anonymous class:
inventory. Sort(new comparator<apple>() {
public int compare(apple a1, apple a2) {
return a1. Getweight(). Compareto(a2. Getweight());
}
});

if the farmer changes his mind about how to sort apples, you can create an ad hoc
comparator to match the new requirement and pass it to the sort method. The inter-
nal details of how to sort are abstracted away. With a lambda expression it would look
like this:
inventory. Sort(
(apple a1, apple a2) -> a1. Getweight(). Compareto(a2. Getweight()));
40 chapter 2 passing code with behavior parameterization

again, dont worry about this new syntax for now; the next chapter covers in detail
how to write and use lambda expressions.

2.4.2 executing a block of code with runnable
java threads allow a block of code to be executed concurrently with the rest of the pro-
gram. But how can you tell a thread what block of code it should run? Several threads
may each run different code. What you need is a way to represent a piece of code to
be executed later. Until java 8, only objects could be passed to the thread construc-
tor, so the typical clumsy usage pattern was to pass an anonymous class containing a
run method that returns void (no result). Such anonymous classes implement the
runnable interface.
In java, you can use the runnable interface to represent a block of code to be exe-
cuted; note that the code returns void (no result):
// java. Lang. Runnable
public interface runnable {
void run();
}

you can use this interface to create threads with your choice of behavior, as follows:
thread t = new thread(new runnable() {
public void run() {
system. Out. Println("hello world");
}
});

but since java 8 you can use a lambda expression, so the call to thread would look
like this:

thread t = new thread(() -> system. Out. Println("hello world"));

2.4.3 returning a result using callable
you may be familiar with the executorservice abstraction that was introduced in
java 5. The executorservice interface decouples how tasks are submitted and exe-
cuted. Whats useful in comparison to using threads and runnable is that by using an
executorservice you can send a task to a pool of threads and have its result stored in
a future. Dont worry if this is unfamiliar, we will revisit this topic in later chapters
when we discuss concurrency in more detail. For now, all you need to know is that the
callable interface is used to model a task that returns a result. You can see it as an
upgraded runnable:
// java. Util. Concurrent. Callable
public interface callable<v> {
v call();
}
summary 41

you can use it, as follows, by submitting a task to an executor service. Here you return
the name of the thread that is responsible for executing the task:
executorservice executorservice = executors. Newcachedthreadpool();
future<string> threadname = executorservice. Submit(new callable<string>() {
@override
public string call() throws exception {
return thread. Currentthread(). Getname();
}
});

using a lambda expression, this code simplifies to the following:
future<string> threadname = executorservice. Submit(
() -> thread. Currentthread(). Getname());

2.4.4 gui event handling
a typical pattern in gui programming is to perform an action in response to a certain
event such as clicking or hovering over text. For example, if the user clicks the send
button, you may wish to display a pop up or perhaps log the action in a file. Again, you
need a way to cope with changes; you should be able to perform any response. In
javafx, you can use an eventhandler to represent a response to an event by passing it
to setonaction:
button button = new button("send");
button. Setonaction(new eventhandler<actionevent>() {
public void handle(actionevent event) {
label. Settext("sent! ! ");
}
});

here, the behavior of the setonaction method is parameterized with eventhandler
objects. With a lambda expression it would look like the following:

button. Setonaction((actionevent event) -> label. Settext("sent! ! "));

summary
 behavior parameterization is the ability for a method to take multiple different
behaviors as parameters and use them internally to accomplish different behaviors.
 Behavior parameterization lets you make your code more adaptive to changing
requirements and saves on engineering efforts in the future.
 Passing code is a way to give new behaviors as arguments to a method. But its
verbose prior to java 8. Anonymous classes helped a bit before java 8 to get rid
of the verbosity associated with declaring multiple concrete classes for an inter-
face that are needed only once.
 The java api contains many methods that can be parameterized with different
behaviors, which include sorting, threads, and gui handling.
Lambda expressions

this chapter covers
 lambdas in a nutshell
 where and how to use lambdas
 the execute-around pattern
 functional interfaces, type inference
 method references
 composing lambdas

in the previous chapter, you saw that passing code with behavior parameterization
is useful for coping with frequent requirement changes in your code. It lets you
define a block of code that represents a behavior and then pass it around. You can
decide to run that block of code when a certain event happens (for example, a but-
ton click) or at certain points in an algorithm (for example, a predicate such as
only apples heavier than 150 g in the filtering algorithm or the customized com-
parison operation in sorting). In general, using this concept you can write code
thats more flexible and reusable.
But you saw that using anonymous classes to represent different behaviors is
unsatisfying. Its verbose, which doesnt encourage programmers to use behavior
parameterization in practice. In this chapter, well teach you about a new feature in

42
lambdas in a nutshell 43

java 8 that tackles this problem: lambda expressions. They let you represent a behav-
ior or pass code in a concise way. For now you can think of lambda expressions as
anonymous functions, methods without declared names, but which can also be passed
as arguments to a method as you can with an anonymous class.
Well show how to construct them, where to use them, and how you can make your
code more concise by using them. We also explain some new goodies such as type
inference and new important interfaces available in the java 8 api. Finally, we intro-
duce method references, a useful new feature that goes hand in hand with lambda
expressions.
This chapter is organized in such a way as to teach you step-by-step how to write
more concise and flexible code. At the end of this chapter, we bring together all the
concepts taught into a concrete example; we take the sorting example shown in chap-
ter 2 and gradually improve it using lambda expressions and method references to
make it more concise and readable. This chapter is important in itself and also
because youll use lambdas extensively throughout the book.

3.1 lambdas in a nutshell
a lambda expression can be understood as a concise representation of an anonymous
function that can be passed around. It doesnt have a name, but it has a list of parame-
ters, a body, a return type, and also possibly a list of exceptions that can be thrown.
Thats one big definition; lets break it down:
 anonymouswe say anonymous because it doesnt have an explicit name like a
method would normally have; less to write and think about!
 Functionwe say function because a lambda isnt associated with a particular
class like a method is. But like a method, a lambda has a list of parameters, a
body, a return type, and a possible list of exceptions that can be thrown.
 Passed arounda lambda expression can be passed as argument to a method or
stored in a variable.
 Conciseyou dont need to write a lot of boilerplate like you do for anonymous
classes.
If youre wondering where the term lambda comes from, it originates from a system
developed in academia called lambda calculus, which is used to describe computations.
Why should you care about lambda expressions? You saw in the previous chapter
that passing code is currently tedious and verbose in java. Well, good news! Lambdas
fix this problem; they let you pass code in a concise way. Lambdas technically dont let
you do anything that you couldnt do prior to java 8. But you no longer have to write
clumsy code using anonymous classes to benefit from behavior parameterization!
Lambda expressions will encourage you to adopt the style of behavior parameteriza-
tion that we described in the previous chapter. The net result is that your code will be
clearer and more flexible. For example, using a lambda expression you can create a
custom comparator object in a more concise way.
44 chapter 3 lambda expressions

before:
comparator<apple> byweight = new comparator<apple>() {
public int compare(apple a1, apple a2){
return a1. Getweight(). Compareto(a2. Getweight());
}
};

after (with lambda expressions):
comparator<apple> byweight =
(apple a1, apple a2) -> a1. Getweight(). Compareto(a2. Getweight());

you must admit that the code looks clearer! Dont worry if all the parts of the lambda
expression dont make sense yet; well explain all the pieces soon. For now, note that
youre literally passing only the code thats needed to compare two apples using their
weight. It looks like youre passing the body of the method compare. Youll learn soon
that you can simplify your code even more. Well explain in the next section exactly
where and how you can use lambda expressions.
The lambda we just showed you has three parts, as shown in figure 3.1:
arrow

(apple a1, apple a2) -> a1. Getweight(). Compareto(a2. Getweight());

lambda lambda
parameters body

figure 3.1 a lambda expression is composed of parameters, an arrow, and a body.

 A list of parametersin this case it mirrors the parameters of the compare method
of a comparatortwo apples.
 An arrowthe arrow -> separates the list of parameters from the body of
the lambda.
 The body of the lambdacompare two apples using their weights. The expression
is considered the lambdas return value.
To illustrate further, the following listing shows five examples of valid lambda expres-
sions in java 8.

Listing 3.1 valid lambda expressions in java 8
takes one parameter of type string and returns an int.
(string s) -> s. Length() it has no return statement as return is implied.
(apple a) -> a. Getweight() > 150
(int x, int y) -> { takes one parameter of type apple and returns a
system. Out. Println("result: "); boolean (whether the apple is heavier than 150 g).
System. Out. Println(x + y);
}
takes two parameters of type int and returns no value
(void return). Its body contains two statements.
Lambdas in a nutshell 45

() -> 42
(apple a1, apple a2) -> a1. Getweight(). Compareto(a2. Getweight())

takes two parameters of type apple and returns an takes no parameter
int representing the comparison of their weights and returns the int 42

this syntax was chosen by the java language designers because it was well received in
other languages, such as c# and scala. Javascript has a similar syntax. The basic syntax
of a lambda is either (referred to as an expression-style lambda)

(parameters) -> expression

or (note the curly braces for statements, this lambda is often called a block-style
lambda)

(parameters) -> { statements; }

as you can see, lambda expressions follow a simple syntax. Working through quiz 3.1
should let you know if you understand the pattern.

Quiz 3.1: lambda syntax
based on the syntax rules just shown, which of the following are not valid lambda
expressions?
1 () -> {}
2 () -> "raoul"
3 () -> { return "mario"; }
4 (integer i) -> return "alan" + i;
5 (string s) -> { "iron man"; }
answer:
4 and 5 are invalid lambdas; the rest are valid. Details:
1 this lambda has no parameters and returns void. Its similar to a method
with an empty body: public void run() { }. Fun fact: this is usually called
the burger lambda. Take a look at it from the side, and you will see it has a
burger shape with two buns.
2 this lambda has no parameters and returns a string as an expression.
3 this lambda has no parameters and returns a string (using an explicit
return statement, within a block).
4 return is a control-flow statement. To make this lambda valid, curly braces
are required as follows: (integer i) -> { return "alan" + i; }.
5 iron man is an expression, not a statement. To make this lambda valid, you
can remove the curly braces and semicolon as follows: (string s) -> "iron
man". Or if you prefer, you can use an explicit return statement as follows:
(string s) -> { return "iron man"; }.
46 chapter 3 lambda expressions

table 3.1 this provides a list of example lambdas with examples of use cases.

Table 3.1 examples of lambdas

use case examples of lambdas

a boolean expression (list<string> list) -> list. Isempty()

creating objects () -> new apple(10)

consuming from an object (apple a) -> {
system. Out. Println(a. Getweight());
}

select/extract from an object (string s) -> s. Length()

combine two values (int a, int b) -> a * b

compare two objects (apple a1, apple a2) ->
a1. Getweight(). Compareto(a2. Getweight())

3.2 where and how to use lambdas
you may now be wondering where youre allowed to use lambda expressions. In the
previous example, you assigned a lambda to a variable of type comparator<apple>.
You could also use another lambda with the filter method you implemented in the
previous chapter:

list<apple> greenapples =
filter(inventory, (apple a) -> green. Equals(a. Getcolor()));

where exactly can you use lambdas? You can use a lambda expression in the context
of a functional interface. In the code shown here, you can pass a lambda as second
argument to the method filter because it expects an object of type predicate<t>,
which is a functional interface. Dont worry if this sounds abstract; well now explain
in detail what this means and what a functional interface is.

3.2.1 functional interface
remember the interface predicate<t> you created in chapter 2 so you could parame-
terize the behavior of the filter method? Its a functional interface! Why? Because
predicate specifies only one abstract method:

public interface predicate<t> {
boolean test (t t);
}

in a nutshell, a functional interface is an interface that specifies exactly one abstract
method. You already know several other functional interfaces in the java api such as
comparator and runnable, which we explored in chapter 2:
where and how to use lambdas 47

public interface comparator<t> { java. Util. Comparator
int compare(t o1, t o2);
}
public interface runnable { java. Lang. Runnable
void run();
}
public interface actionlistener extends eventlistener { java. Awt. Event. Actionlistener
void actionperformed(actionevent e);
}
public interface callable<v> { java. Util. Concurrent. Callable
v call() throws exception;
}
public interface privilegedaction<t> { java. Security. Privilegedaction
t run();
}

note youll see in chapter 13 that interfaces can now also have default meth-
ods (a method with a body that provides some default implementation for a
method in case it isnt implemented by a class). An interface is still a func-
tional interface if it has many default methods as long as it specifies only one
abstract method.

To check your understanding, quiz 3.2 should let you know if you grasp the concept of
a functional interface.

Quiz 3.2: functional interface
which of these interfaces are functional interfaces?
Public interface adder {
int add(int a, int b);
}
public interface smartadder extends adder {
int add(double a, double b);
}
public interface nothing {
}

answer:
only adder is a functional interface.
Smartadder isnt a functional interface because it specifies two abstract methods
called add (one is inherited from adder).
Nothing isnt a functional interface because it declares no abstract method at all.

What can you do with functional interfaces? Lambda expressions let you provide the
implementation of the abstract method of a functional interface directly inline and
treat the whole expression as an instance of a functional interface (more technically speaking,
an instance of a concrete implementation of the functional interface). You can achieve
48 chapter 3 lambda expressions

the same thing with an anonymous inner class, although its clumsier: you provide an
implementation and instantiate it directly inline. The following code is valid because
runnable is a functional interface defining only one abstract method, run:

runnable r1 = () -> system. Out. Println("hello world 1"); uses a lambda
runnable r2 = new runnable() {
public void run() { uses an
system. Out. Println("hello world 2"); anonymous class
}
};
public static void process(runnable r) { prints hello
r. Run(); world 2
} prints hello
process(r1); world 1 prints hello world 3
process(r2); with a lambda passed
process(() -> system. Out. Println("hello world 3")); directly

3.2.2 function descriptor
the signature of the abstract method of the functional interface describes the signa-
ture of the lambda expression. We call this abstract method a function descriptor. For
example, the runnable interface can be viewed as the signature of a function that
accepts nothing and returns nothing (void) because it has only one abstract method
called run, which accepts nothing and returns nothing (void).1
we use a special notation throughout this chapter to describe the signatures of
lambdas and functional interfaces. The notation () -> void represents a function with
an empty list of parameters returning void. This is exactly what the runnable inter-
face represents. As another example, (apple, apple) -> int denotes a function tak-
ing two apples as parameters and returning an int. We will provide more information
about function descriptors in section 3.4 and table 3.2 later in the chapter.
You may already be wondering how lambda expressions are type checked. We
detail how the compiler checks whether a lambda is valid in a given context in sec-
tion 3.5. For now, it suffices to understand that a lambda expression can be assigned
to a variable or passed to a method expecting a functional interface as argument, pro-
vided the lambda expression has the same signature as the abstract method of the
functional interface. For instance, in our earlier example, you could pass a lambda
directly to the process method as follows:
public void process(runnable r) {
r. Run();
}
process(() -> system. Out. Println("this is awesome! ! "));

1
some languages such as scala provide explicit type annotations in their type system to describe the type of a
function (called function types). Java reuses existing nominal types provided by functional interfaces and
maps them into a form of function types behind the scenes.
Where and how to use lambdas 49

this code when executed will print this is awesome! ! The lambda expression () ->
system. Out. Println("this is awesome! ! ") takes no parameters and returns void.
This is exactly the signature of the run method defined in the runnable interface.

Lambdas and void method invocation
although this may feel weird, the following lambda expression is valid:
process(() -> system. Out. Println("this is awesome"));

after all, system. Out. Println returns void so this is clearly not an expression! Why
dont we have to enclose the body with curly braces like this?
Process(() -> { system. Out. Println("this is awesome"); });

it turns out that theres a special rule for a void method invocation defined in the java
language specification. You dont have to enclose a single void method invocation
in braces.

You may be wondering, why can we pass a lambda only where a functional interface
is expected? The language designers considered alternative approaches such as add-
ing function types (a bit like the special notation we introduced to describe the signa-
ture of lambda expressionswell revisit this topic in chapters 20 and 21) to java. But
they chose this way because it fits naturally without increasing the complexity of the
language. In addition, most java programmers are already familiar with the idea of an
interface with a single abstract method (for example, for event handling). However,
the most important reason is that functional interfaces were already extensively used
before java 8. This means that they provide a nice migration path for using lambda
expressions. In fact, if youve been using functional interfaces such as comparator
and runnable or even your own interfaces that happen to define only a single abstract
method, you can now use lambda expressions without changing your apis. Try quiz
3.3 to test your knowledge of where lambdas can be used.

Quiz 3.3: where can you use lambdas?
Which of the following are valid uses of lambda expressions?
1 execute(() -> {});
public void execute(runnable r) {
r. Run();
}

2 public callable<string> fetch() {
return () -> "tricky example ; -)";
}

3 predicate<apple> p = (apple a) -> a. Getweight();
50 chapter 3 lambda expressions

(continued)
answer:
only 1 and 2 are valid.
The first example is valid because the lambda () -> {} has the signature () -> void,
which matches the signature of the abstract method run defined in runnable. Note
that running this code will do nothing because the body of the lambda is empty!
The second example is also valid. Indeed, the return type of the method fetch is
callable<string>. Callable<string> defines a method with the signature () ->
string when t is replaced with string. Because the lambda () -> "tricky example
; -)" has the signature () -> string, the lambda can be used in this context.
The third example is invalid because the lambda expression (apple a) -> a. Get-
weight() has the signature (apple) -> integer, which is different from the signa-
ture of the method test defined in predicate<apple>: (apple) -> boolean.

What about @functionalinterface?
If you explore the new java api, you will notice that functional interfaces are generally
annotated with @functionalinterface. (we show an extensive list in section 3.4,
where we explore how to use functional interfaces in depth. ) this annotation is
used to indicate that the interface is intended to be a functional interface and is
therefore useful for documentation. In addition, the compiler will return a meaning-
ful error if you define an interface using the @functionalinterface annotation,
and it isnt a functional interface. For example, an error message could be multiple
non-overriding abstract methods found in interface foo to indicate that more than
one abstract method is available. Note that the @functionalinterface annota-
tion isnt mandatory, but its good practice to use it when an interface is designed
for that purpose. You can think of it like the @override notation to indicate that a
method is overridden.

3.3 putting lambdas into practice:
the execute-around pattern
lets look at an example of how lambdas, together with behavior parameterization,
can be used in practice to make your code more flexible and concise. A recurrent
pattern in resource processing (for example, dealing with files or databases) is to
open a resource, do some processing on it, and then close the resource. The setup
and cleanup phases are always similar and surround the important code doing the
processing. This is called the execute-around pattern, as illustrated in figure 3.2. For
example, in the following code, the highlighted lines show the boilerplate code
required to read one line from a file (note also that you use java 7s try-with-resources
statement, which already simplifies the code, because you dont have to close the
resource explicitly):
putting lambdas into practice: the execute-around pattern 51

public string processfile() throws ioexception {
try (bufferedreader br =
new bufferedreader(new filereader("data. Txt"))) {
return br. Readline();
this is the line that
} does useful work.
}

init/preparation code init/preparation code

task a task b

cleanup/finishing code cleanup/finishing code

figure 3.2 tasks a and b are surrounded by boilerplate code
responsible for preparation/cleanup.

3.3.1 step 1: remember behavior parameterization
this current code is limited. You can read only the first line of the file. What if youd
like to return the first two lines instead or even the word used most frequently? Ideally,
youd like to reuse the code doing setup and cleanup and tell the processfile
method to perform different actions on the file. Does this sound familiar? Yes, you
need to parameterize the behavior of processfile. You need a way to pass behavior to
processfile so it can execute different behaviors using a bufferedreader.
Passing behavior is exactly what lambdas are for. What should the new process-
file method look like if you want to read two lines at once? You need a lambda that
takes a bufferedreader and returns a string. For example, heres how to print two
lines of a bufferedreader:
string result
= processfile((bufferedreader br) -> br. Readline() + br. Readline());

3.3.2 step 2: use a functional interface to pass behaviors
we explained earlier that lambdas can be used only in the context of a functional
interface. You need to create one that matches the signature bufferedreader ->
string and that may throw an ioexception. Lets call this interface bufferedreader-
processor:

@functionalinterface
public interface bufferedreaderprocessor {
string process(bufferedreader b) throws ioexception;
}
52 chapter 3 lambda expressions

you can now use this interface as the argument to your new processfile method:
public string processfile(bufferedreaderprocessor p) throws ioexception {
...
}

3.3.3 step 3: execute a behavior!
Any lambdas of the form bufferedreader -> string can be passed as arguments,
because they match the signature of the process method defined in the buffered-
readerprocessor interface. You now need only a way to execute the code represented
by the lambda inside the body of processfile. Remember, lambda expressions let
you provide the implementation of the abstract method of a functional interface
directly inline, and they treat the whole expression as an instance of a functional interface.
You can therefore call the method process on the resulting bufferedreaderprocessor
object inside the processfile body to perform the processing:

public string processfile(bufferedreaderprocessor p) throws ioexception {
try (bufferedreader br =
new bufferedreader(new filereader("data. Txt"))) {
return p. Process(br);
} processes the
} bufferedreader object

3.3.4 step 4: pass lambdas
you can now reuse the processfile method and process files in different ways by
passing different lambdas.
The following shows processing one line:
string oneline =
processfile((bufferedreader br) -> br. Readline());

the following shows processing two lines:
string twolines =
processfile((bufferedreader br) -> br. Readline() + br. Readline());

figure 3.3 summarizes the four steps taken to make the processfile method
more flexible.
Weve shown how you can make use of functional interfaces to pass lambdas. But
you had to define your own interfaces. In the next section, we explore new interfaces
that were added to java 8 that you can reuse to pass multiple different lambdas.
Using functional interfaces 53

public string processfile() throws ioexception { 1
try (bufferedreader br =
new bufferedreader(new filereader("data. Txt"))){
return br. Readline();
}
}

public interface bufferedreaderprocessor {
2
string process(bufferedreader b) throws ioexception;
}

public string processfile(bufferedreaderprocessor p) throws
ioexception {
...
}

public string processfile(bufferedreaderprocessor p)
3
throws ioexception {
try (bufferedreader br =
new bufferedreader(new filereader("data. Txt"))){
return p. Process(br);
}
}

string oneline = processfile((bufferedreader br) ->
4
br. Readline());

string twolines = processfile((bufferedreader br) ->
br. Readline + br. Readline());

figure 3.3 four-step process to apply the execute-around pattern

3.4 using functional interfaces
as you learned in section 3.2.1, a functional interface specifies exactly one abstract
method. Functional interfaces are useful because the signature of the abstract method
can describe the signature of a lambda expression. The signature of the abstract
method of a functional interface is called a function descriptor. In order to use different
lambda expressions, you need a set of functional interfaces that can describe common
function descriptors. Several functional interfaces are already available in the java api
such as comparable, runnable, and callable, which you saw in section 3.2.
The java library designers for java 8 have helped you by introducing several new
functional interfaces inside the java. Util. Function package. Well describe the
interfaces predicate, consumer, and function next. A more complete list is available
in table 3.2 at the end of this section.
54 chapter 3 lambda expressions

3.4.1 predicate
the java. Util. Function. Predicate<t> interface defines an abstract method named
test that accepts an object of generic type t and returns a boolean. Its exactly the
same one that you created earlier, but its available out of the box! You might want to
use this interface when you need to represent a boolean expression that uses an
object of type t. For example, you can define a lambda that accepts string objects, as
shown in the following listing.

Listing 3.2 working with a predicate

@functionalinterface
public interface predicate<t> {
boolean test(t t);
}
public <t> list<t> filter(list<t> list, predicate<t> p) {
list<t> results = new arraylist<>();
for(t t: list) {
if(p. Test(t)) {
results. Add(t);
}
}
return results;
}
predicate<string> nonemptystringpredicate = (string s) -> ! S. Isempty();
list<string> nonempty = filter(listofstrings, nonemptystringpredicate);

if you look up the javadoc specification of the predicate interface, you may notice
additional methods such as and and or. Dont worry about them for now. Well come
back to these in section 3.8.

3.4.2 consumer
the java. Util. Function. Consumer<t> interface defines an abstract method named
accept that takes an object of generic type t and returns no result (void). You might
use this interface when you need to access an object of type t and perform some oper-
ations on it. For example, you can use it to create a method foreach, which takes a list
of integers and applies an operation on each element of that list. In the following
listing, youll use this foreach method combined with a lambda to print all the ele-
ments of the list.

Listing 3.3 working with a consumer

@functionalinterface
public interface consumer<t> {
void accept(t t);
}
public <t> void foreach(list<t> list, consumer<t> c) {
for(t t: list) {
c. Accept(t);
}
using functional interfaces 55

}
foreach( the lambda is the implementation
arrays. Aslist(1,2,3,4,5), of the accept method from
(integer i) -> system. Out. Println(i) consumer.
);

3.4.3 function
the java. Util. Function. Function<t, r> interface defines an abstract method
named apply that takes an object of generic type t as input and returns an object of
generic type r. You might use this interface when you need to define a lambda that
maps information from an input object to an output (for example, extracting the
weight of an apple or mapping a string to its length). In the listing that follows, we
show how you can use it to create a method map to transform a list of strings into a list
of integers containing the length of each string.

Listing 3.4 working with a function

@functionalinterface
public interface function<t, r> {
r apply(t t);
}
public <t, r> list<r> map(list<t> list, function<t, r> f) {
list<r> result = new arraylist<>();
for(t t: list) {
result. Add(f. Apply(t));
}
return result;
}
// [7, 2, 6] implements
list<integer> l = map( the apply
arrays. Aslist("lambdas", "in", "action"), method of
(string s) -> s. Length() function
);

primitive specializations
we described three functional interfaces that are generic: predicate<t>, consumer<t>,
and function<t, r>. There are also functional interfaces that are specialized with cer-
tain types.
To refresh a little: every java type is either a reference type (for example, byte,
integer, object, list) or a primitive type (for example, int, double, byte, char).
But generic parameters (for example, the t in consumer<t>) can be bound only to
reference types. This is due to how generics are internally implemented.2 as a result,
in java theres a mechanism to convert a primitive type into a corresponding refer-
ence type. This mechanism is called boxing. The opposite approach (converting a

2
some other languages, such as c#, dont have this restriction. Other languages, such as scala, have only ref-
erence types. We revisit this issue in chapter 20.
56 chapter 3 lambda expressions

reference type into a corresponding primitive type) is called unboxing. Java also has an
autoboxing mechanism to facilitate the task for programmers: boxing and unboxing
operations are done automatically. For example, this is why the following code is valid
(an int gets boxed to an integer):
list<integer> list = new arraylist<>();
for (int i = 300; i < 400; i++){
list. Add(i);
}

but this comes with a performance cost. Boxed values are a wrapper around primitive
types and are stored on the heap. Therefore, boxed values use more memory and
require additional memory lookups to fetch the wrapped primitive value.
Java 8 also added a specialized version of the functional interfaces we described
earlier in order to avoid autoboxing operations when the inputs or outputs are primi-
tives. For example, in the following code, using an intpredicate avoids a boxing
operation of the value 1000, whereas using a predicate<integer> would box the
argument 1000 to an integer object:

public interface intpredicate {
boolean test(int t);
}
intpredicate evennumbers = (int i) -> i % 2 == 0; true (no boxing)
evennumbers. Test(1000);
predicate<integer> oddnumbers = (integer i) -> i % 2 ! = 0;
oddnumbers. Test(1000); false (boxing)

in general, the appropriate primitive type precedes the names of functional interfaces
that have a specialization for the input type parameter (for example, doublepredicate,
intconsumer, longbinaryoperator, intfunction, and so on). The function interface
also has variants for the output type parameter: tointfunction<t>, inttodouble-
function, and so on.
Table 3.2 summarizes the most commonly used functional interfaces available in
the java api and their function descriptors, along with their primitive specializations.
Keep in mind that these are only a starter kit, and you can always create your own if
needed (quiz 3.7 invents trifunction for this purpose). Creating your own interfaces
can also help when a domain-specific name will help with program comprehension
and maintenance. Remember, the notation (t, u) -> r shows how to think about a
function descriptor. The left side of the arrow is a list representing the types of the
arguments, and the right side represents the types of the results. In this case, it rep-
resents a function with two arguments of respectively generic type t and u and that has
a return type of r.
Youve now seen a lot of functional interfaces that can be used to describe the
signature of various lambda expressions. To check your understanding so far, try
quiz 3.4.
Using functional interfaces 57

table 3.2 common functional interfaces added in java 8

functional interface predicate<t> consumer<t>

predicate<t> t -> boolean intpredicate, longpredicate,
doublepredicate

consumer<t> t -> void intconsumer, longconsumer,
doubleconsumer

function<t, r> t -> r intfunction<r>,
inttodoublefunction,
inttolongfunction,
longfunction<r>,
longtodoublefunction,
longtointfunction,
doublefunction<r>,
doubletointfunction,
doubletolongfunction,
tointfunction<t>,
todoublefunction<t>,
tolongfunction<t>

supplier<t> () -> t booleansupplier, intsupplier,
longsupplier, doublesupplier

unaryoperator<t> t -> t intunaryoperator,
longunaryoperator,
doubleunaryoperator

binaryoperator<t> (t, t) -> t intbinaryoperator,
longbinaryoperator,
doublebinaryoperator

bipredicate<t, u> (t, u) -> boolean

biconsumer<t, u> (t, u) -> void objintconsumer<t>,
objlongconsumer<t>,
objdoubleconsumer<t>

bifunction<t, u, r> (t, u) -> r tointbifunction<t, u>,
tolongbifunction<t, u>,
todoublebifunction<t, u>

quiz 3.4: functional interfaces
what functional interfaces would you use for the following function descriptors
(lambda-expression signatures)? Youll find most of the answers in table 3.2. As a
further exercise, come up with valid lambda expressions that you can use with these
functional interfaces.
1 t -> r
2 (int, int) -> int
3 t -> void
58 chapter 3 lambda expressions

(continued)
4 () -> t
5 (t, u) -> r

answers:
1 function<t, r> is a good candidate. Its typically used for converting an
object of type t into an object of type r (for example, function<apple,
integer> to extract the weight of an apple).
2 intbinaryoperator has a single abstract method called applyasint repre-
senting a function descriptor (int, int) -> int.
3 consumer<t> has a single abstract method called accept representing a
function descriptor t -> void.
4 supplier<t> has a single abstract method called get representing a func-
tion descriptor () -> t.
5 bifunction<t, u, r> has a single abstract method called apply represent-
ing a function descriptor (t, u) -> r.

To summarize the discussion about functional interfaces and lambdas, table 3.3 pro-
vides a summary of use cases, examples of lambdas, and functional interfaces that can
be used.

Table 3.3 examples of lambdas with functional interfaces

use case example of lambda matching functional interface

a boolean (list<string> list) -> list. Isempty() predicate<list<string>>
expression

creating () -> new apple(10) supplier<apple>
objects

consuming (apple a) -> consumer<apple>
from an object system. Out. Println(a. Getweight())

select/extract (string s) -> s. Length() function<string,
from an object integer> or
tointfunction<string>

combine two (int a, int b) -> a * b intbinaryoperator
values

compare two (apple a1, apple a2) -> comparator<apple> or
objects a1. Getweight(). Compareto(a2. Getweight bifunction<apple, apple,
()) integer> or
tointbifunction<apple,
apple>
type checking, type inference, and restrictions 59

what about exceptions, lambdas, and functional interfaces?
Note that none of the functional interfaces allow for a checked exception to be
thrown. You have two options if you need the body of a lambda expression to throw
an exception: define your own functional interface that declares the checked excep-
tion, or wrap the lambda body with a try/catch block.
For example, in section 3.3 we introduced a new functional interface buffered-
readerprocessor that explicitly declared an ioexception:
@functionalinterface
public interface bufferedreaderprocessor {
string process(bufferedreader b) throws ioexception;
}
bufferedreaderprocessor p = (bufferedreader br) -> br. Readline();

but you may be using an api that expects a functional interface such as function<t,
r> and theres no option to create your own. Youll see in the next chapter that the
streams api makes heavy use of the functional interfaces from table 3.2. In this
case, you can explicitly catch the checked exception:
function<bufferedreader, string> f =
(bufferedreader b) -> {
try {
return b. Readline();
}
catch(ioexception e) {
throw new runtimeexception(e);
}
};

youve now seen how to create lambdas and where and how to use them. Next, well
explain some more advanced details: how lambdas are type checked by the compiler
and rules you should be aware of, such as lambdas referencing local variables inside
their body and void-compatible lambdas. Theres no need to fully understand the
next section right away, and you may wish to come back to it later and move on to sec-
tion 3.6 about method references.

3.5 type checking, type inference, and restrictions
when we first mentioned lambda expressions, we said that they let you generate an
instance of a functional interface. Nonetheless, a lambda expression itself doesnt
contain the information about which functional interface its implementing. In order
to have a more formal understanding of lambda expressions, you should know what
the type of a lambda is.

3.5.1 type checking
the type of a lambda is deduced from the context in which the lambda is used. The
type expected for the lambda expression inside the context (for example, a method
parameter that its passed to or a local variable that its assigned to) is called the
60 chapter 3 lambda expressions

target type. Lets look at an example to see what happens behind the scenes when you
use a lambda expression. Figure 3.4 summarizes the type-checking process for the
following code:
list<apple> heavierthan150g =
filter(inventory, (apple apple) -> apple. Getweight() > 150);

the type-checking process is deconstructed as follows:
 first, you look up the declaration of the filter method.
 Second, it expects, as the second formal parameter, an object of type predi-
cate<apple> (the target type).
 Third, predicate<apple> is a functional interface defining a single abstract
method called test.
 Fourth, the test method describes a function descriptor that accepts an apple
and returns a boolean.
 Finally, any argument to the filter method needs to match this requirement.

Filter(inventory, (apple apple) -> apple. Getweight() > 150)

1 whats the context in
which the lambda is
used? Lets first look up
the definition of filter.

Filter(list<apple>inventory, predicate<apple> p)

2 cool, the target type
is predicate<apple>
(t is bound to apple)!

Target type 5 the function descriptor
apple -> boolean matches
the signature of the lambda!
3 whats the abstract it takes an apple and returns
method in the a boolean, so the code
predicate<apple> type checks.
Interface?

Boolean test(apple apple)

4 cool, its test, which
takes an apple and
returns a boolean!

Apple -> boolean

figure 3.4 deconstructing the type-checking process of a lambda expression
type checking, type inference, and restrictions 61

the code is valid because the lambda expression that were passing also takes an apple
as parameter and returns a boolean. Note that if the lambda expression was throwing
an exception, then the declared throws clause of the abstract method would also have
to match.

3.5.2 same lambda, different functional interfaces
because of the idea of target typing, the same lambda expression can be associated with
different functional interfaces if they have a compatible abstract method signature.
For example, both interfaces callable and privilegedaction described earlier rep-
resent functions that accept nothing and return a generic type t. The following two
assignments are therefore valid:
callable<integer> c = () -> 42;
privilegedaction<integer> p = () -> 42;

in this case the first assignment has target type callable<integer> and the second
assignment has target type privilegedaction<integer>.
In table 3.3 we showed a similar example; the same lambda can be used with multi-
ple different functional interfaces:
comparator<apple> c1 =
(apple a1, apple a2) -> a1. Getweight(). Compareto(a2. Getweight());
tointbifunction<apple, apple> c2 =
(apple a1, apple a2) -> a1. Getweight(). Compareto(a2. Getweight());
bifunction<apple, apple, integer> c3 =
(apple a1, apple a2) -> a1. Getweight(). Compareto(a2. Getweight());

diamond operator
those of you who are familiar with javas evolution will recall that java 7 had already
introduced the idea of types being inferred from context with generic inference using
the diamond operator (<>) (this idea can be found even earlier with generic methods).
A given class-instance expression can appear in two or more different contexts, and
the appropriate type argument will be inferred as exemplified here:
list<string> listofstrings = new arraylist<>();
list<integer> listofintegers = new arraylist<>();

special void-compatibility rule
if a lambda has a statement expression as its body, its compatible with a function
descriptor that returns void (provided the parameter list is compatible, too). For
example, both of the following lines are legal even though the method add of a list
returns a boolean and not void as expected in the consumer context (t -> void):
// predicate has a boolean return
predicate<string> p = (string s) -> list. Add(s);
// consumer has a void return
consumer<string> b = (string s) -> list. Add(s);
62 chapter 3 lambda expressions

by now you should have a good understanding of when and where youre allowed to
use lambda expressions. They can get their target type from an assignment context,
method-invocation context (parameters and return), and a cast context. To check
your knowledge, try quiz 3.5.

Quiz 3.5: type checkingwhy wont the following code compile?
How could you fix the problem?

Object o = () -> { system. Out. Println("tricky example"); };

answer:
the context of the lambda expression is object (the target type). But object isnt a
functional interface. To fix this you can change the target type to runnable, which
represents a function descriptor () -> void:

runnable r = () -> { system. Out. Println("tricky example"); };

you could also fix the problem by casting the lambda expression to runnable, which
explicitly provides a target type.

Object o = (runnable) () -> { system. Out. Println("tricky example"); };

this technique can be useful in the context of overloading with a method taking two
different functional interfaces that have the same function descriptor. You can cast
the lambda in order to explicitly disambiguate which method signature should be
selected.
For example, the call execute(() -> {}) using the method execute, as shown in
the following, would be ambiguous, because both runnable and action have the
same function descriptor:
public void execute(runnable runnable) {
runnable. Run();
}
public void execute(action<t> action) {
action. Act();
}
@functionalinterface
interface action {
void act();
}

but, you can explicitly disambiguate the call by using a cast expression: execute
((action) () -> {});

youve seen how the target type can be used to check whether a lambda can be used in
a particular context. It can also be used to do something slightly different: infer the
types of the parameters of a lambda.
Type checking, type inference, and restrictions 63

3.5.3 type inference
you can simplify your code one step further. The java compiler deduces what func-
tional interface to associate with a lambda expression from its surrounding context
(the target type), meaning it can also deduce an appropriate signature for the lambda
because the function descriptor is available through the target type. The benefit is
that the compiler has access to the types of the parameters of a lambda expression,
and they can be omitted in the lambda syntax. The java compiler infers the types of
the parameters of a lambda as shown here:3
no explicit type on
the parameter apple
list<apple> greenapples =
filter(inventory, apple -> green. Equals(apple. Getcolor()));

the benefits of code readability are more noticeable with lambda expressions that
have several parameters. For example, heres how to create a comparator object:
without type inference
comparator<apple> c =
(apple a1, apple a2) -> a1. Getweight(). Compareto(a2. Getweight());
comparator<apple> c =
(a1, a2) -> a1. Getweight(). Compareto(a2. Getweight()); with type inference

note that sometimes its more readable to include the types explicitly, and sometimes
its more readable to exclude them. Theres no rule for which way is better; developers
must make their own choices about what makes their code more readable.

3.5.4 using local variables
all the lambda expressions weve shown so far used only their arguments inside their
body. But lambda expressions are also allowed to use free variables (variables that arent
the parameters and are defined in an outer scope) like anonymous classes can.
Theyre called capturing lambdas. For example, the following lambda captures the vari-
able portnumber:
int portnumber = 1337;
runnable r = () -> system. Out. Println(portnumber);

nonetheless, theres a small twist. There are some restrictions on what you can do with
these variables. Lambdas are allowed to capture (to reference in their bodies)
instance variables and static variables without restrictions. But when local variables are
captured, they have to be explicitly declared final or be effectively final. Lambda
expressions can capture local variables that are assigned to only once. (note: captur-
ing an instance variable can be seen as capturing the final local variable this. ) for

3
note that when a lambda has a single parameter whose type is inferred, the parentheses surrounding the
parameter name can also be omitted.
64 chapter 3 lambda expressions

example, the following code doesnt compile because the variable portnumber is
assigned to twice:

int portnumber = 1337; error: local variable
runnable r = () -> system. Out. Println(portnumber); portnumber is not final
portnumber = 31337; or effectively final.

Restrictions on local variables
you may be asking yourself why local variables have these restrictions. First, theres a
key difference in how instance and local variables are implemented behind the
scenes. Instance variables are stored on the heap, whereas local variables live on the
stack. If a lambda could access the local variable directly and the lambda was used in a
thread, then the thread using the lambda could try to access the variable after the
thread that allocated the variable had deallocated it. Hence, java implements access to
a free local variable as access to a copy of it, rather than access to the original variable.
This makes no difference if the local variable is assigned to only oncehence the
restriction.
Second, this restriction also discourages typical imperative programming patterns
(which, as we explain in later chapters, prevent easy parallelization) that mutate an
outer variable.

Closure
you may have heard of the term closure and may be wondering whether lambdas
meet the definition of a closure (not to be confused with the clojure programming
language). To put it scientifically, a closure is an instance of a function that can
reference nonlocal variables of that function with no restrictions. For example, a
closure could be passed as argument to another function. It could also access and
modify variables defined outside its scope. Now, java 8 lambdas and anonymous
classes do something similar to closures: they can be passed as argument to
methods and can access variables outside their scope. But they have a restriction:
they cant modify the content of local variables of a method in which the lambda is
defined. Those variables have to be implicitly final. It helps to think that lambdas
close over values rather than variables. As explained previously, this restriction
exists because local variables live on the stack and are implicitly confined to the
thread theyre in. Allowing capture of mutable local variables opens new thread-
unsafe possibilities, which are undesirable (instance variables are fine because they
live on the heap, which is shared across threads).

Well now describe another great feature that was introduced in java 8 code: method ref-
erences. Think of them as shorthand versions of certain lambdas.

3.6 method references
method references let you reuse existing method definitions and pass them like
lambdas. In some cases they appear more readable and feel more natural than using
method references 65

lambda expressions. Heres our sorting example written with a method reference
and a bit of help from the updated java 8 api (we explore this example in more detail
in section 3.7).
Before:
inventory. Sort((apple a1, apple a2)
a1. Getweight(). Compareto(a2. Getweight()));

after (using a method reference and java. Util. Comparator. Comparing):

your first method
inventory. Sort(comparing(apple: : getweight));
reference

dont worry about the new syntax and how things work. Youll learn that over the next
few sections!

3.6.1 in a nutshell
why should you care about method references? Method references can be seen as
shorthand for lambdas calling only a specific method. The basic idea is that if a
lambda represents call this method directly, its best to refer to the method by name
rather than by a description of how to call it. Indeed, a method reference lets you cre-
ate a lambda expression from an existing method implementation. But by referring to
a method name explicitly, your code can gain better readability. How does it work? When
you need a method reference, the target reference is placed before the delimiter : :
and the name of the method is provided after it. For example, apple: : getweight is a
method reference to the method getweight defined in the apple class. (remember
that no brackets are needed after getweight because youre not calling it at the
moment, youre merely quoting its name. ) this method reference is shorthand for
the lambda expression (apple apple) -> apple. Getweight(). Table 3.4 gives a few
more examples of possible method references in java 8.

Table 3.4 examples of lambdas and method reference equivalents

lambda method reference equivalent

(apple apple) -> apple. Getweight() apple: : getweight

() -> thread. Currentthread(): : dumpstack
thread. Currentthread(). Dumpstack()

(str, i) -> str. Substring(i) string: : substring

(string s) -> system. Out. Println(s) system. Out: : println
(string s) -> this. Isvalidname(s) this: : isvalidname

you can think of method references as syntactic sugar for lambdas that refer only to a
single method because you write less to express the same thing.
66 chapter 3 lambda expressions

recipe for constructing method references
there are three main kinds of method references:
1 a method reference to a static method (for example, the method parseint of
integer, written integer: : parseint)
2 a method reference to an instance method of an arbitrary type (for example,
the method length of a string, written string: : length)
3 a method reference to an instance method of an existing object or expression (for
example, suppose you have a local variable expensivetransaction that holds
an object of type transaction, which supports an instance method getvalue;
you can write expensivetransaction: : getvalue)
the second and third kinds of method references may be a bit overwhelming at first.
The idea with the second kind of method references, such as string: : length, is that
youre referring to a method to an object that will be supplied as one of the parame-
ters of the lambda. For example, the lambda expression (string s) -> s. Toupper-
case() can be rewritten as string: : touppercase. But the third kind of method
reference refers to a situation when youre calling a method in a lambda to an exter-
nal object that already exists. For example, the lambda expression () -> expensive-
transaction. Getvalue() can be rewritten as expensivetransaction: : getvalue.
This third kind of method reference is particularly useful when you need to pass
around a method defined as a private helper. For example, say you defined a helper
method isvalidname:
private boolean isvalidname(string string) {
return character. Isuppercase(string. Charat(0));
}

you can now pass this method around in the context of a predicate<string> using a
method reference:

filter(words, this: : isvalidname)

to help you digest this new knowledge, the shorthand rules to refactor a lambda expres-
sion to an equivalent method reference follow simple recipes, shown in figure 3.5.
Note that there are also special forms of method references for constructors, array
constructors, and super-calls. Lets apply method references in a concrete example.
Say youd like to sort a list of strings, ignoring case differences. The sort method on
a list expects a comparator as parameter. You saw earlier that comparator describes a
function descriptor with the signature (t, t) -> int. You can define a lambda expres-
sion that uses the method comparetoignorecase in the string class as follows (note
that comparetoignorecase is predefined in the string class):

list<string> str = arrays. Aslist("a", "b", "a", "b");
str. Sort((s1, s2) -> s1. Comparetoignorecase(s2));
method references 67

1 lambda (args) -> classname. Staticmethod(args)

method reference classname: : staticmethod

2 lambda (arg0, rest) -> arg0. Instancemethod(rest)

arg0 is of type
classname
method reference classname: : instancemethod

3 lambda (args) -> expr. Instancemethod(args)

method reference expr: : instancemethod

figure 3.5 recipes for constructing method references for three different types of lambda
expressions

the lambda expression has a signature compatible with the function descriptor of
comparator. Using the recipes described previously, the example can also be written
using a method reference; this results in more concise code, as follows:
list<string> str = arrays. Aslist("a", "b", "a", "b");
str. Sort(string: : comparetoignorecase);

note that the compiler goes through a similar type-checking process as for lambda
expressions to figure out whether a method reference is valid with a given functional
interface. The signature of the method reference has to match the type of the context.
To check your understanding of method references, do try quiz 3.6!

Quiz 3.6: method references
what are equivalent method references for the following lambda expressions?
1 tointfunction<string> stringtoint =
(string s) -> integer. Parseint(s);

2 bipredicate<list<string>, string> contains =
(list, element) -> list. Contains(element);

3 predicate<string> startswithnumber =
(string string) -> this . Startswithnumber(string);
68 chapter 3 lambda expressions

(continued)
answers:
1 this lambda expression forwards its argument to the static method parseint
of integer. This method takes a string to parse and returns an int. As a
result, the lambda can be rewritten using recipe b from figure 3.5 (lambda
expressions calling a static method) as follows:

tointfunction<string> stringtoint = integer: : parseint;

2 this lambda uses its first argument to call the method contains on it.
Because the first argument is of type list, you can use recipe c from fig-
ure 3.5 as follows:

bipredicate<list<string>, string> contains = list: : contains;

this is because the target type describes a function descriptor (list<string>,
string) -> boolean, and list: : contains can be unpacked to that func-
tion descriptor.
3 this expression-style lambda invokes a private helper method. You can use
recipe d from figure 3.5 as follows:

predicate<string> startswithnumber = this: : startswithnumber

weve shown only how to reuse existing method implementations and create method
references. But you can do something similar with constructors of a class.

3.6.2 constructor references
you can create a reference to an existing constructor using its name and the keyword
new as follows: classname: : new. It works similarly to a reference to a static method.
For example, suppose theres a zero-argument constructor. This fits the signature () ->
apple of supplier; you can do the following:
constructor reference to the
default apple() constructor
supplier<apple> c1 = apple: : new;
apple a1 = c1. Get();
calling suppliers get method
produces a new apple.
Which is equivalent to
lambda expression to create an
supplier<apple> c1 = () -> new apple();
apple using the default constructor
apple a1 = c1. Get();
calling suppliers get method
produces a new apple.

If you have a constructor with signature apple(integer weight), it fits the signature
of the function interface, so you can do this
method references 69

function<integer, apple> c2 = apple: : new;
constructor reference to
apple a2 = c2. Apply(110); apple (integer weight)
calling functions apply method with
a given weight produces an apple.

Which is equivalent to lambda expression to create
an apple with a given weight
function<integer, apple> c2 = (weight) -> new apple(weight);
apple a2 = c2. Apply(110);
calling functions apply method with a
given weight produces a new apple object.

In the following code, each element of a list of integers is passed to the constructor
of apple using a similar map method we defined earlier, resulting in a list of apples
with various weights:
passing a constructor
list<integer> weights = arrays. Aslist(7, 3, 4, 10); reference to the map method
list<apple> apples = map(weights, apple: : new);
public list<apple> map(list<integer> list, function<integer, apple> f) {
list<apple> result = new arraylist<>();
for(integer i: list) {
result. Add(f. Apply(i));
}
return result;
}

if you have a two-argument constructor, apple (color color, integer weight), it fits
the signature of the bifunction interface, so you can do this:
constructor reference to apple
bifunction<color, integer, apple> c3 = apple: : new; (color color, integer weight)
apple a3 = c3. Apply(green, 110);
bifunctions apply method with a given color
and weight produces a new apple object.
Which is equivalent to

lambda expression to create an
bifunction<string, integer, apple> c3 = apple with a given color and weight
(color, weight) -> new apple(color, weight);
apple a3 = c3. Apply(green, 110);
bifunctions apply method with a given color
and weight produces a new apple object.

The capability of referring to a constructor without instantiating it enables interesting
applications. For example, you can use a map to associate constructors with a string
value. You can then create a method givemefruit that, given a string and an integer,
can create different types of fruits with different weights, as follows:

static map<string, function<integer, fruit>> map = new hashmap<>();
static {
map. Put("apple", apple: : new);
70 chapter 3 lambda expressions

map. Put("orange", orange: : new);
get a function<integer,
// etc...
Fruit> from the map
}
public static fruit givemefruit(string fruit, integer weight){
return map. Get(fruit. Tolowercase())
. Apply(weight);
} functions apply method with an integer
weight parameter creates the requested fruit.

To check your understanding of method and constructor references, try out quiz 3.7.

Quiz 3.7: constructor references
you saw how to transform zero-, one-, and two-argument constructors into constructor
references. What would you need to do in order to use a constructor reference for a
three-argument constructor such as rgb(int, int, int)?
Answer:
you saw that the syntax for a constructor reference is classname: : new, so in this
case its rgb: : new. But you need a functional interface that will match the signature
of that constructor reference. Because there isnt one in the functional interface
starter set, you can create your own:
public interface trifunction<t, u, v, r> {
r apply(t t, u u, v v);
}

and you can now use the constructor reference as follows:

trifunction<integer, integer, integer, rgb> colorfactory = rgb: : new;

weve gone through a lot of new information: lambdas, functional interfaces, and
method references. Well put it all into practice in the next section.

3.7 putting lambdas and method references into practice
to wrap up this chapter and our discussion on lambdas, well continue with our initial
problem of sorting a list of apples with different ordering strategies. And well show
how you can progressively evolve a naive solution into a concise solution, using all the
concepts and features explained so far in the book: behavior parameterization,
anonymous classes, lambda expressions, and method references. The final solution
well work toward is the following (note that all source code is available on the
books website: www. Manning. Com/books/modern-java-in-action):

inventory. Sort(comparing(apple: : getweight));
putting lambdas and method references into practice 71

3.7.1 step 1: pass code
youre lucky; the java 8 api already provides you with a sort method available on
list so you dont have to implement it. The hard part is done! But how can you pass
an ordering strategy to the sort method? Well, the sort method has the following
signature:

void sort(comparator<? Super e> c)

it expects a comparator object as argument to compare two apples! This is how you
can pass different strategies in java: they have to be wrapped in an object. We say that
the behavior of sort is parameterized: its behavior will be different based on different
ordering strategies passed to it.
Your first solution looks like this:
public class applecomparator implements comparator<apple> {
public int compare(apple a1, apple a2){
return a1. Getweight(). Compareto(a2. Getweight());
}
}
inventory. Sort(new applecomparator());

3.7.2 step 2: use an anonymous class
rather than implementing comparator for the purpose of instantiating it once, you
saw that you could use an anonymous class to improve your solution:
inventory. Sort(new comparator<apple>() {
public int compare(apple a1, apple a2){
return a1. Getweight(). Compareto(a2. Getweight());
}
});

3.7.3 step 3: use lambda expressions
but your current solution is still verbose. Java 8 introduced lambda expressions, which
provide a lightweight syntax to achieve the same goal: passing code. You saw that a
lambda expression can be used where a functional interface is expected. As a reminder,
a functional interface is an interface defining only one abstract method. The signa-
ture of the abstract method (called function descriptor) can describe the signature of a
lambda expression. In this case, the comparator represents a function descriptor
(t, t) -> int. Because youre using apples, it represents more specifically (apple,
apple) -> int. Your new improved solution looks therefore as follows:
inventory. Sort((apple a1, apple a2)
-> a1. Getweight(). Compareto(a2. Getweight())
);
72 chapter 3 lambda expressions

we explained that the java compiler could infer the types of the parameters of a lambda
expression by using the context in which the lambda appears. You can therefore
rewrite your solution as follows:

inventory. Sort((a1, a2) -> a1. Getweight(). Compareto(a2. Getweight()));

can you make your code even more readable? Comparator includes a static helper
method called comparing that takes a function extracting a comparable key and
produces a comparator object (we explain why interfaces can have static methods in
chapter 13). It can be used as follows (note that you now pass a lambda with only
one argument; the lambda specifies how to extract the key for comparison from an
apple):

comparator<apple> c = comparator. Comparing((apple a) -> a. Getweight());

you can now rewrite your solution in a slightly more compact form:
import static java. Util. Comparator. Comparing;
inventory. Sort(comparing(apple -> apple. Getweight()));

3.7.4 step 4: use method references
we explained that method references are syntactic sugar for lambda expressions that
forwards their arguments. You can use a method reference to make your code slightly
less verbose (assuming a static import of java. Util. Comparator. Comparing):

inventory. Sort(comparing(apple: : getweight));

congratulations, this is your final solution! Why is this better than code prior to java
8? Its not only because its shorter; its also obvious what it means. The code reads like
the problem statement sort inventory comparing the weight of the apples.

3.8 useful methods to compose lambda expressions
several functional interfaces in the java 8 api contain convenience methods. Specifi-
cally, many functional interfaces such as comparator, function, and predicate that
are used to pass lambda expressions provide methods that allow composition. What
does this mean? In practice it means you can combine several simple lambda expres-
sions to build more complicated ones. For example, you can combine two predicates
into a larger predicate that performs an or operation between the two predicates.
Moreover, you can also compose functions such that the result of one becomes the
input of another function. You may wonder how its possible that there are additional
methods in a functional interface. (after all, this goes against the definition of a func-
tional interface! ) the trick is that the methods that well introduce are called default
methods (theyre not abstract methods). We explain them in detail in chapter 13. For
useful methods to compose lambda expressions 73

now, trust us and read chapter 13 later, when you want to find out more about default
methods and what you can do with them.

3.8.1 composing comparators
youve seen that you can use the static method comparator. Comparing to return a
comparator based on a function that extracts a key for comparison as follows:

comparator<apple> c = comparator. Comparing(apple: : getweight);

reversed order
what if you wanted to sort the apples by decreasing weight? Theres no need to create
a different instance of a comparator. The interface includes a default method
reversed that reverses the ordering of a given comparator. You can modify the previ-
ous example to sort the apples by decreasing weight by reusing the initial comparator:

inventory. Sort(comparing(apple: : getweight). Reversed());

sorts by decreasing weight
chaining comparators
this is all nice, but what if you find two apples that have the same weight? Which
apple should have priority in the sorted list? You may want to provide a second
comparator to further refine the comparison. For example, after two apples are com-
pared based on their weight, you may want to sort them by country of origin. The
thencomparing method allows you to do that. It takes a function as parameter (like
the method comparing) and provides a second comparator if two objects are consid-
ered equal using the initial comparator. You can solve the problem elegantly again
as follows:
sorts by decreasing weight
inventory. Sort(comparing(apple: : getweight)
. Reversed()
sorts further by country when
. Thencomparing(apple: : getcountry)); two apples have same weight

3.8.2 composing predicates
the predicate interface includes three methods that let you reuse an existing
predicate to create more complicated ones: negate, and, and or. For example, you
can use the method negate to return the negation of a predicate, such as an apple
that is not red:
produces the negation of
predicate<apple> notredapple = redapple. Negate(); the existing predicate
object redapple

you may want to combine two lambdas to say that an apple is both red and heavy with
the and method:
chains two predicates
predicate<apple> redandheavyapple = to produce another
redapple. And(apple -> apple. Getweight() > 150);
predicate object
74 chapter 3 lambda expressions

you can combine the resulting predicate one step further to express apples that are
red and heavy (above 150 g) or only green apples:

predicate<apple> redandheavyappleorgreen = chains three predicates to
redapple. And(apple -> apple. Getweight() > 150) construct a more complex
. Or(apple -> green. Equals(a. Getcolor())); predicate object

why is this great? From simpler lambda expressions you can represent more compli-
cated lambda expressions that still read like the problem statement! Note that the
precedence of methods and and or in the chain is from left to rightthere is no
equivalent of bracketing. So a. Or(b). And(c) must be read as (a || b) && c. Similarly,
a. And(b). Or(c) must be read as as (a && b) || c.

3.8.3 composing functions
finally, you can also compose lambda expressions represented by the function inter-
face. The function interface comes with two default methods for this, andthen and
compose, which both return an instance of function.
The method andthen returns a function that first applies a given function to an
input and then applies another function to the result of that application. For exam-
ple, given a function f that increments a number (x -> x + 1) and another function g
that multiples a number by 2, you can combine them to create a function h that first
increments a number and then multiplies the result by 2:

function<integer, integer> f = x -> x + 1; in mathematics
function<integer, integer> g = x -> x * 2; youd write g(f(x))
function<integer, integer> h = f. Andthen(g); or (g o f)(x).
Int result = h. Apply(1);
this returns 4.

You can also use the method compose similarly to first apply the function given as argu-
ment to compose and then apply the function to the result. For example, in the previous
example using compose, it would mean f(g(x)) instead of g(f(x)) using andthen:

function<integer, integer> f = x -> x + 1; in mathematics
function<integer, integer> g = x -> x * 2; youd write f(g(x))
function<integer, integer> h = f. Compose(g); or (f o g)(x).
Int result = h. Apply(1);
this returns 3.

Figure 3.6 illustrates the difference between andthen and compose.
This all sounds a bit too abstract. How can you use these in practice? Lets say you
have various utility methods that do text transformation on a letter represented as
a string:
public class letter{
public static string addheader(string text) {
return "from raoul, mario and alan: " + text;
}
useful methods to compose lambda expressions 75

public static string addfooter(string text) {
return text + " kind regards";
}
public static string checkspelling(string text) {
return text. Replaceall("labda", "lambda");
}
}

f. Andthen(g)

inputs results
f g
1 2 4
f g
2 3 6
f g
3 4 8

function<integer, integer> f = x -> x + 1;
function<integer, integer> g = x -> x * 2;

f. Compose(g)

inputs results
g f
1 2 3
g f
2 4 5
g f
3 6 7

figure 3.6 using andthen versus compose

you can now create various transformation pipelines by composing the utility meth-
ods. For example, creating a pipeline that first adds a header, then checks spelling,
and finally adds a footer, as shown in the following (and as illustrated in figure 3.7):
function<string, string> addheader = letter: : addheader;
function<string, string> transformationpipeline
= addheader. Andthen(letter: : checkspelling)
. Andthen(letter: : addfooter);

transformation pipeline

andthen andthen
addheader checkspelling addfooter

figure 3.7 a transformation pipeline using andthen
76 chapter 3 lambda expressions

a second pipeline might be only adding a header and footer without checking for
spelling:
function<string, string> addheader = letter: : addheader;
function<string, string> transformationpipeline
= addheader. Andthen(letter: : addfooter);

3.9 similar ideas from mathematics
if you feel comfortable with high school mathematics, this section gives another view-
point of the idea of lambda expressions and passing around functions. Feel free to
skip it; nothing else in the book depends on it. But you may enjoy seeing another
perspective.

3.9.1 integration
suppose you have a (mathematical, not java) function f, perhaps defined by

f(x) = x + 10

then, one question thats often asked (at school and in science and engineering
degrees) is to find the area beneath the function when drawn on paper (counting the
x-axis as the zero line). For example, you write

 37 f(x)dx or  37 (x + 10)dx
for the area shown in figure 3.8.

20

18 f(x) = x + 10

16

14

12 7
f(x)dx
10 3

8
area under
6 the function

4

2

0 figure 3.8 area under the function
2 4 6 8 10 f(x) = x + 10 for x from 3 to 7
similar ideas from mathematics 77

in this example, the function f is a straight line, and so you can easily work out this area
by the trapezium method (drawing triangles and rectangles) to discover the solution:

1/2  ((3 + 10) + (7 + 10))  (7  3) = 60

now, how might you express this in java? Your first problem is reconciling the strange
notation like the integration symbol or dy/dx with familiar programming language
notation.
Indeed, thinking from first principles you need a method, perhaps called integrate,
that takes three arguments: one is f, and the others are the limits (3.0 and 7.0 here).
Thus, you want to write in java something that looks like this, where the function f is
passed as an argument:

integrate(f, 3, 7)

note that you cant write something as simple as

integrate(x + 10, 3, 7)

for two reasons. First, the scope of x is unclear, and second, this would pass a value of
x+10 to integrate instead of passing the function f.
Indeed, the secret role of dx in mathematics is to say that function taking argu-
ment x whose result is x + 10.

3.9.2 connecting to java 8 lambdas
as we mentioned earlier, java 8 uses the notation (double x) -> x + 10 (a lambda
expression) for exactly this purpose; hence you can write

integrate((double x) -> x + 10, 3, 7)

or

integrate((double x) -> f(x), 3, 7)

or, using a method reference as mentioned earlier,

integrate(c: : f, 3, 7)

if c is a class containing f as a static method. The idea is that youre passing the code
for f to the method integrate.
You may now wonder how youd write the method integrate itself. Continue to
suppose that f is a linear function (straight line). Youd probably like to write in a
form similar to mathematics:
public double integrate((double -> double) f, double a, double b) {
return (f(a) + f(b)) * (b - a) / 2.0
} incorrect java code! (you cant write
functions as you do in mathematics. )
78 chapter 3 lambda expressions

but because lambda expressions can be used only in a context expecting a functional
interface (in this case, doublefunction4), you have to write it the following way:
public double integrate(doublefunction<double> f, double a, double b) {
return (f. Apply(a) + f. Apply(b)) * (b - a) / 2.0;
}

or using doubleunaryoperator, which also avoids boxing the result:
public double integrate(doubleunaryoperator f, double a, double b) {
return (f. Applyasdouble(a) + f. Applyasdouble(b)) * (b - a) / 2.0;
}

as a side remark, its a bit of a shame that you have to write f. Apply(a) instead of sim-
ply writing f(a) as in mathematics, but java just cant get away from the view that
everything is an object instead of the idea of a function being truly independent!

Summary
 a lambda expression can be understood as a kind of anonymous function: it doesnt
have a name, but it has a list of parameters, a body, a return type, and also possi-
bly a list of exceptions that can be thrown.
 Lambda expressions let you pass code concisely.
 A functional interface is an interface that declares exactly one abstract method.
 Lambda expressions can be used only where a functional interface is expected.
 Lambda expressions let you provide the implementation of the abstract method
of a functional interface directly inline and treat the whole expression as an instance
of a functional interface.
 Java 8 comes with a list of common functional interfaces in the java. Util
. Function package, which includes predicate<t>, function<t, r>, supplier<t>,
consumer<t>, and binaryoperator<t>, described in table 3.2.
 Primitive specializations of common generic functional interfaces such as
predicate<t> and function<t, r> can be used to avoid boxing operations:
intpredicate, inttolongfunction, and so on.
 The execute-around pattern (for when you need to execute some given behav-
ior in the middle of boilerplate code thats necessary in a method, for example,
resource allocation and cleanup) can be used with lambdas to gain additional
flexibility and reusability.
 The type expected for a lambda expression is called the target type.
 Method references let you reuse an existing method implementation and pass
it around directly.
 Functional interfaces such as comparator, predicate, and function have sev-
eral default methods that can be used to combine lambda expressions.

4
using doublefunction<double> is more efficient than using function<double, double> as it avoids
boxing the result.
Part 2

functional-style data
processing with streams

t he second part of this book is a deep exploration of the new streams api,
which lets you write powerful code that processes a collection of data in a declar-
ative way. By the end of this second part, youll have a full understanding of what
streams are and how you can use them in your codebase to process a collection
of data concisely and efficiently.
Chapter 4 introduces the concept of a stream and explains how it compares
with a collection.
Chapter 5 investigates in detail the stream operations available to express
sophisticated data-processing queries. Youll look at many patterns such as filter-
ing, slicing, finding, matching, mapping, and reducing.
Chapter 6 covers collectorsa feature of the streams api that lets you
express even more complex data-processing queries.
In chapter 7, youll learn about how streams can automatically run in parallel
and leverage your multicore architectures. In addition, youll learn about vari-
ous pitfalls to avoid when using parallel streams correctly and effectively.
Introducing streams

this chapter covers
 what is a stream?
 Collections versus streams
 internal versus external iteration
 intermediate versus terminal operations

what would you do without collections in java? Nearly every java application makes
and processes collections. Collections are fundamental to many programming tasks:
they let you group and process data. To illustrate collections in action, imagine you
are tasked to create a collection of dishes to represent a menu to calculate different
queries. For example, you may want to find out the total number of calories for the
menu. Or, you may need to filter the menu to select only low-calorie dishes for a
special healthy menu. But despite collections being necessary for almost any java
application, manipulating collections is far from perfect:
 much business logic entails database-like operations such as grouping a list of
dishes by category (for example, all vegetarian dishes) or finding the most
expensive dish. How many times do you find yourself re-implementing these
operations using iterators? Most databases let you specify such operations
declaratively. For example, the following sql query lets you select (or filter)

81
82 chapter 4 introducing streams

the names of dishes that are low in calories: select name from dishes where
calorie < 400. As you can see, in sql you dont need to implement how to filter
using the calorie attribute of a dish (as you would with java collections, for
example, using an iterator and an accumulator). Instead, you write what you
want as result. This basic idea means that you worry less about how to explicitly
implement such queriesits handled for you! Why cant you do something
similar with collections?
 How would you process a large collection of elements? To gain performance
youd need to process it in parallel and use multicore architectures. But writing
parallel code is complicated in comparison to working with iterators. In addi-
tion, its no fun to debug!
What could the java language designers do to save your precious time and make your
life as programmers easier? You may have guessed: the answer is streams.

4.1 what are streams?
Streams are an update to the java api that let you manipulate collections of data in a
declarative way (you express a query rather than code an ad hoc implementation for
it). For now you can think of them as fancy iterators over a collection of data. In addi-
tion, streams can be processed in parallel transparently, without you having to write any
multithreaded code! We explain in detail in chapter 7 how streams and parallelization
work. To see the benefits of using streams, compare the following code to return the
names of dishes that are low in calories, sorted by number of caloriesfirst in java 7
and then in java 8 using streams. Dont worry about the java 8 code too much; we
explain it in detail in the next sections!
Before (java 7):

list<dish> lowcaloricdishes = new arraylist<>(); filters the elements
for(dish dish: menu) { using an accumulator
if(dish. Getcalories() < 400) {
lowcaloricdishes. Add(dish);
} sorts the
} dishes with an
collections. Sort(lowcaloricdishes, new comparator<dish>() { anonymous class
public int compare(dish dish1, dish dish2) {
return integer. Compare(dish1. Getcalories(), dish2. Getcalories());
}
});
list<string> lowcaloricdishesname = new arraylist<>(); processes the
for(dish dish: lowcaloricdishes) { sorted list to select
lowcaloricdishesname. Add(dish. Getname()); the names of dishes
}

in this code you use a garbage variable, lowcaloricdishes. Its only purpose is to act
as an intermediate throwaway container. In java 8, this implementation detail is
pushed into the library where it belongs.
What are streams? 83

after (java 8):
selects dishes
import static java. Util. Comparator. Comparing; that are below
import static java. Util. Stream. Collectors. Tolist; 400 calories
list<string> lowcaloricdishesname =
menu. Stream() sorts them
. Filter(d -> d. Getcalories() < 400) by calories
. Sorted(comparing(dish: : getcalories))
stores all the . Map(dish: : getname)
names in a list . Collect(tolist()); extracts the names
of these dishes

to exploit a multicore architecture and execute this code in parallel, you need only to
change stream() to parallelstream():
list<string> lowcaloricdishesname =
menu. Parallelstream()
. Filter(d -> d. Getcalories() < 400)
. Sorted(comparing(dishes: : getcalories))
. Map(dish: : getname)
. Collect(tolist());

you may be wondering what exactly happens when you invoke the method parallel-
stream. How many threads are being used? What are the performance benefits?
Should you use this method at all? Chapter 7 covers these questions in detail. For now,
you can see that the new approach offers several immediate benefits from a software
engineering point of view:
 the code is written in a declarative way: you specify what you want to achieve
(filter dishes that are low in calories) as opposed to specifying how to implement
an operation (using control-flow blocks such as loops and if conditions). As
you saw in the previous chapter, this approach, together with behavior parame-
terization, enables you to cope with changing requirements: you could easily
create an additional version of your code to filter high-calorie dishes using a
lambda expression, without having to copy and paste code. Another way to
think about the benefit of this approach is that the threading model is decou-
pled from the query itself. Because you are providing a recipe for a query, it
could be executed sequentially or in parallel. You will learn more about this
in chapter 7.
 You chain together several building-block operations to express a complicated
data-processing pipeline (you chain the filter by linking sorted, map, and
collect operations, as illustrated in figure 4.1) while keeping your code readable
and its intent clear. The result of the filter is passed to the sorted method,
which is then passed to the map method and then to the collect method.
Because operations such as filter (or sorted, map, and collect) are available as
high-level building blocks that dont depend on a specific threading model, their internal
implementation could be single-threaded or could potentially maximize your multicore
84 chapter 4 introducing streams

lambda lambda lambda

menu filter sorted map collect

figure 4.1 chaining stream operations forming a stream pipeline

architecture transparently! In practice, this means you no longer have to worry about
threads and locks to figure out how to parallelize certain data processing tasks: the
streams api does it for you!
The new streams api is expressive. For example, after reading this chapter and
chapters 5 and 6, youll be able to write code like the following:
map<dish. Type, list<dish>> dishesbytype =
menu. Stream(). Collect(groupingby(dish: : gettype));

this particular example is explained in detail in chapter 6. It groups dishes by their
types inside a map. For example, the map may contain the following result:
{fish=[prawns, salmon],
other=[french fries, rice, season fruit, pizza],
meat=[pork, beef, chicken]}

now consider how youd implement this with the typical imperative programming
approach using loops. But dont waste too much of your time. Instead, embrace the
power of streams in this and the following chapters!

Other libraries: guava, apache, and lambdaj
there have been many attempts at providing java programmers with better libraries
to manipulate collections. For example, guava is a popular library created by google.
It provides additional container classes such as multimaps and multisets. The apache
commons collections library provides similar features. Finally, lambdaj, written by
mario fusco, coauthor of this book, provides many utilities to manipulate collections
in a declarative manner, inspired by functional programming.
Now, java 8 comes with its own official library for manipulating collections in a more
declarative style.

To summarize, the streams api in java 8 lets you write code thats
 declarativemore concise and readable
 composablegreater flexibility
 parallelizablebetter performance
what are streams? 85

for the remainder of this chapter and the next, well use the following domain for our
examples: a menu thats nothing more than a list of dishes
list<dish> menu = arrays. Aslist(
new dish("pork", false, 800, dish. Type. Meat),
new dish("beef", false, 700, dish. Type. Meat),
new dish("chicken", false, 400, dish. Type. Meat),
new dish("french fries", true, 530, dish. Type. Other),
new dish("rice", true, 350, dish. Type. Other),
new dish("season fruit", true, 120, dish. Type. Other),
new dish("pizza", true, 550, dish. Type. Other),
new dish("prawns", false, 300, dish. Type. Fish),
new dish("salmon", false, 450, dish. Type. Fish) );

where a dish is an immutable class defined as
public class dish {
private final string name;
private final boolean vegetarian;
private final int calories;
private final type type;
public dish(string name, boolean vegetarian, int calories, type type) {
this. Name = name;
this. Vegetarian = vegetarian;
this. Calories = calories;
this. Type = type;
}
public string getname() {
return name;
}
public boolean isvegetarian() {
return vegetarian;
}
public int getcalories() {
return calories;
}
public type gettype() {
return type;
}
@override
public string tostring() {
return name;
}
public enum type { meat, fish, other }
}

well now explore how you can use the streams api in more detail. Well compare
streams to collections and provide some background. In the next chapter, well inves-
tigate in detail the stream operations available to express sophisticated data process-
ing queries. Well look at many patterns such as filtering, slicing, finding, matching,
mapping, and reducing. There will be many quizzes and exercises to try to solidify
your understanding.
86 chapter 4 introducing streams

next, well discuss how you can create and manipulate numeric streams (for exam-
ple, to generate a stream of even numbers or pythagorean triples). Finally, well dis-
cuss how you can create streams from different sources, such as from a file. Well also
discuss how to generate streams with an infinite number of elementssomething you
definitely cant do with collections!

4.2 getting started with streams
we start our discussion of streams with collections, because thats the simplest way to
begin working with streams. Collections in java 8 support a new stream method that
returns a stream (the interface definition is available in java. Util. Stream. Stream).
Youll later see that you can also get streams in various other ways (for example, gener-
ating stream elements from a numeric range or from i/o resources).
First, what exactly is a stream? A short definition is a sequence of elements from a
source that supports data-processing operations. Lets break down this definition
step-by-step:
 sequence of elementslike a collection, a stream provides an interface to a
sequenced set of values of a specific element type. Because collections are data
structures, theyre mostly about storing and accessing elements with specific
time/space complexities (for example, an arraylist versus a linkedlist). But
streams are about expressing computations such as filter, sorted, and map,
which you saw earlier. Collections are about data; streams are about computa-
tions. We explain this idea in greater detail in the coming sections.
 Sourcestreams consume from a data-providing source such as collections,
arrays, or i/o resources. Note that generating a stream from an ordered collec-
tion preserves the ordering. The elements of a stream coming from a list will
have the same order as the list.
 Data-processing operationsstreams support database-like operations and com-
mon operations from functional programming languages to manipulate data,
such as filter, map, reduce, find, match, sort, and so on. Stream operations
can be executed either sequentially or in parallel.
In addition, stream operations have two important characteristics:
 pipeliningmany stream operations return a stream themselves, allowing oper-
ations to be chained to form a larger pipeline. This enables certain optimiza-
tions that we explain in the next chapter, such as laziness and short-circuiting. A
pipeline of operations can be viewed as a database-like query on the data
source.
 Internal iterationin contrast to collections, which are iterated explicitly using
an iterator, stream operations do the iteration behind the scenes for you. We
briefly mentioned this idea in chapter 1 and will return to it later in the next
section.
Getting started with streams 87

lets look at a code example to explain all of these ideas:

gets a stream from
menu (the list of dishes)
import static java. Util. Stream. Collectors. Tolist; creates a pipeline of
list<string> threehighcaloricdishnames = operations: first filter
menu. Stream() high-calorie dishes
. Filter(dish -> dish. Getcalories() > 300)
. Map(dish: : getname)
gets the names
. Limit(3) of the dishes
. Collect(tolist());
system. Out. Println(threehighcaloricdishnames);
selects only
stores the results gives results [pork, the first three
in another list beef, chicken]

in this example, you first get a stream from the list of dishes by calling the stream
method on menu. The data source is the list of dishes (the menu) and it provides a
sequence of elements to the stream. Next, you apply a series of data-processing operations on
the stream: filter, map, limit, and collect. All these operations except collect
return another stream so they can be connected to form a pipeline, which can be
viewed as a query on the source. Finally, the collect operation starts processing the
pipeline to return a result (its different because it returns something other than a
streamhere, a list). No result is produced, and indeed no element from menu is
even selected, until collect is invoked. You can think of it as if the method invoca-
tions in the chain are queued up until collect is called. Figure 4.2 shows the
sequence of stream operations: filter, map, limit, and collect, each of which is
briefly described here:
 filtertakes a lambda to exclude certain elements from the stream. In this
case, you select dishes that have more than 300 calories by passing the lambda
d -> d. Getcalories() > 300.
 Maptakes a lambda to transform an element into another one or to extract
information. In this case, you extract the name for each dish by passing the
method reference dish: : getname, which is equivalent to the lambda d ->
d. Getname().
 Limittruncates a stream to contain no more than a given number of elements.
 Collectconverts a stream into another form. In this case you convert the
stream into a list. It looks like a bit of magic; well describe how collect works
in more detail in chapter 6. At the moment, you can see collect as an opera-
tion that takes as an argument various recipes for accumulating the elements of
a stream into a summary result. Here, tolist() describes a recipe for convert-
ing a stream into a list.
Notice how the code we described is different than what youd write if you were to
process the list of menu items step-by-step. First, you use a much more declarative style
88 chapter 4 introducing streams

menu stream

stream<dish>

filter(d -> d. Getcalories() > 300)

stream<dish>

map(dish: : getname)

stream<string>

limit(3)

stream<string>

collect(tolist())

list<string>

figure 4.2 filtering a menu using a stream to find out three high-calorie dish names

to process the data in the menu where you say what needs to be done: find names of
three high-calorie dishes. You dont implement the filtering (filter), extracting
(map), or truncating (limit) functionalities; theyre available through the streams
library. As a result, the streams api has more flexibility to decide how to optimize this
pipeline. For example, the filtering, extracting, and truncating steps could be merged
into a single pass and stop as soon as three dishes are found. We show an example to
demonstrate that in the next chapter.
Lets stand back a little and examine the conceptual differences between the col-
lections api and the new streams api before we explore in more detail what opera-
tions you can perform with a stream.

4.3 streams vs. Collections
both the existing java notion of collections and the new notion of streams provide
interfaces to data structures representing a sequenced set of values of the element
type. By sequenced, we mean that we commonly step through the values in turn rather
than randomly accessing them in any order. Whats the difference?
Well start with a visual metaphor. Consider a movie stored on a dvd. This is a col-
lection (perhaps of bytes or of frameswe dont care which here) because it contains
streams vs. Collections 89

the whole data structure. Now consider watching the same video when its being
streamed over the internet. This is now a stream (of bytes or frames). The streaming
video player needs to have downloaded only a few frames in advance of where the user
is watching, so you can start displaying values from the beginning of the stream before
most of the values in the stream have even been computed (consider streaming a live
football game). Note particularly that the video player may lack the memory to buffer
the whole stream in memory as a collectionand the startup time would be appalling
if you had to wait for the final frame to appear before you could start showing the
video. You might choose for video-player implementation reasons to buffer a part of a
stream into a collection, but this is distinct from the conceptual difference.
In coarsest terms, the difference between collections and streams has to do with
when things are computed. A collection is an in-memory data structure that holds all
the values the data structure currently hasevery element in the collection has to be
computed before it can be added to the collection. (you can add things to, and
remove them from, the collection, but at each moment in time, every element in the
collection is stored in memory; elements have to be computed before becoming part
of the collection. )
by contrast, a stream is a conceptually fixed data structure (you cant add or
remove elements from it) whose elements are computed on demand. This gives rise to
significant programming benefits. In chapter 6, well show how simple it is to con-
struct a stream containing all the prime numbers (2, 3, 5, 7, 11, . . . ) even though
there are an infinite number of them. The idea is that a user will extract only the val-
ues they require from a stream and these elements are producedinvisibly to the
useronly as and when required. This is a form of a producer-consumer relationship.
Another view is that a stream is like a lazily constructed collection: values are com-
puted when theyre solicited by a consumer (in management speak this is demand-
driven, or even just-in-time, manufacturing).
In contrast, a collection is eagerly constructed (supplier-driven: fill your warehouse
before you start selling, like a christmas novelty that has a limited life). Imagine apply-
ing this to the primes example. Attempting to construct a collection of all prime num-
bers would result in a program loop that forever computes a new primeadding it to
the collectionbut could never finish making the collection, so the consumer would
never get to see it.
Figure 4.3 illustrates the difference between a stream and a collection, applied to
our dvd versus internet streaming example.
Another example is a browser internet search. Suppose you search for a phrase
with many matches in google or in an e-commerce online shop. Instead of waiting for
the whole collection of results along with their photographs to be downloaded, you
get a stream whose elements are the best 10 or 20 matches, along with a button to
click for the next 10 or 20. When you, the consumer, click for the next 10, the supplier
computes these on demand, before returning them to your browser for display.
90 chapter 4 introducing streams

a collection in java 8 is like a stream in java 8 is like a movie
a movie stored on dvd. Streamed over the internet.

Eager construction means lazy construction means
waiting for computation values are computed
of all values. Only as needed. Internet

all file data
loaded from
dvd

like a dvd, a collection holds all the values that like a streaming video, values
the data structure currently hasevery element in are computed as they are needed.
The collection has to be computed before it
can be added to the collection.

Figure 4.3 streams versus collections

4.3.1 traversable only once
note that, similarly to iterators, a stream can be traversed only once. After that a
stream is said to be consumed. You can get a new stream from the initial data source
to traverse it again as you would for an iterator (assuming its a repeatable source like
a collection; if its an i/o channel, youre out of luck). For example, the following
code would throw an exception indicating the stream has been consumed:

list<string> title = arrays. Aslist("modern", "java", "in", "action");
stream<string> s = title. Stream();
s. Foreach(system. Out: : println); prints each word in the title
s. Foreach(system. Out: : println);
java. Lang. Illegalstateexception: stream has
already been operated upon or closed

keep in mind that you can consume a stream only once!

Streams and collections philosophically
for readers who like philosophical viewpoints, you can see a stream as a set of val-
ues spread out in time. In contrast, a collection is a set of values spread out in space
(here, computer memory), which all exist at a single point in timeand which you
access using an iterator to access members inside a for-each loop.
Streams vs. Collections 91

another key difference between collections and streams is how they manage the itera-
tion over data.

4.3.2 external vs. Internal iteration
using the collection interface requires iteration to be done by the user (for exam-
ple, using for-each); this is called external iteration. The streams library, by contrast,
uses internal iterationit does the iteration for you and takes care of storing the result-
ing stream value somewhere; you merely provide a function saying whats to be done.
The following code listings illustrate this difference.

Listing 4.1 collections: external iteration with a for-each loop

list<string> names = new arraylist<>(); explicitly iterates the list
for(dish dish: menu) {
of menu sequentially
names. Add(dish. Getname());
} extracts the name and
adds it to an accumulator

note that the for-each hides some of the iteration complexity. The for-each con-
struct is syntactic sugar that translates into something much uglier using an iterator
object.

Listing 4.2 collections: external iteration using an iterator behind the scenes

list<string> names = new arraylist<>();
iterator<string> iterator = menu. Iterator();
while(iterator. Hasnext()) {
dish dish = iterator. Next(); iterates
names. Add(dish. Getname());
explicitly
}

listing 4.3 streams: internal iteration

list<string> names = menu. Stream() parameterizes map with the
. Map(dish: : getname) getname method to extract
starts executing the . Collect(tolist()); the name of a dish
pipeline of operations;
no iteration

lets use an analogy to understand the differences and benefits of internal iteration.
Lets say youre talking to your two-year-old daughter, sofia, and want her to put her
toys away:

you: sofia, lets put the toys away. Is there a toy on the ground?
Sofia: yes, the ball.
You: okay, put the ball in the box. Is there something else?
Sofia: yes, theres my doll.
You: okay, put the doll in the box. Is there something else?
Sofia: yes, theres my book.
92 chapter 4 introducing streams

you: okay, put the book in the box. Is there something else?
Sofia: no, nothing else.
You: fine, were finished.

This is exactly what you do every day with your java collections. You iterate a collection
externally, explicitly pulling out and processing the items one by one. It would be far
better if you could tell sofia, put all the toys that are on the floor inside the box.
There are two other reasons why an internal iteration is preferable: first, sofia could
choose to take the doll with one hand and the ball with the other at the same time,
and second, she could decide to take the objects closest to the box first and then the
others. In the same way, using an internal iteration, the processing of items could be
transparently done in parallel or in a different order that may be more optimized.
These optimizations are difficult if you iterate the collection externally as youre used
to doing in java. This may seem like nit-picking, but its much of the raison-detre of
java 8s introduction of streams. The internal iteration in the streams library can auto-
matically choose a data representation and implementation of parallelism to match
your hardware. By contrast, once youve chosen external iteration by writing for-
each, then youve committed to self-manage any parallelism. (self-managing in practice
means either one fine day well parallelize this or starting the long and arduous bat-
tle involving tasks and synchronized. ) java 8 needed an interface like collection
but without iterators, ergo stream! Figure 4.4 illustrates the difference between a
stream (internal iteration) and a collection (external iteration).

Stream
internal iteration

elements

your code

stream

collection
external iteration

elements

foreach your code

collection

figure 4.4 internal versus external iteration
stream operations 93

weve described the conceptual differences between collections and streams. Specifi-
cally, streams make use of internal iteration, where a library takes care of iterating for
you. But this is useful only if you have a list of predefined operations to work with (for
example, filter or map) that hide the iteration. Most of these operations take lambda
expressions as arguments so you can parameterize their behavior as we showed in the
previous chapter. The java language designers shipped the streams api with an exten-
sive list of operations you can use to express complicated data processing queries.
Well briefly look at this list of operations now and explore them in more detail with
examples in the next chapter. To check your understanding of external versus inter-
nal iteration, try quiz 4.1 below.

Quiz 4.1: external vs. Internal iteration
based on what you learned about external iteration in listing 4.1 and 4.2, which
stream operation would you use to refactor the following code?

List<string> highcaloricdishes = new arraylist<>();
iterator<string> iterator = menu. Iterator();
while(iterator. Hasnext()) {
dish dish = iterator. Next();
if(dish. Getcalories() > 300) {
highcaloricdishes. Add(d. Getname());
}
}

answer: you need to use the filter pattern

list<string> highcaloricdish =
menu. Stream()
. Filter(dish -> dish. Getcalories() > 300)
. Collect(tolist());

dont worry if youre still unfamiliar with how to precisely write a stream query, you
will learn this in more detail in the next chapter.

4.4 stream operations
the streams interface in java. Util. Stream. Stream defines many operations. They
can be classified into two categories. Lets look at our previous example once again:

gets a stream from
list<string> names = menu. Stream()
the list of dishes
. Filter(dish -> dish. Getcalories() > 300)
intermediate . Map(dish: : getname)
. Limit(3) intermediate
operation operation
. Collect(tolist());

converts the intermediate
stream into a list operation
94 chapter 4 introducing streams

you can see two groups of operations:
 filter, map, and limit can be connected together to form a pipeline.
 Collect causes the pipeline to be executed and closes it.

Stream operations that can be connected are called intermediate operations, and opera-
tions that close a stream are called terminal operations. Figure 4.5 highlights these two
groups. Why is the distinction important?

Lambda lambda integer

menu filter map limit collect

intermediate terminal
operations operation

figure 4.5 intermediate versus terminal operations

4.4.1 intermediate operations
intermediate operations such as filter or sorted return another stream as the
return type. This allows the operations to be connected to form a query. Whats
important is that intermediate operations dont perform any processing until a ter-
minal operation is invoked on the stream pipelinetheyre lazy. This is because
intermediate operations can usually be merged and processed into a single pass by
the terminal operation.
To understand whats happening in the stream pipeline, modify the code so each
lambda also prints the current dish its processing. (like many demonstration and
debugging techniques, this is appalling programming style for production code, but
directly explains the order of evaluation when youre learning. )
list<string> names =
menu. Stream()
. Filter(dish -> {
system. Out. Println("filtering: " + dish. Getname());
prints the dishes return dish. Getcalories() > 300;
as theyre filtered })
. Map(dish -> {
system. Out. Println("mapping: " + dish. Getname());
return dish. Getname();
})
prints the dishes as you
. Limit(3) extract their names
. Collect(tolist());
system. Out. Println(names);

this code, when executed, will print the following:
filtering: pork
mapping: pork
stream operations 95

filtering: beef
mapping: beef
filtering: chicken
mapping: chicken
[pork, beef, chicken]

by doing this, you can notice that the streams library performs several optimizations
exploiting the lazy nature of streams. First, despite the fact that many dishes have
more than 300 calories, only the first three are selected! This is because of the limit
operation and a technique called short-circuiting, as well explain in the next chapter.
Second, despite the fact that filter and map are two separate operations, they were
merged into the same pass (compiler experts call this technique loop fusion).

4.4.2 terminal operations
terminal operations produce a result from a stream pipeline. A result is any non-
stream value such as a list, an integer, or even void. For example, in the following
pipeline, foreach is a terminal operation that returns void and applies a lambda to
each dish in the source. Passing system. Out. Println to foreach asks it to print every
dish in the stream created from menu:
menu. Stream(). Foreach(system. Out: : println);

to check your understanding of intermediate versus terminal operations, try out
quiz 4.2.

Quiz 4.2: intermediate vs. Terminal operations
in the stream pipeline that follows, can you identify the intermediate and terminal
operations?
Long count = menu. Stream()
. Filter(dish -> dish. Getcalories() > 300)
. Distinct()
. Limit(3)
. Count();

answer:
the last operation in the stream pipeline count returns a long, which is a non-stream
value. Its therefore a terminal operation. All previous operations, filter, distinct,
limit, are connected and return a stream. They are therefore intermediate operations.

4.4.3 working with streams
to summarize, working with streams in general involves three items:
 a data source (such as a collection) to perform a query on
 a chain of intermediate operations that form a stream pipeline
 a terminal operation that executes the stream pipeline and produces a result
96 chapter 4 introducing streams

the idea behind a stream pipeline is similar to the builder pattern (see http: //en
. Wikipedia. Org/wiki/builder_pattern). In the builder pattern, theres a chain of calls
to set up a configuration (for streams this is a chain of intermediate operations), fol-
lowed by a call to a build method (for streams this is a terminal operation).
For convenience, tables 4.1 and 4.2 summarize the intermediate and terminal
stream operations youve seen in the code examples so far. Note that this is an incom-
plete list of operations provided by the streams api; youll see several more in the
next chapter!

Table 4.1 intermediate operations

operation type return type argument of the operation function descriptor

filter intermediate stream<t> predicate<t> t -> boolean

map intermediate stream<r> function<t, r> t -> r

limit intermediate stream<t>

sorted intermediate stream<t> comparator<t> (t, t) -> int

distinct intermediate stream<t>

table 4.2 terminal operations

operation type return type purpose

foreach terminal void consumes each element from a stream and applies a
lambda to each of them.

Count terminal long returns the number of elements in a stream.

Collect terminal (generic) reduces the stream to create a collection such as a
list, a map, or even an integer. See chapter 6 for
more detail.

4.5 road map
in the next chapter, well detail the available stream operations with use cases so you
can see what kinds of queries you can express with them. We look at many patterns
such as filtering, slicing, finding, matching, mapping, and reducing, which can be
used to express sophisticated data-processing queries.
Chapter 6 then explores collectors in detail. In this chapter we have only made use
of the collect() terminal operation on streams (see table 4.2) in the stylized form of
collect(tolist()), which creates a list whose elements are the same as those of the
stream its applied to.
Summary 97

summary
 a stream is a sequence of elements from a source that supports data-processing
operations.
 Streams make use of internal iteration: the iteration is abstracted away through
operations such as filter, map, and sorted.
 There are two types of stream operations: intermediate and terminal operations.
 Intermediate operations such as filter and map return a stream and can be
chained together. Theyre used to set up a pipeline of operations but dont pro-
duce any result.
 Terminal operations such as foreach and count return a non-stream value and
process a stream pipeline to return a result.
 The elements of a stream are computed on demand (lazily).
Working with streams

this chapter covers
 filtering, slicing, and mapping
 finding, matching, and reducing
 using numeric streams (primitive stream
specializations)
 creating streams from multiple sources
 infinite streams

in the previous chapter, you saw that streams let you move from external iteration to
internal iteration. Instead of writing code, as follows, where you explicitly manage
the iteration over a collection of data (external iteration),
list<dish> vegetariandishes = new arraylist<>();
for(dish d: menu) {
if(d. Isvegetarian()){
vegetariandishes. Add(d);
}
}

you can use the streams api (internal iteration), which supports the filter and
collect operations, to manage the iteration over the collection of data for you. All
you need to do is pass the filtering behavior as argument to the filter method:

98
filtering 99

import static java. Util. Stream. Collectors. Tolist;
list<dish> vegetariandishes =
menu. Stream()
. Filter(dish: : isvegetarian)
. Collect(tolist());

this different way of working with data is useful because you let the streams api man-
age how to process the data. As a consequence, the streams api can work out several
optimizations behind the scenes. In addition, using internal iteration, the streams api
can decide to run your code in parallel. Using external iteration, this isnt possible
because youre committed to a single-threaded step-by-step sequential iteration.
In this chapter, youll have an extensive look at the various operations supported
by the streams api. You will learn about operations available in java 8 and also new
additions in java 9. These operations will let you express complex data-processing que-
ries such as filtering, slicing, mapping, finding, matching, and reducing. Next, well
explore special cases of streams: numeric streams, streams built from multiple sources
such as files and arrays, and finally infinite streams.

5.1 filtering
in this section, well look at the ways to select elements of a stream: filtering with a
predicate and filtering only unique elements.

5.1.1 filtering with a predicate
the stream interface supports a filter method (which you should be familiar with
by now). This operation takes as argument a predicate (a function returning a boolean)
and returns a stream including all elements that match the predicate. For example,
you can create a vegetarian menu by filtering all vegetarian dishes as illustrated in fig-
ure 5.1 and the code that follows it:

menu stream

stream<dish>

filter(dish: : isvegetarian)

stream<dish>

collect(tolist())

list<dish>

figure 5.1 filtering a stream with a predicate
100 chapter 5 working with streams

list<dish> vegetarianmenu = menu. Stream() use a method
. Filter(dish: : isvegetarian) reference to
. Collect(tolist()); check if a dish is
vegetarian friendly.

5.1.2 filtering unique elements
streams also support a method called distinct that returns a stream with unique ele-
ments (according to the implementation of the hashcode and equals methods of the
objects produced by the stream). For example, the following code filters all even num-
bers from a list and then eliminates duplicates (using the equals method for the com-
parison). Figure 5.2 shows this visually.

List<integer> numbers = arrays. Aslist(1, 2, 1, 3, 3, 2, 4);
numbers. Stream()
. Filter(i -> i % 2 == 0)
. Distinct()
. Foreach(system. Out: : println);

numbers stream

1 2 1 3 3 2 4 stream<integer>

filter(i -> i % 2 == 0)

2 2 4 stream<integer>

distinct()

2 4 stream<integer>

foreach(system. Out: : println)

system. Out. Println(2);
void
system. Out. Println(4);

figure 5.2 filtering unique elements in a stream

5.2 slicing a stream
in this section, well discuss how to select and skip elements in a stream in different
ways. There are operations available that let you efficiently select or drop elements
using a predicate, ignore the first few elements of a stream, or truncate a stream to a
given size.
Slicing a stream 101

5.2.1 slicing using a predicate
java 9 added two new methods that are useful for efficiently selecting elements in a
stream: takewhile and dropwhile.
Using takewhile
lets say you have the following special list of dishes:

list<dish> specialmenu = arrays. Aslist(
new dish("seasonal fruit", true, 120, dish. Type. Other),
new dish("prawns", false, 300, dish. Type. Fish),
new dish("rice", true, 350, dish. Type. Other),
new dish("chicken", false, 400, dish. Type. Meat),
new dish("french fries", true, 530, dish. Type. Other));

how would you select the dishes that have fewer than 320 calories? Instinctively, you
know already from the previous section that the operation filter can be used as
follows:

list<dish> filteredmenu
= specialmenu. Stream()
. Filter(dish -> dish. Getcalories() < 320) lists seasonal
. Collect(tolist());
fruit, prawns

but, youll notice that the initial list was already sorted on the number of calories! The
downside of using the filter operation here is that you need to iterate through the
whole stream and the predicate is applied to each element. Instead, you could stop
once you found a dish that is greater than (or equal to) 320 calories. With a small list
this may not seem like a huge benefit, but it can become useful if you work with poten-
tially large stream of elements. But how do you specify this? The takewhile operation
is here to rescue you! It lets you slice any stream (even an infinite stream as you will
learn later) using a predicate. But thankfully, it stops once it has found an element
that fails to match. Heres how you can use it:

list<dish> slicedmenu1
= specialmenu. Stream()
. Takewhile(dish -> dish. Getcalories() < 320) lists seasonal
. Collect(tolist());
fruit, prawns

using dropwhile
how about getting the other elements though? How about finding the elements that
have greater than 320 calories? You can use the dropwhile operation for this:

list<dish> slicedmenu2
= specialmenu. Stream()
. Dropwhile(dish -> dish. Getcalories() < 320) lists rice, chicken,
. Collect(tolist());
french fries
102 chapter 5 working with streams

the dropwhile operation is the complement of takewhile. It throws away the ele-
ments at the start where the predicate is false. Once the predicate evaluates to true it
stops and returns all the remaining elements, and it even works if there are an infinite
number of remaining elements!

5.2.2 truncating a stream
streams support the limit(n) method, which returns another stream thats no lon-
ger than a given size. The requested size is passed as argument to limit. If the
stream is ordered, the first elements are returned up to a maximum of n. For exam-
ple, you can create a list by selecting the first three dishes that have more than 300
calories as follows:

list<dish> dishes = specialmenu
. Stream()
. Filter(dish -> dish. Getcalories() > 300)
lists rice, chicken, . Limit(3)
french fries . Collect(tolist());

figure 5.3 illustrates a combination of filter and limit. You can see that only the
first three elements that match the predicate are selected, and the result is immedi-
ately returned.

Menu stream

stream<dish>

filter(d -> d. Getcalories() > 300)

stream<dish>

limit(3)

stream<dish>

collect(tolist())

list<dish>

figure 5.3 truncating a stream
slicing a stream 103

note that limit also works on unordered streams (for example, if the source is a set).
In this case you shouldnt assume any order on the result produced by limit.

5.2.3 skipping elements
streams support the skip(n) method to return a stream that discards the first n ele-
ments. If the stream has fewer than n elements, an empty stream is returned. Note that
limit(n) and skip(n) are complementary! For example, the following code skips the
first two dishes that have more than 300 calories and returns the rest. Figure 5.4 illus-
trates this query.

List<dish> dishes = menu. Stream()
. Filter(d -> d. Getcalories() > 300)
. Skip(2)
. Collect(tolist());

menu stream

stream<dish>

filter(d -> d. Getcalories() > 300)

stream<dish>

skip(2)

stream<dish>

collect(tolist())

list<dish>

figure 5.4 skipping elements in a stream

put what youve learned in this section into practice with quiz 5.1 before we move to
mapping operations.
104 chapter 5 working with streams

quiz 5.1: filtering
how would you use streams to filter the first two meat dishes?
Answer:
you can solve this problem by composing the methods filter and limit together
and using collect(tolist()) to convert the stream into a list as follows:
list<dish> dishes =
menu. Stream()
. Filter(dish -> dish. Gettype() == dish. Type. Meat)
. Limit(2)
. Collect(tolist());

5.3 mapping
a common data processing idiom is to select information from certain objects. For
example, in sql you can select a particular column from a table. The streams api
provides similar facilities through the map and flatmap methods.

5.3.1 applying a function to each element of a stream
streams support the map method, which takes a function as argument. The function is
applied to each element, mapping it into a new element (the word mapping is used
because it has a meaning similar to transforming but with the nuance of creating a new
version of rather than modifying). For example, in the following code you pass a
method reference dish: : getname to the map method to extract the names of the dishes
in the stream:
list<string> dishnames = menu. Stream()
. Map(dish: : getname)
. Collect(tolist());

because the method getname returns a string, the stream outputted by the map method
is of type stream<string>.
Lets take a slightly different example to solidify your understanding of map. Given
a list of words, youd like to return a list of the number of characters for each word.
How would you do it? Youd need to apply a function to each element of the list. This
sounds like a job for the map method! The function to apply should take a word and
return its length. You can solve this problem, as follows, by passing a method refer-
ence string: : length to map:
list<string> words = arrays. Aslist("modern", "java", "in", "action");
list<integer> wordlengths = words. Stream()
. Map(string: : length)
. Collect(tolist());

lets return to the example where you extracted the name of each dish. What if you
wanted to find out the length of the name of each dish? You could do this by chaining
another map as follows:
mapping 105

list<integer> dishnamelengths = menu. Stream()
. Map(dish: : getname)
. Map(string: : length)
. Collect(tolist());

5.3.2 flattening streams
you saw how to return the length for each word in a list using the map method. Lets
extend this idea a bit further: how could you return a list of all the unique characters for
a list of words? For example, given the list of words ["hello, " "world"] youd like to
return the list ["h, " "e, " "l, " "o, " "w, " "r, " "d"].
You might think that this is easy, that you can map each word into a list of charac-
ters and then call distinct to filter duplicate characters. A first go could be like the
following:
words. Stream()
. Map(word -> word. Split(""))
. Distinct()
. Collect(tolist());

the problem with this approach is that the lambda passed to the map method returns
a string[] (an array of string) for each word. The stream returned by the map
method is of type stream<string[]>. What you want is stream<string> to represent
a stream of characters. Figure 5.5 illustrates the problem.

Stream of words

hello world stream<string>

map(s -> s. Split(""))

h e l l o w o r l d stream<string[]>

distinct()

h e l l o w o r l d stream<string[]>

collect(tolist())

h e l l o w o r l d list<string[]>

figure 5.5 incorrect use of map to find unique characters from a list of words
106 chapter 5 working with streams

luckily theres a solution to this problem using the method flatmap! Lets see step-
by-step how to solve it.
Attempt using map and arrays. Stream
first, you need a stream of characters instead of a stream of arrays. Theres a method
called arrays. Stream()that takes an array and produces a stream:
string[] arrayofwords = {"goodbye", "world"};
stream<string> streamofwords = arrays. Stream(arrayofwords);

use it in the previous pipeline to see what happens:
converts each word into an
words. Stream()
array of its individual letters
. Map(word -> word. Split(""))
. Map(arrays: : stream)
. Distinct() makes each array into
. Collect(tolist());
a separate stream

the current solution still doesnt work! This is because you now end up with a list of
streams (more precisely, list<stream<string>>). Indeed, you first convert each word
into an array of its individual letters and then make each array into a separate stream.
Using flatmap
you can fix this problem by using flatmap as follows:
list<string> uniquecharacters =
converts each word into an
words. Stream()
array of its individual letters
. Map(word -> word. Split(""))
. Flatmap(arrays: : stream)
. Distinct() flattens each generated
. Collect(tolist());
stream into a single stream

using the flatmap method has the effect of mapping each array not with a stream but
with the contents of that stream. All the separate streams that were generated when using
map(arrays: : stream) get amalgamatedflattened into a single stream. Figure 5.6
illustrates the effect of using the flatmap method. Compare it with what map does in
figure 5.5.
In a nutshell, the flatmap method lets you replace each value of a stream with
another stream and then concatenates all the generated streams into a single stream.
Well come back to flatmap in chapter 11 when we discuss more advanced java 8
patterns such as using the new library class optional for null checking. To solidify
your understanding of map and flatmap, try out quiz 5.2.
Mapping 107

stream of words

hello world stream<string>

map(s -> s. Split(""))

h e l l o w o r l d stream<string[]>

flatmap(arrays: : stream)

h e l l o w o r l d stream<string>

distinct()

h e l o w r d stream<string>

collect(tolist())

h e l o w r d list<string>

figure 5.6 using flatmap to find the unique characters from a list of words

quiz 5.2: mapping
1. Given a list of numbers, how would you return a list of the square of each number?
For example, given [1, 2, 3, 4, 5] you should return [1, 4, 9, 16, 25].
Answer:
you can solve this problem by using map with a lambda that takes a number and
returns the square of the number:
list<integer> numbers = arrays. Aslist(1, 2, 3, 4, 5);
list<integer> squares =
numbers. Stream()
. Map(n -> n * n)
. Collect(tolist());

2. Given two lists of numbers, how would you return all pairs of numbers? For example,
given a list [1, 2, 3] and a list [3, 4] you should return [(1, 3), (1, 4), (2, 3), (2, 4),
(3, 3), (3, 4)]. For simplicity, you can represent a pair as an array with two elements.
108 chapter 5 working with streams

(continued)
answer:
you could use two maps to iterate on the two lists and generate the pairs. But this
would return a stream<stream<integer[]>>. What you need to do is flatten the
generated streams to result in a stream<integer[]>. This is what flatmap is for:

list<integer> numbers1 = arrays. Aslist(1, 2, 3);
list<integer> numbers2 = arrays. Aslist(3, 4);
list<int[]> pairs =
numbers1. Stream()
. Flatmap(i -> numbers2. Stream()
. Map(j -> new int[]{i, j})
)
. Collect(tolist());

3. How would you extend the previous example to return only pairs whose sum is
divisible by 3?
Answer:
you saw earlier that filter can be used with a predicate to filter elements from a
stream. Because after the flatmap operation you have a stream of int[] that rep-
resent a pair, you only need a predicate to check if the sum is divisible by 3:

list<integer> numbers1 = arrays. Aslist(1, 2, 3);
list<integer> numbers2 = arrays. Aslist(3, 4);
list<int[]> pairs =
numbers1. Stream()
. Flatmap(i ->
numbers2. Stream()
. Filter(j -> (i + j) % 3 == 0)
. Map(j -> new int[]{i, j})
)
. Collect(tolist());

the result is [(2, 4), (3, 3)].

5.4 finding and matching
another common data processing idiom is finding whether some elements in a set of
data match a given property. The streams api provides such facilities through the
allmatch, anymatch, nonematch, findfirst, and findany methods of a stream.

5.4.1 checking to see if a predicate matches at least one element
the anymatch method can be used to answer the question is there an element in the
stream matching the given predicate? For example, you can use it to find out whether
the menu has a vegetarian option:
finding and matching 109

if(menu. Stream(). Anymatch(dish: : isvegetarian)) {
system. Out. Println("the menu is (somewhat) vegetarian friendly! ! ");
}

the anymatch method returns a boolean and is therefore a terminal operation.

5.4.2 checking to see if a predicate matches all elements
the allmatch method works similarly to anymatch but will check to see if all the ele-
ments of the stream match the given predicate. For example, you can use it to find out
whether the menu is healthy (all dishes are below 1000 calories):
boolean ishealthy = menu. Stream()
. Allmatch(dish -> dish. Getcalories() < 1000);

nonematch
the opposite of allmatch is nonematch. It ensures that no elements in the stream
match the given predicate. For example, you could rewrite the previous example as
follows using nonematch:
boolean ishealthy = menu. Stream()
. Nonematch(d -> d. Getcalories() >= 1000);

these three operationsanymatch, allmatch, and nonematchmake use of what we
call short-circuiting, a stream version of the familiar java short-circuiting && and ||
operators.

Short-circuiting evaluation
some operations dont need to process the whole stream to produce a result. For
example, say you need to evaluate a large boolean expression chained with and
operators. You need only find out that one expression is false to deduce that the
whole expression will return false, no matter how long the expression is; theres no
need to evaluate the entire expression. This is what short-circuiting refers to.
In relation to streams, certain operations such as allmatch, nonematch, findfirst,
and findany dont need to process the whole stream to produce a result. As soon
as an element is found, a result can be produced. Similarly, limit is also a short-
circuiting operation. The operation only needs to create a stream of a given size with-
out processing all the elements in the stream. Such operations are useful (for exam-
ple, when you need to deal with streams of infinite size, because they can turn an
infinite stream into a stream of finite size). Well show examples of infinite streams
in section 5.7.

5.4.3 finding an element
the findany method returns an arbitrary element of the current stream. It can be
used in conjunction with other stream operations. For example, you may wish to find
110 chapter 5 working with streams

a dish thats vegetarian. You can combine the filter method and findany to express
this query:
optional<dish> dish =
menu. Stream()
. Filter(dish: : isvegetarian)
. Findany();

the stream pipeline will be optimized behind the scenes to perform a single pass and
finish as soon as a result is found by using short-circuiting. But wait a minute; whats
this optional thing in the code?
Optional in a nutshell
the optional<t> class (java. Util. Optional) is a container class to represent the
existence or absence of a value. In the previous code, its possible that findany doesnt
find any element. Instead of returning null, which is well known for being error-
prone, the java 8 library designers introduced optional<t>. We wont go into the
details of optional here, because well show in detail in chapter 11 how your code can
benefit from using optional to avoid bugs related to null checking. But for now, its
good to know that there are a few methods available in optional that force you to
explicitly check for the presence of a value or deal with the absence of a value:
 ispresent() returns true if optional contains a value, false otherwise.
 Ifpresent(consumer<t> block) executes the given block if a value is present.
We introduced the consumer functional interface in chapter 3; it lets you pass a
lambda that takes an argument of type t and returns void.
 T get() returns the value if present; otherwise it throws a nosuchelement-
exception.
 T orelse(t other) returns the value if present; otherwise it returns a default
value.
For example, in the previous code youd need to explicitly check for the presence of a
dish in the optional object to access its name:

menu. Stream() returns an
. Filter(dish: : isvegetarian) optional<dish>. If a value is contained,
. Findany() its printed; otherwise
. Ifpresent(dish -> system. Out. Println(dish. Getname()); nothing happens.

5.4.4 finding the first element
some streams have an encounter order that specifies the order in which items logically
appear in the stream (for example, a stream generated from a list or from a sorted
sequence of data). For such streams you may wish to find the first element. Theres
the findfirst method for this, which works similarly to findany (for example, the
code that follows, given a list of numbers, finds the first square thats divisible by 3):
reducing 111

list<integer> somenumbers = arrays. Aslist(1, 2, 3, 4, 5);
optional<integer> firstsquaredivisiblebythree =
somenumbers. Stream()
. Map(n -> n * n)
. Filter(n -> n % 3 == 0)
. Findfirst(); // 9

when to use findfirst and findany
you may wonder why we have both findfirst and findany. The answer is parallel-
ism. Finding the first element is more constraining in parallel. If you dont care about
which element is returned, use findany because its less constraining when using
parallel streams.

5.5 reducing
the terminal operations youve seen either return a boolean (allmatch and so on),
void (foreach), or an optional object (findany and so on). Youve also been using
collect to combine all elements in a stream into a list.
In this section, youll see how you can combine elements of a stream to express
more complicated queries such as calculate the sum of all calories in the menu, or
what is the highest calorie dish in the menu? Using the reduce operation. Such
queries combine all the elements in the stream repeatedly to produce a single value
such as an integer. These queries can be classified as reduction operations (a stream
is reduced to a value). In functional programming-language jargon, this is referred
to as a fold because you can view this operation as repeatedly folding a long piece of
paper (your stream) until it forms a small square, which is the result of the fold
operation.

5.5.1 summing the elements
before we investigate how to use the reduce method, it helps to first see how youd
sum the elements of a list of numbers using a for-each loop:
int sum = 0;
for (int x : numbers) {
sum += x;
}

each element of numbers is combined iteratively with the addition operator to form a
result. You reduce the list of numbers into one number by repeatedly using addition.
There are two parameters in this code:
 the initial value of the sum variable, in this case 0
 the operation to combine all the elements of the list, in this case +

wouldnt it be great if you could also multiply all the numbers without having to repeat-
edly copy and paste this code? This is where the reduce operation, which abstracts over
112 chapter 5 working with streams

this pattern of repeated application, can help. You can sum all the elements of a stream
as follows:

int sum = numbers. Stream(). Reduce(0, (a, b) -> a + b);

reduce takes two arguments:
 an initial value, here 0.
 A binaryoperator<t> to combine two elements and produce a new value; here
you use the lambda (a, b) -> a + b.
You could just as easily multiply all the elements by passing a different lambda, (a, b)
-> a * b, to the reduce operation:

int product = numbers. Stream(). Reduce(1, (a, b) -> a * b);

figure 5.7 illustrates how the reduce operation works on a stream: the lambda com-
bines each element repeatedly until the stream containing the integers 4, 5, 3, 9, are
reduced to a single value.

Numbers stream

4 5 3 9 stream<integer>

reduce(0, (a, b) -> a + b)

0 +

4 +

9 +

12 +

21 integer

figure 5.7 using reduce to sum the numbers in a stream

lets take an in-depth look into how the reduce operation happens to sum a stream of
numbers. First, 0 is used as the first parameter of the lambda (a), and 4 is consumed
from the stream and used as the second parameter (b). 0 + 4 produces 4, and it
reducing 113

becomes the new accumulated value. Then the lambda is called again with the accu-
mulated value and the next element of the stream, 5, which produces the new accumu-
lated value, 9. Moving forward, the lambda is called again with the accumulated value
and the next element, 3, which produces 12. Finally, the lambda is called with 12 and
the last element of the stream, 9, which produces the final value, 21.
You can make this code more concise by using a method reference. From java 8
the integer class now comes with a static sum method to add two numbers, which is
what you want instead of repeatedly writing out the same code as lambda:

int sum = numbers. Stream(). Reduce(0, integer: : sum);

no initial value
theres also an overloaded variant of reduce that doesnt take an initial value, but it
returns an optional object:

optional<integer> sum = numbers. Stream(). Reduce((a, b) -> (a + b));

why does it return an optional<integer>? Consider the case when the stream con-
tains no elements. The reduce operation cant return a sum because it doesnt have
an initial value. This is why the result is wrapped in an optional object to indicate that
the sum may be absent. Now see what else you can do with reduce.

5.5.2 maximum and minimum
it turns out that reduction is all you need to compute maxima and minima as well!
Lets see how you can apply what you just learned about reduce to calculate the maxi-
mum or minimum element in a stream. As you saw, reduce takes two parameters:
 an initial value
 a lambda to combine two stream elements and produce a new value

the lambda is applied step-by-step to each element of the stream with the addition
operator, as shown in figure 5.7. You need a lambda that, given two elements, returns
the maximum of them. The reduce operation will use the new value with the next ele-
ment of the stream to produce a new maximum until the whole stream is consumed!
You can use reduce as follows to calculate the maximum in a stream; this is illustrated
in figure 5.8.

Optional<integer> max = numbers. Stream(). Reduce(integer: : max);

to calculate the minimum, you need to pass integer. Min to the reduce operation
instead of integer. Max:

optional<integer> min = numbers. Stream(). Reduce(integer: : min);

you could have equally well used the lambda (x, y) -> x < y ? X : y instead of
integer: : min, but the latter is definitely easier to read!
114 chapter 5 working with streams

numbers stream

4 5 3 9 stream<integer>

reduce(integer: : max)

4 max

5 max

5 max

9 optional<integer>

figure 5.8 a reduce operationcalculating the maximum

to test your understanding of the reduce operation, try quiz 5.3.

Quiz 5.3: reducing
how would you count the number of dishes in a stream using the map and reduce
methods?
Answer:
you can solve this problem by mapping each element of a stream into the number 1
and then summing them using reduce! This is equivalent to counting, in order, the
number of elements in the stream:
int count = menu. Stream()
. Map(d -> 1)
. Reduce(0, (a, b) -> a + b);

a chain of map and reduce is commonly known as the map-reduce pattern, made
famous by googles use of it for web searching because it can be easily parallelized.
Note that in chapter 4 you saw the built-in method count to count the number of ele-
ments in the stream:

long count = menu. Stream(). Count();
reducing 115

benefit of the reduce method and parallelism
the benefit of using reduce compared to the step-by-step iteration summation that
you wrote earlier is that the iteration is abstracted using internal iteration, which
enables the internal implementation to choose to perform the reduce operation in
parallel. The iterative summation example involves shared updates to a sum variable,
which doesnt parallelize gracefully. If you add in the needed synchronization, youll
likely discover that thread contention robs you of all the performance that parallelism
was supposed to give you! Parallelizing this computation requires a different approach:
partition the input, sum the partitions, and combine the sums. But now the code is
starting to look very different. Youll see what this looks like in chapter 7 using the
fork/join framework. But for now its important to realize that the mutable-accumulator
pattern is a dead end for parallelization. You need a new pattern, and this is what
reduce provides you. Youll also see in chapter 7 that to sum all the elements in par-
allel using streams, theres almost no modification to your code: stream() becomes
parallelstream():

int sum = numbers. Parallelstream(). Reduce(0, integer: : sum);

but theres a price to pay to execute this code in parallel, as well explain later: the
lambda passed to reduce cant change state (for example, instance variables), and the
operation needs to be associative and commutative so it can be executed in any order.

You have seen reduction examples that produced an integer: the sum of a stream,
the maximum of a stream, or the number of elements in a stream. Youll see, in sec-
tion 5.6, that additional built-in methods such as sum and max are available to help you
write slightly more concise code for common reduction patterns. Well investigate a
more complex form of reductions using the collect method in the next chapter. For
example, instead of reducing a stream into an integer, you can also reduce it into a
map if you want to group dishes by types.

Stream operations: stateless vs. Stateful
youve seen a lot of stream operations. An initial presentation can make them seem
a panacea. Everything works smoothly, and you get parallelism for free when you use
parallelstream instead of stream to get a stream from a collection.
Certainly for many applications this is the case, as youve seen in the previous exam-
ples. You can turn a list of dishes into a stream, filter to select various dishes of
a certain type, then map down the resulting stream to add on the number of calories,
and then reduce to produce the total number of calories of the menu. You can even
do such stream calculations in parallel. But these operations have different charac-
teristics. There are issues about what internal state they need to operate.
Operations like map and filter take each element from the input stream and produce
zero or one result in the output stream. These operations are in general stateless: they
dont have an internal state (assuming the user-supplied lambda or method reference
has no internal mutable state).
116 chapter 5 working with streams

(continued)
but operations like reduce, sum, and max need to have internal state to accumulate
the result. In this case the internal state is small. In our example it consisted of an
int or double. The internal state is of bounded size no matter how many elements
are in the stream being processed.
By contrast, some operations such as sorted or distinct seem at first to behave
like filter or mapall take a stream and produce another stream (an intermediate
operation)but theres a crucial difference. Both sorting and removing duplicates
from a stream require knowing the previous history to do their job. For example, sort-
ing requires all the elements to be buffered before a single item can be added to the
output stream; the storage requirement of the operation is unbounded. This can be
problematic if the data stream is large or infinite. (what should reversing the stream
of all prime numbers do? It should return the largest prime number, which mathemat-
ics tells us doesnt exist. ) we call these operations stateful operations.

Youve now seen a lot of stream operations that you can use to express sophisticated
data processing queries! Table 5.1 summarizes the operations seen so far. You get to
practice them in the next section through an exercise.

Table 5.1 intermediate and terminal operations

type/functional function
operation type return type
interface used descriptor

filter intermediate stream<t> predicate<t> t -> boolean

distinct intermediate stream<t>
(stateful-unbounded)

takewhile intermediate stream<t> predicate<t> t -> boolean

dropwhile intermediate stream<t> predicate<t> t -> boolean

skip intermediate stream<t> long
(stateful-bounded)

limit intermediate stream<t> long
(stateful-bounded)

map intermediate stream<r> function<t, r> t -> r

flatmap intermediate stream<r> function<t, t -> stream<r>
stream<r>>

sorted intermediate stream<t> comparator<t> (t, t) -> int
(stateful-unbounded)

anymatch terminal boolean predicate<t> t -> boolean

nonematch terminal boolean predicate<t> t -> boolean
putting it all into practice 117

table 5.1 intermediate and terminal operations (continued)

type/functional function
operation type return type
interface used descriptor

allmatch terminal boolean predicate<t> t -> boolean

findany terminal optional<t>

findfirst terminal optional<t>

foreach terminal void consumer<t> t -> void

collect terminal r collector<t, a, r>

reduce terminal optional<t> binaryoperator<t> (t, t) -> t
(stateful-bounded)

count terminal long

5.6 putting it all into practice
in this section, you get to practice what youve learned about streams so far. We now
explore a different domain: traders executing transactions. Youre asked by your
manager to find answers to eight queries. Can you do it? We give the solutions in
section 5.6.2, but you should try them yourself first to get some practice:
1 find all transactions in the year 2011 and sort them by value (small to high).
2 what are all the unique cities where the traders work?
3 find all traders from cambridge and sort them by name.
4 return a string of all traders names sorted alphabetically.
5 are any traders based in milan?
6 print the values of all transactions from the traders living in cambridge.
7 whats the highest value of all the transactions?
8 find the transaction with the smallest value.

5.6.1 the domain: traders and transactions
heres the domain youll be working with, a list of traders and transactions:
trader raoul = new trader("raoul", "cambridge");
trader mario = new trader("mario", "milan");
trader alan = new trader("alan", "cambridge");
trader brian = new trader("brian", "cambridge");
list<transaction> transactions = arrays. Aslist(
new transaction(brian, 2011, 300),
new transaction(raoul, 2012, 1000),
new transaction(raoul, 2011, 400),
new transaction(mario, 2012, 710),
new transaction(mario, 2012, 700),
new transaction(alan, 2012, 950)
);
118 chapter 5 working with streams

trader and transaction are classes defined as follows:
public class trader{
private final string name;
private final string city;
public trader(string n, string c){
this. Name = n;
this. City = c;
}
public string getname(){
return this. Name;
}
public string getcity(){
return this. City;
}
public string tostring(){
return "trader: "+this. Name + " in " + this. City;
}
}
public class transaction{
private final trader trader;
private final int year;
private final int value;
public transaction(trader trader, int year, int value){
this. Trader = trader;
this. Year = year;
this. Value = value;
}
public trader gettrader(){
return this. Trader;
}
public int getyear(){
return this. Year;
}
public int getvalue(){
return this. Value;
}
public string tostring(){
return "{" + this. Trader + ", " +
"year: "+this. Year+", " +
"value: " + this. Value +"}";
}
}

5.6.2 solutions
we now provide the solutions in the following code listings, so you can verify your
understanding of what youve learned so far. Well done!

Listing 5.1 finds all transactions in 2011 and sort by value (small to high)

passes a predicate to filter to
list<transaction> tr2011 = select transactions in year 2011
transactions. Stream()
. Filter(transaction -> transaction. Getyear() == 2011)
putting it all into practice 119

collects all the elements . Sorted(comparing(transaction: : getvalue))
sorts them by
of the resulting stream . Collect(tolist()); using the value of
into a list the transaction

listing 5.2 what are all the unique cities where the traders work?

Extracts the city from each trader
list<string> cities = associated with the transaction
transactions. Stream()
. Map(transaction -> transaction. Gettrader(). Getcity())
. Distinct()
. Collect(tolist()); selects only unique cities

you havent seen this yet, but you could also drop distinct() and use toset() instead,
which would convert the stream into a set. Youll learn more about it in chapter 6.
Set<string> cities =
transactions. Stream()
. Map(transaction -> transaction. Gettrader(). Getcity())
. Collect(toset());

listing 5.3 finds all traders from cambridge and sort them by name

list<trader> traders = extracts all traders
transactions. Stream() from the transactions
. Map(transaction: : gettrader)
. Filter(trader -> trader. Getcity(). Equals("cambridge"))
selects only the
traders from . Distinct()
. Sorted(comparing(trader: : getname)) removes any duplicates
cambridge
. Collect(tolist());
sorts the resulting stream
of traders by their names

listing 5.4 returns a string of all traders names sorted alphabetically

extracts all the names of the
string traderstr = traders as a stream of strings
transactions. Stream()
. Map(transaction -> transaction. Gettrader(). Getname())
. Distinct()
removes
duplicate . Sorted() sorts the names alphabetically
names . Reduce("", (n1, n2) -> n1 + n2);

combines the names one by one to form a
string that concatenates all the names

note that this solution is inefficient (all strings are repeatedly concatenated, which
creates a new string object at each iteration). In the next chapter, youll see a more
efficient solution that uses joining() as follows (which internally makes use of a
stringbuilder):
string traderstr =
transactions. Stream()
120 chapter 5 working with streams

. Map(transaction -> transaction. Gettrader(). Getname())
. Distinct()
. Sorted()
. Collect(joining());

listing 5.5 are any traders based in milan?

Boolean milanbased =
transactions. Stream()
. Anymatch(transaction -> transaction. Gettrader()
. Getcity()
. Equals("milan"));
pass a predicate to anymatch to
check if theres a trader from milan.

Listing 5.6 prints all transactions values from the traders living in cambridge

selects the transactions where
transactions. Stream()
the traders live in cambridge
. Filter(t -> "cambridge". Equals(t. Gettrader(). Getcity()))
prints . Map(transaction: : getvalue)
each value . Foreach(system. Out: : println); extracts the values
of these trades

listing 5.7 whats the highest value of all the transactions?

Optional<integer> highestvalue =
transactions. Stream() extracts the value of
. Map(transaction: : getvalue)
each transaction
. Reduce(integer: : max);
calculates the max of
the resulting stream

listing 5.8 finds the transaction with the smallest value

optional<transaction> smallesttransaction =
finds the smallest transaction by
repeatedly comparing the values
transactions. Stream()
of each transaction
. Reduce((t1, t2) ->
t1. Getvalue() < t2. Getvalue() ? T1 : t2);

you can do better. A stream supports the methods min and max that take a comparator
as argument to specify which key to compare with when calculating the minimum
or maximum:
optional<transaction> smallesttransaction =
transactions. Stream()
. Min(comparing(transaction: : getvalue));
numeric streams 121

5.7 numeric streams
you saw earlier that you could use the reduce method to calculate the sum of the ele-
ments of a stream. For example, you can calculate the number of calories in the menu
as follows:
int calories = menu. Stream()
. Map(dish: : getcalories)
. Reduce(0, integer: : sum);

the problem with this code is that theres an insidious boxing cost. Behind the scenes
each integer needs to be unboxed to a primitive before performing the summation.
In addition, wouldnt it be nicer if you could call a sum method directly as follows?
Int calories = menu. Stream()
. Map(dish: : getcalories)
. Sum();

but this isnt possible. The problem is that the method map generates a stream<t>.
Even though the elements of the stream are of type integer, the streams interface
doesnt define a sum method. Why not? Say you had only a stream<dish> like the
menu; it wouldnt make any sense to be able to sum dishes. But dont worry; the
streams api also supplies primitive stream specializations that support specialized meth-
ods to work with streams of numbers.

5.7.1 primitive stream specializations
java 8 introduces three primitive specialized stream interfaces to tackle this issue, int-
stream, doublestream, and longstream, which respectively specialize the elements of
a stream to be int, long, and doubleand thereby avoid hidden boxing costs. Each
of these interfaces brings new methods to perform common numeric reductions,
such as sum to calculate the sum of a numeric stream and max to find the maximum
element. In addition, they have methods to convert back to a stream of objects when
necessary. The thing to remember is that the additional complexity of these specializa-
tions isnt inherent to streams. It reflects the complexity of boxingthe (efficiency-
based) difference between int and integer and so on.
Mapping to a numeric stream
the most common methods youll use to convert a stream to a specialized version are
maptoint, maptodouble, and maptolong. These methods work exactly like the method
map that you saw earlier but return a specialized stream instead of a stream<t>. For
example, you can use maptoint as follows to calculate the sum of calories in the menu:
int calories = menu. Stream()
returns a stream<dish>
. Maptoint(dish: : getcalories)
. Sum(); returns an intstream

here, the method maptoint extracts all the calories from each dish (represented as an
integer) and returns an intstream as the result (rather than a stream<integer>).
122 chapter 5 working with streams

you can then call the sum method defined on the intstream interface to calculate the
sum of calories! Note that if the stream were empty, sum would return 0 by default.
Intstream also supports other convenience methods such as max, min, and average.
Converting back to a stream of objects
similarly, once you have a numeric stream, you may be interested in converting it back
to a nonspecialized stream. For example, the operations of an intstream are
restricted to produce primitive integers: the map operation of an intstream takes a
lambda that takes an int and produces an int (an intunaryoperator). But you may
want to produce a different value such as a dish. For this you need to access the oper-
ations defined in the streams interface that are more general. To convert from a
primitive stream to a general stream (each int will be boxed to an integer) you can
use the method boxed, as follows:
converts a stream to
a numeric stream
intstream intstream = menu. Stream(). Maptoint(dish: : getcalories);
stream<integer> stream = intstream. Boxed();
converts the numeric
stream to a stream

youll learn in the next section that boxed is particularly useful when you deal with
numeric ranges that need to be boxed into a general stream.
Default values: optionalint
the sum example was convenient because it has a default value: 0. But if you want to
calculate the maximum element in an intstream, youll need something different
because 0 is a wrong result. How can you differentiate that the stream has no element
and that the real maximum is 0? Earlier we introduced the optional class, which is a
container that indicates the presence or absence of a value. Optional can be parame-
terized with reference types such as integer, string, and so on. Theres a primitive
specialized version of optional as well for the three primitive stream specializations:
optionalint, optionaldouble, and optionallong.
For example, you can find the maximal element of an intstream by calling the max
method, which returns an optionalint:
optionalint maxcalories = menu. Stream()
. Maptoint(dish: : getcalories)
. Max();

you can now process the optionalint explicitly to define a default value if theres no
maximum:

int max = maxcalories. Orelse(1); provides an explicit default
maximum if theres no value
numeric streams 123

5.7.2 numeric ranges
a common use case when dealing with numbers is working with ranges of numeric val-
ues. For example, suppose youd like to generate all numbers between 1 and 100. Java
8 introduces two static methods available on intstream and longstream to help gen-
erate such ranges: range and rangeclosed. Both methods take the starting value of
the range as the first parameter and the end value of the range as the second parame-
ter. But range is exclusive, whereas rangeclosed is inclusive. Lets look at an example:

intstream evennumbers = intstream. Rangeclosed(1, 100)
represents
the range . Filter(n -> n % 2 == 0);
represents stream
1 to 100 system. Out. Println(evennumbers. Count()); of even numbers
represents 50 even from 1 to 100
numbers from 1 to 100
here you use the rangeclosed method to generate a range of all numbers from 1 to
100. It produces a stream so you can chain the filter method to select only even
numbers. At this stage no computation has been done. Finally, you call count on the
resulting stream. Because count is a terminal operation, it will process the stream and
return the result 50, which is the number of even numbers from 1 to 100, inclusive.
Note that by comparison, if you were using intstream. Range(1, 100) instead, the
result would be 49 even numbers because range is exclusive.

5.7.3 putting numerical streams into practice: pythagorean triples
now well look at a more difficult example so you can solidify what youve learned
about numeric streams and all the stream operations youve learned so far. Your mis-
sion, if you choose to accept it, is to create a stream of pythagorean triples.
Pythagorean triple
whats a pythagorean triple? We have to go back a few years in the past. In one of your
exciting math classes, you learned that the famous greek mathematician pythagoras
discovered that certain triples of numbers (a, b, c) satisfy the formula a * a + b * b =
c * c where a, b, and c are integers. For example, (3, 4, 5) is a valid pythagorean triple
because 3 * 3 + 4 * 4 = 5 * 5 or 9 + 16 = 25. There are an infinite number of such tri-
ples. For example, (5, 12, 13), (6, 8, 10), and (7, 24, 25) are all valid pythagorean tri-
ples. Such triples are useful because they describe the three side lengths of a right-
angled triangle, as illustrated in figure 5.9.
Representing a triple
where do you start? The first step is to define a triple. Instead of (more properly)
defining a new class to represent a triple, you can use an array of int with three ele-
ments. For example, new int[]{3, 4, 5} to represent the tuple (3, 4, 5). You can now
access each individual component of the tuple using array indexing.
Filtering good combinations
lets assume someone provides you with the first two numbers of the triple: a and b.
How do you know whether that will form a good combination? You need to test whether
124 chapter 5 working with streams

a * a + b * b = c * c

c a

b

figure 5.9 the pythagorean theorem

the square root of a * a + b * b is a whole number. This is expressed in java as math
. Sqrt(a*a + b*b) % 1 == 0. (given a floating-point number, x, in java its fractional part is
obtained using x % 1.0, and whole numbers like 5.0 have zero fractional part. ) our code
uses this idea in a filter operation (youll see how to use this later to form valid code):

filter(b -> math. Sqrt(a*a + b*b) % 1 == 0)

assuming that surrounding code has given a value for a, and assuming stream pro-
vides possible values for b, filter will select only those values for b that can form a
pythagorean triple with a.
Generating tuples
following the filter, you know that both a and b can form a correct combination.
You now need to create a triple. You can use the map operation to transform each ele-
ment into a pythagorean triple as follows:
stream. Filter(b -> math. Sqrt(a*a + b*b) % 1 == 0)
. Map(b -> new int[]{a, b, (int) math. Sqrt(a * a + b * b)});

generating b values
youre getting closer! You now need to generate values for b. You saw that stream
. Rangeclosed allows you to generate a stream of numbers in a given interval. You can
use it to provide numeric values for b, here 1 to 100:
intstream. Rangeclosed(1, 100)
. Filter(b -> math. Sqrt(a*a + b*b) % 1 == 0)
. Boxed()
. Map(b -> new int[]{a, b, (int) math. Sqrt(a * a + b * b)});
numeric streams 125

note that you call boxed after the filter to generate a stream<integer> from the
intstream returned by rangeclosed. This is because map returns an array of int for
each element of the stream. The map method from an intstream expects only
another int to be returned for each element of the stream, which isnt what you want!
You can rewrite this using the method maptoobj of an intstream, which returns an
object-valued stream:
intstream. Rangeclosed(1, 100)
. Filter(b -> math. Sqrt(a*a + b*b) % 1 == 0)
. Maptoobj(b -> new int[]{a, b, (int) math. Sqrt(a * a + b * b)});

generating a values
theres one crucial component that we assumed was given: the value for a. You now
have a stream that produces pythagorean triples provided the value a is known. How
can you fix this? Just like with b, you need to generate numeric values for a! The final
solution is as follows:
stream<int[]> pythagoreantriples =
intstream. Rangeclosed(1, 100). Boxed()
. Flatmap(a ->
intstream. Rangeclosed(a, 100)
. Filter(b -> math. Sqrt(a*a + b*b) % 1 == 0)
. Maptoobj(b ->
new int[]{a, b, (int)math. Sqrt(a * a + b * b)})
);

okay, whats the flatmap about? First, you create a numeric range from 1 to 100 to gen-
erate values for a. For each given value of a youre creating a stream of triples. Mapping
a value of a to a stream of triples would result in a stream of streams! The flatmap
method does the mapping and also flattens all the generated streams of triples into a
single stream. As a result, you produce a stream of triples. Note also that you change the
range of b to be a to 100. Theres no need to start the range at the value 1 because this
would create duplicate triples (for example, (3, 4, 5) and (4, 3, 5)).
Running the code
you can now run your solution and select explicitly how many triples youd like to
return from the generated stream using the limit operation that you saw earlier:
pythagoreantriples. Limit(5)
. Foreach(t ->
system. Out. Println(t[0] + ", " + t[1] + ", " + t[2]));

this will print
3, 4, 5
5, 12, 13
6, 8, 10
7, 24, 25
8, 15, 17
126 chapter 5 working with streams

can you do better?
The current solution isnt optimal because you calculate the square root twice. One
possible way to make your code more compact is to generate all triples of the form
(a*a, b*b, a*a+b*b) and then filter the ones that match your criteria:

stream<double[]> pythagoreantriples2 =
intstream. Rangeclosed(1, 100). Boxed()
. Flatmap(a ->
intstream. Rangeclosed(a, 100) produces triples
. Maptoobj(
the third element of the tuple b -> new double[]{a, b, math. Sqrt(a*a + b*b)})
must be a whole number. . Filter(t -> t[2] % 1 == 0));

5.8 building streams
hopefully, by now youre convinced that streams are powerful and useful to express
data-processing queries. You were able to get a stream from a collection using the
stream method. In addition, we showed you how to create numerical streams from a
range of numbers. But you can create streams in many more ways! This section shows
how you can create a stream from a sequence of values, from an array, from a file, and
even from a generative function to create infinite streams!

5.8.1 streams from values
you can create a stream with explicit values by using the static method stream. Of,
which can take any number of parameters. For example, in the following code you
create a stream of strings directly using stream. Of. You then convert the strings to
uppercase before printing them one by one:
stream<string> stream = stream. Of("modern ", "java ", "in ", "action");
stream. Map(string: : touppercase). Foreach(system. Out: : println);

you can get an empty stream using the empty method as follows:

stream<string> emptystream = stream. Empty();

5.8.2 stream from nullable
in java 9, a new method was added that lets you create a stream from a nullable object.
After playing with streams, you may have encountered a situation where you extracted
an object that may be null and then needs to be converted into a stream (or an empty
stream for null). For example, the method system. Getproperty returns null if there
is no property with the given key. To use it together with a stream, youd need to
explicitly check for null as follows:
string homevalue = system. Getproperty("home");
stream<string> homevaluestream
= homevalue == null ? Stream. Empty() : stream. Of(value);
building streams 127

using stream. Ofnullable you can rewrite this code more simply:
stream<string> homevaluestream
= stream. Ofnullable(system. Getproperty("home"));

this pattern can be particularly handy in conjunction with flatmap and a stream of
values that may include nullable objects:
stream<string> values =
stream. Of("config", "home", "user")
. Flatmap(key -> stream. Ofnullable(system. Getproperty(key)));

5.8.3 streams from arrays
you can create a stream from an array using the static method arrays. Stream, which
takes an array as parameter. For example, you can convert an array of primitive ints
into an intstream and then sum the intstream to produce an int, as follows:
int[] numbers = {2, 3, 5, 7, 11, 13};
int sum = arrays. Stream(numbers). Sum(); the sum is 41.

5.8.4 streams from files
javas nio api (non-blocking i/o), which is used for i/o operations such as process-
ing a file, has been updated to take advantage of the streams api. Many static meth-
ods in java. Nio. File. Files return a stream. For example, a useful method is
files. Lines, which returns a stream of lines as strings from a given file. Using what
youve learned so far, you could use this method to find out the number of unique
words in a file as follows:
streams are autocloseable so
long uniquewords = 0; theres no need for try-finally
try(stream<string> lines =
files. Lines(paths. Get("data. Txt"), charset. Defaultcharset())){
uniquewords = lines. Flatmap(line -> arrays. Stream(line. Split(" ")))
. Distinct() generates
. Count(); removes a stream
counts the number duplicates of words
}
of unique words
catch(ioexception e){
deals with the exception if one
}
occurs when opening the file

you use files. Lines to return a stream where each element is a line in the given file.
This call is surrounded by a try/catch block because the source of the stream is an
i/o resource. In fact, the call files. Lines will open an i/o resource, which needs to
be closed to avoid a leak. In the past, youd need an explicit finally block to do this.
Conveniently, the stream interface implements the interface autocloseable. This
means that the management of the resource is handled for you within the try block.
Once you have a stream of lines, you can then split each line into words by calling the
split method on line. Notice how you use flatmap to produce one flattened stream
128 chapter 5 working with streams

of words instead of multiple streams of words for each line. Finally, you count each dis-
tinct word in the stream by chaining the methods distinct and count.

5.8.5 streams from functions: creating infinite streams!
The streams api provides two static methods to generate a stream from a function:
stream. Iterate and stream. Generate. These two operations let you create what we
call an infinite stream, a stream that doesnt have a fixed size like when you create a
stream from a fixed collection. Streams produced by iterate and generate create
values on demand given a function and can therefore calculate values forever! Its
generally sensible to use limit(n) on such streams to avoid printing an infinite num-
ber of values.
Iterate
lets look at a simple example of how to use iterate before we explain it:
stream. Iterate(0, n -> n + 2)
. Limit(10)
. Foreach(system. Out: : println);

the iterate method takes an initial value, here 0, and a lambda (of type unary-
operator<t>) to apply successively on each new value produced. Here you return the
previous element added with 2 using the lambda n -> n + 2. As a result, the iterate
method produces a stream of all even numbers: the first element of the stream is the
initial value 0. Then it adds 2 to produce the new value 2; it adds 2 again to produce
the new value 4 and so on. This iterate operation is fundamentally sequential
because the result depends on the previous application. Note that this operation pro-
duces an infinite streamthe stream doesnt have an end because values are computed
on demand and can be computed forever. We say the stream is unbounded. As we dis-
cussed earlier, this is a key difference between a stream and a collection. Youre using
the limit method to explicitly limit the size of the stream. Here you select only the
first 10 even numbers. You then call the foreach terminal operation to consume the
stream and print each element individually.
In general, you should use iterate when you need to produce a sequence of suc-
cessive values (for example, a date followed by its next date: january 31, february 1,
and so on). To see a more difficult example of how you can apply iterate, try out
quiz 5.4.

Quiz 5.4: fibonacci tuples series
the fibonacci series is famous as a classic programming exercise. The numbers in
the following sequence are part of the fibonacci series: 0, 1, 1, 2, 3, 5, 8, 13, 21,
34, 55. . . . The first two numbers of the series are 0 and 1, and each subsequent
number is the sum of the previous two.
Building streams 129

the series of fibonacci tuples is similar; you have a sequence of a number and its
successor in the series: (0, 1), (1, 1), (1, 2), (2, 3), (3, 5), (5, 8), (8, 13), (13, 21). . . .

Your task is to generate the first 20 elements of the series of fibonacci tuples using
the iterate method!

Let us help you get started. The first problem is that the iterate method takes a
unaryoperator<t> as argument, and you need a stream of tuples such as (0, 1).
You can, again rather sloppily, use an array of two elements to represent a tuple. For
example, new int[]{0, 1} represents the first element of the fibonacci series (0,
1). This will be the initial value of the iterate method:

stream. Iterate(new int[]{0, 1}, ? ? ? )
. Limit(20)
. Foreach(t -> system. Out. Println("(" + t[0] + ", " + t[1] +")"));

in this quiz, you need to figure out the highlighted ? ? ? In the code. Remember that
iterate will apply the given lambda successively.

Answer:

stream. Iterate(new int[]{0, 1},
t -> new int[]{t[1], t[0]+t[1]})
. Limit(20)
. Foreach(t -> system. Out. Println("(" + t[0] + ", " + t[1] +")"));

how does it work? Iterate needs a lambda to specify the successor element. In the
case of the tuple (3, 5) the successor is (5, 3+5) = (5, 8). The next one is (8, 5+8).
Can you see the pattern? Given a tuple, the successor is (t[1], t[0] + t[1]). This is
what the following lambda specifies: t -> new int[]{t[1], t[0] + t[1]}. By running
this code youll get the series (0, 1), (1, 1), (1, 2), (2, 3), (3, 5), (5, 8), (8, 13), (13,
21). . . . Note that if you wanted to print the normal fibonacci series, you could use
a map to extract only the first element of each tuple:

stream. Iterate(new int[]{0, 1},
t -> new int[]{t[1], t[0] + t[1]})
. Limit(10)
. Map(t -> t[0])
. Foreach(system. Out: : println);

this code will produce the fibonacci series: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34 . . .

In java 9, the iterate method was enhanced with support for a predicate. For exam-
ple, you can generate numbers starting at 0 but stop the iteration once the number is
greater than 100:

intstream. Iterate(0, n -> n < 100, n -> n + 4)
. Foreach(system. Out: : println);
130 chapter 5 working with streams

the iterate method takes a predicate as its second argument that tells you when to
continue iterating up until. Note that you may think that you can use the filter oper-
ation to achieve the same result:
intstream. Iterate(0, n -> n + 4)
. Filter(n -> n < 100)
. Foreach(system. Out: : println);

unfortunately that isnt the case. In fact, this code wouldnt terminate! The reason is
that theres no way to know in the filter that the numbers continue to increase, so it
keeps on filtering them infinitely! You could solve the problem by using takewhile,
which would short-circuit the stream:
intstream. Iterate(0, n -> n + 4)
. Takewhile(n -> n < 100)
. Foreach(system. Out: : println);

but, you have to admit that iterate with a predicate is a bit more concise!
Generate
similarly to the method iterate, the method generate lets you produce an infinite
stream of values computed on demand. But generate doesnt apply successively a
function on each new produced value. It takes a lambda of type supplier<t> to pro-
vide new values. Lets look at an example of how to use it:
stream. Generate(math: : random)
. Limit(5)
. Foreach(system. Out: : println);

this code will generate a stream of five random double numbers from 0 to 1. For
example, one run gives the following:
0.9410810294106129
0.6586270755634592
0.9592859117266873
0.13743396659487006
0.3942776037651241

the static method math. Random is used as a generator for new values. Again you limit
the size of the stream explicitly using the limit method; otherwise the stream would
be unbounded!
You may be wondering whether theres anything else useful you can do using the
method generate. The supplier we used (a method reference to math. Random) was
stateless: it wasnt recording any values somewhere that can be used in later computa-
tions. But a supplier doesnt have to be stateless. You can create a supplier that stores
state that it can modify and use when generating the next value of the stream. As an
example, well show how you can also create the fibonacci series from quiz 5.4 using
generate so that you can compare it with the approach using the iterate method!
Building streams 131

but its important to note that a supplier thats stateful isnt safe to use in parallel
code. The stateful intsupplier for fibonacci is shown at the end of this chapter for
completeness but should generally be avoided! We discuss the problem of operations
with side effects and parallel streams further in chapter 7.
Well use an intstream in our example to illustrate code thats designed to avoid
boxing operations. The generate method on intstream takes an intsupplier
instead of a supplier<t>. For example, heres how to generate an infinite stream of
ones:

intstream ones = intstream. Generate(() -> 1);

you saw in chapter 3 that lambdas let you create an instance of a functional inter-
face by providing the implementation of the method directly inline. You can also
pass an explicit object, as follows, by implementing the getasint method defined in
the intsupplier interface (although this seems gratuitously long-winded, please bear
with us):

intstream twos = intstream. Generate(new intsupplier(){
public int getasint(){
return 2;
}
});

the generate method will use the given supplier and repeatedly call the getasint
method, which always returns 2. But the difference between the anonymous class used
here and a lambda is that the anonymous class can define state via fields, which the
getasint method can modify. This is an example of a side effect. All lambdas youve
seen so far were side-effect free; they didnt change any state.
To come back to our fibonacci tasks, what you need to do now is create an int-
supplier that maintains in its state the previous value in the series, so getasint can use
it to calculate the next element. In addition, it can update the state of the intsupplier
for the next time its called. The following code shows how to create an intsupplier
that will return the next fibonacci element when its called:

intsupplier fib = new intsupplier(){
private int previous = 0;
private int current = 1;
public int getasint(){
int oldprevious = this. Previous;
int nextvalue = this. Previous + this. Current;
this. Previous = this. Current;
this. Current = nextvalue;
return oldprevious;
}
};
intstream. Generate(fib). Limit(10). Foreach(system. Out: : println);
132 chapter 5 working with streams

the code creates an instance of intsupplier. This object has a mutable state: it tracks
the previous fibonacci element and the current fibonacci element in two instance
variables. The getasint method changes the state of the object when its called so
that it produces new values on each call. In comparison, our approach using iterate
was purely immutable; you didnt modify existing state but were creating new tuples at
each iteration. Youll learn in chapter 7 that you should always prefer an immutable
approach in order to process a stream in parallel and expect a correct result.
Note that because youre dealing with a stream of infinite size, you have to limit its
size explicitly using the operation limit; otherwise, the terminal operation (in this
case foreach) will compute forever. Similarly, you cant sort or reduce an infinite
stream because all elements need to be processed, but this would take forever because
the stream contains an infinite number of elements!

5.9 overview
its been a long but rewarding chapter! You can now process collections more effec-
tively. Indeed, streams let you express sophisticated data processing queries concisely.
In addition, streams can be parallelized transparently.

Summary
 the streams api lets you express complex data processing queries. Common
stream operations are summarized in table 5.1.
 You can filter and slice a stream using the filter, distinct, takewhile (java 9),
dropwhile (java 9), skip, and limit methods.
 The methods takewhile and dropwhile are more efficient than a filter when
you know that the source is sorted.
 You can extract or transform elements of a stream using the map and flatmap
methods.
 You can find elements in a stream using the findfirst and findany methods.
You can match a given predicate in a stream using the allmatch, nonematch,
and anymatch methods.
 These methods make use of short-circuiting: a computation stops as soon as a
result is found; theres no need to process the whole stream.
 You can combine all elements of a stream iteratively to produce a result using
the reduce method, for example, to calculate the sum or find the maximum of
a stream.
 Some operations such as filter and map are stateless: they dont store any state.
Some operations such as reduce store state to calculate a value. Some opera-
tions such as sorted and distinct also store state because they need to buffer
all the elements of a stream before returning a new stream. Such operations are
called stateful operations.
Summary 133

 there are three primitive specializations of streams: intstream, doublestream,
and longstream. Their operations are also specialized accordingly.
 Streams can be created not only from a collection but also from values, arrays,
files, and specific methods such as iterate and generate.
 An infinite stream has an infinite number of elements (for example all possible
strings). This is possible because the elements of a stream are only produced on
demand. You can get a finite stream from an infinite stream using methods such
as limit.
Collecting data
with streams

this chapter covers
 creating and using a collector with the
collectors class
 reducing streams of data to a single value
 summarization as a special case of reduction
 grouping and partitioning data
 developing your own custom collectors

you learned in the previous chapter that streams help you process collections with
database-like operations. You can view java 8 streams as fancy lazy iterators of sets of
data. They support two types of operations: intermediate operations such as filter
or map and terminal operations such as count, findfirst, foreach, and reduce.
Intermediate operations can be chained to convert a stream into another stream.
These operations dont consume from a stream; their purpose is to set up a pipe-
line of streams. By contrast, terminal operations do consume from a streamto
produce a final result (for example, returning the largest element in a stream).
They can often shorten computations by optimizing the pipeline of a stream.
We already used the collect terminal operation on streams in chapters 4 and 5,
but we employed it there mainly to combine all the elements of a stream into a

134
135

list. In this chapter, youll discover that collect is a reduction operation, like
reduce, that takes as an argument various recipes for accumulating the elements of a
stream into a summary result. These recipes are defined by a new collector inter-
face, so its important to distinguish collection, collector, and collect!
Here are some example queries of what youll be able to do using collect and
collectors:
 group a list of transactions by currency to obtain the sum of the values of all
transactions with that currency (returning a map<currency, integer>)
 partition a list of transactions into two groups: expensive and not expensive
(returning a map<boolean, list<transaction>>)
 create multilevel groupings, such as grouping transactions by cities and then
further categorizing by whether theyre expensive or not (returning a map<string,
map<boolean, list<transaction>>>)
excited? Great. Lets start by exploring an example that benefits from collectors.
Imagine a scenario where you have a list of transactions, and you want to group
them based on their nominal currency. Prior to java 8, even a simple use case like this
is cumbersome to implement, as shown in the following listing.

Listing 6.1 grouping transactions by currency in imperative style

iterates the list creates the map where the grouped
of transactions transaction will be accumulated
map<currency, list<transaction>> transactionsbycurrencies =
new hashmap<>();
for (transaction transaction : transactions) {
currency currency = transaction. Getcurrency(); extracts the
list<transaction> transactionsforcurrency = transactions currency
transactionsbycurrencies. Get(currency);
if (transactionsforcurrency == null) {
transactionsforcurrency = new arraylist<>();
transactionsbycurrencies
. Put(currency, transactionsforcurrency);
}
transactionsforcurrency. Add(transaction);
adds the currently traversed
}
transaction to the list of
transactions with the same
if theres no entry in the
currency
grouping map for this
currency, creates it

if youre an experienced java developer, youll probably feel comfortable writing
something like this, but you have to admit that its a lot of code for such a simple task.
Even worse, this is probably harder to read than to write! The purpose of the code
isnt immediately evident at first glance, even though it can be expressed in a straight-
forward manner in plain english: group a list of transactions by their currency. As
youll learn in this chapter, you can achieve exactly the same result with a single
136 chapter 6 collecting data with streams

statement by using a more general collector parameter to the collect method on
stream rather than the tolist special case used in the previous chapter:
map<currency, list<transaction>> transactionsbycurrencies =
transactions. Stream(). Collect(groupingby(transaction: : getcurrency));

the comparison is quite embarrassing, isnt it?

6.1 collectors in a nutshell
the previous example clearly shows one of the main advantages of functional-style
programming over an imperative approach: you have to formulate the result you want
to obtain the what and not the steps performed to obtain it, the how. In the previ-
ous example, the argument passed to the collect method is an implementation of
the collector interface, which is a recipe for how to build a summary of the elements
in the stream. In the previous chapter, the tolist recipe said, make a list of each ele-
ment in turn. In this example, the groupingby recipe says, make a map whose keys
are (currency) buckets and whose values are a list of elements in those buckets.
The difference between the imperative and functional versions of this example is
even more pronounced if you perform multilevel groupings: in that case the imperative
code quickly becomes harder to read, maintain, and modify due to the number of
deeply nested loops and conditions required. In comparison, the functional-style ver-
sion, as youll discover in section 6.3, can be easily enhanced with an additional collector.

6.1.1 collectors as advanced reductions
this last observation brings up another typical benefit of a well-designed functional
api: its higher degree of composability and reusability. Collectors are extremely use-
ful, because they provide a concise yet flexible way to define the criteria that collect
uses to produce the resulting collection. More specifically, invoking the collect method
on a stream triggers a reduction operation (parameterized by a collector) on the
elements of the stream itself. This reduction operation, illustrated in figure 6.1, does for
you internally what you had to code imperatively in listing 6.1. It traverses each ele-
ment of the stream and lets the collector process them.
Typically, the collector applies a transforming function to the element. Quite
often this is the identity transformation, which has no effect (for example, as in
tolist). The function then accumulates the result in a data structure that forms the
final output of this process. For instance, in our transaction-grouping example shown
previously, the transformation function extracts the currency from each transaction,
and subsequently the transaction itself is accumulated in the resulting map, using the
currency as key.
The implementation of the methods of the collector interface defines how to
perform a reduction operation on a stream, such as the one in our currency example.
Well investigate how to create customized collectors in sections 6.5 and 6.6. But the
collectors utility class provides lots of static factory methods to conveniently create
collectors in a nutshell 137

.
.
.
. Collector
stream result

transforming
function

2 extract the
transactions
. 1 traverse each currency 3 add the
.
. Transaction in currency/transaction
. The stream pair to the grouping
map

figure 6.1 the reduction process grouping the transactions by currency

an instance of the most common collectors that are ready to use. The most straight-
forward and frequently used collector is the tolist static method, which gathers all
the elements of a stream into a list:
list<transaction> transactions =
transactionstream. Collect(collectors. Tolist());

6.1.2 predefined collectors
in the rest of this chapter, well mainly explore the features of the predefined collec-
tors, those that can be created from the factory methods (such as groupingby) pro-
vided by the collectors class. These offer three main functionalities:
 reducing and summarizing stream elements to a single value
 grouping elements
 partitioning elements

we start with collectors that allow you to reduce and summarize. These are handy in a
variety of use cases, such as finding the total amount of the transacted values in the list
of transactions in the previous example.
Youll then see how to group the elements of a stream, generalizing the previous
example to multiple levels of grouping or combining different collectors to apply fur-
ther reduction operations on each of the resulting subgroups. Well also describe par-
titioning as a special case of grouping, using a predicate (a one-argument function
returning a boolean) as a grouping function.
At the end of section 6.4 youll find a table summarizing all the predefined col-
lectors explored in this chapter. Finally, in section 6.5 youll learn more about the
collector interface before you explore (in section 6.6) how you can create your own
138 chapter 6 collecting data with streams

custom collectors to be used in the cases not covered by the factory methods of the
collectors class.

6.2 reducing and summarizing
to illustrate the range of possible collector instances that can be created from the
collectors factory class, well reuse the domain we introduced in the previous chap-
ter: a menu consisting of a list of delicious dishes!
As you learned, collectors (the parameters to the stream method collect) are typ-
ically used in cases where its necessary to reorganize the streams items into a collec-
tion. But more generally, they can be used every time you want to combine all the
items in the stream into a single result. This result can be of any type, as complex as a
multilevel map representing a tree or as simple as a single integer, perhaps represent-
ing the sum of all the calories in the menu. Well look at both of these result types: sin-
gle integers in section 6.2.2 and multilevel grouping in section 6.3.1.
As a first simple example, lets count the number of dishes in the menu, using the
collector returned by the counting factory method:

long howmanydishes = menu. Stream(). Collect(collectors. Counting());

you can write this far more directly as

long howmanydishes = menu. Stream(). Count();

but the counting collector can be useful when used in combination with other collec-
tors, as well demonstrate later.
In the rest of this chapter, well assume that youve imported all the static factory
methods of the collectors class with

import static java. Util. Stream. Collectors. *;

so you can write counting() instead of collectors. Counting() and so on.
Lets continue exploring simple predefined collectors by looking at how you can
find the maximum and minimum values in a stream.

6.2.1 finding maximum and minimum in a stream of values
suppose you want to find the highest-calorie dish in the menu. You can use two collec-
tors, collectors. Maxby and collectors. Minby, to calculate the maximum or mini-
mum value in a stream. These two collectors take a comparator as argument to
compare the elements in the stream. Here you create a comparator comparing dishes
based on their calorie content and pass it to collectors. Maxby:
comparator<dish> dishcaloriescomparator =
comparator. Comparingint(dish: : getcalories);
optional<dish> mostcaloriedish =
menu. Stream()
. Collect(maxby(dishcaloriescomparator));
reducing and summarizing 139

you may wonder what the optional<dish> is about. To answer this we have to ask the
question, what if menu were empty? Theres no dish to return! Java 8 introduces
optional, which is a container that may or may not contain a value. Here it perfectly
represents the idea that there may or may not be a dish returned. We briefly men-
tioned it in chapter 5 when you encountered the method findany. Dont worry about
it for now; we devote chapter 11 to the study of optional<t> and its operations.
Another common reduction operation that returns a single value is to sum the val-
ues of a numeric field of the objects in a stream. Alternatively, you may want to aver-
age the values. Such operations are called summarization operations. Lets see how you
can express them using collectors.

6.2.2 summarization
the collectors class provides a specific factory method for summing: collectors
. Summingint. It accepts a function that maps an object into the int that has to be
summed and returns a collector that, when passed to the usual collect method, per-
forms the requested summarization. For instance, you can find the total number of
calories in your menu list with

int totalcalories = menu. Stream(). Collect(summingint(dish: : getcalories));

here the collection process proceeds as illustrated in figure 6.2. While traversing the
stream each dish is mapped into its number of calories, and that number is added to
an accumulator starting from an initial value (in this case the value is 0).
The collectors. Summinglong and collectors. Summingdouble methods behave
exactly the same way and can be used where the field to be summed is respectively a
long or a double.
But theres more to summarization than mere summing. A collectors. Averaging-
int, together with its averaginglong and averagingdouble counterparts, is also avail-
able to calculate the average of the same set of numeric values:

double avgcalories =
menu. Stream(). Collect(averagingint(dish: : getcalories));

so far, youve seen how to use collectors to count the elements in a stream, find the
maximum and minimum values of a numeric property of those elements, and calcu-
late their sum and average. Quite often, though, you may want to retrieve two or
more of these results, and possibly youd like to do it in a single operation. In this
case, you can use the collector returned by the summarizingint factory method.
For example, you can count the elements in the menu and obtain the sum, aver-
age, maximum, and minimum of the calories contained in each dish with a single
summarizing operation:

intsummarystatistics menustatistics =
menu. Stream(). Collect(summarizingint(dish: : getcalories));
140 chapter 6 collecting data with streams

transforming
function
stream

pork beef salmon

.
.
.
.
Dish: : getcalories dish: : getcalories dish: : getcalories

800 700 450

0 +

800 +

1500 +

4300

figure 6.2 the aggregation process of the summingint collector

this collector gathers all that information in a class called intsummarystatistics
that provides convenient getter methods to access the results. Printing the menu-
statistic object produces the following output:
intsummarystatistics{count=9, sum=4300, min=120,
average=477.777778, max=800}

as usual, there are corresponding summarizinglong and summarizingdouble fac-
tory methods with associated types longsummarystatistics and doublesummary-
statistics. These are used when the property to be collected is a primitive-type long
or a double.

6.2.3 joining strings
the collector returned by the joining factory method concatenates into a single string,
all strings resulting from invoking the tostring method on each object in the stream.
This means you can concatenate the names of all the dishes in the menu as follows:
string shortmenu = menu. Stream(). Map(dish: : getname). Collect(joining());

note that joining internally makes use of a stringbuilder to append the generated
strings into one. Also note that if the dish class had a tostring method returning the
reducing and summarizing 141

dishs name, youd obtain the same result without needing to map over the original
stream with a function extracting the name from each dish:

string shortmenu = menu. Stream(). Collect(joining());

both produce the string

porkbeefchickenfrench friesriceseason fruitpizzaprawnssalmon

which is hard to read. Fortunately, the joining factory method is overloaded, with
one of its overloaded variants taking a string used to delimit two consecutive elements,
so you can obtain a comma-separated list of the dishes names with

string shortmenu = menu. Stream(). Map(dish: : getname). Collect(joining(", "));

which, as expected, will generate

pork, beef, chicken, french fries, rice, season fruit, pizza, prawns, salmon

until now, weve explored various collectors that reduce a stream to a single value. In
the next section, well demonstrate how all the reduction processes of this form are
special cases of the more general reduction collector provided by the collectors
. Reducing factory method.

6.2.4 generalized summarization with reduction
all the collectors weve discussed so far are, in reality, only convenient specializations
of a reduction process that can be defined using the reducing factory method. The
collectors. Reducing factory method is a generalization of all of them. The special
cases discussed earlier are arguably provided only for programmer convenience. (but
remember that programmer convenience and readability are of prime importance! )
for instance, its possible to calculate the total calories in your menu with a collector
created from the reducing method as follows:
int totalcalories = menu. Stream(). Collect(reducing(
0, dish: : getcalories, (i, j) -> i + j));

it takes three arguments:
 the first argument is the starting value of the reduction operation and will also
be the value returned in the case of a stream with no elements, so clearly 0 is
the appropriate value in the case of a numeric sum.
 The second argument is the same function you used in section 6.2.2 to trans-
form a dish into an int representing its calorie content.
 The third argument is a binaryoperator that aggregates two items into a single
value of the same type. Here, it sums two ints.
142 chapter 6 collecting data with streams

similarly, you could find the highest-calorie dish using the one-argument version of
reducing as follows:
optional<dish> mostcaloriedish =
menu. Stream(). Collect(reducing(
(d1, d2) -> d1. Getcalories() > d2. Getcalories() ? D1 : d2));

you can think of the collector created with the one-argument reducing factory
method as a particular case of the three-argument method, which uses the first item in
the stream as a starting point and an identity function (a function that returns its input
argument as is) as a transformation function. This also implies that the one-argument
reducing collector wont have any starting point when passed to the collect method
of an empty stream and, as we explained in section 6.2.1, for this reason it returns an
optional<dish> object.

Collect vs. Reduce
weve discussed reductions a lot in the previous chapter and this one. You may won-
der what the differences between the collect and reduce methods of the stream
interface are, because often you can obtain the same results using either method.
For instance, you can achieve what is done by the tolist collector using the
reduce method as follows:
stream<integer> stream = arrays. Aslist(1, 2, 3, 4, 5, 6). Stream();
list<integer> numbers = stream. Reduce(
new arraylist<integer>(),
(list<integer> l, integer e) -> {
l. Add(e);
return l; },
(list<integer> l1, list<integer> l2) -> {
l1. Addall(l2);
return l1; });

this solution has two problems: a semantic one and a practical one. The semantic
problem lies in the fact that the reduce method is meant to combine two values and
produce a new one; its an immutable reduction. In contrast, the collect method is
designed to mutate a container to accumulate the result its supposed to produce.
This means that the previous snippet of code is misusing the reduce method,
because its mutating in place the list used as accumulator. As youll see in more
detail in the next chapter, using the reduce method with the wrong semantic is also
the cause of a practical problem: this reduction process cant work in parallel,
because the concurrent modification of the same data structure operated by multiple
threads can corrupt the list itself. In this case, if you want thread safety, youll need
to allocate a new list every time, which would impair performance by object alloca-
tion. This is the main reason why the collect method is useful for expressing reduc-
tion working on a mutable container but crucially in a parallel-friendly way, as youll
learn later in the chapter.
Reducing and summarizing 143

collection framework flexibility: doing the same operation in different ways
you can further simplify the previous sum example using the reducing collector by
using a reference to the sum method of the integer class instead of the lambda
expression you used to encode the same operation. This results in the following:

int totalcalories = menu. Stream(). Collect(reducing(0, initial value
dish: : getcalories,
aggregating integer: : sum)); transformation
function function

logically, this reduction operation proceeds as shown in figure 6.3, where an accumu-
latorinitialized with a starting valueis iteratively combined using an aggregating
function, with the result of the application of the transforming function on each ele-
ment of the stream.

Transforming
function
stream

pork beef salmon

.
.
.
Dish: : getcalories dish: : getcalories . Dish: : getcalories

0 integer: : sum integer: : sum integer: : sum total

initial value
aggregating
function

figure 6.3 the reduction process calculating the total number of calories in the menu

the counting collector we mentioned at the beginning of section 6.2 is, in reality, sim-
ilarly implemented using the three-argument reducing factory method. It transforms
each element in the stream into an object of type long with value 1 and then sums all
these ones. It is implemented as follows:

public static <t> collector<t, ? , long> counting() {
return reducing(0l, e -> 1l, long: : sum);
}
144 chapter 6 collecting data with streams

use of the generic ? Wildcard
in the code snippet just shown, you probably noticed the ? Wildcard, used as the sec-
ond generic type in the signature of the collector returned by the counting factory
method. You should already be familiar with this notation, especially if you use the
java collection framework quite frequently. But here it means only that the type of
the collectors accumulator is unknown, or equivalently the accumulator itself can be
of any type. We used it here to exactly report the signature of the method as originally
defined in the collectors class, but in the rest of the chapter, we avoid any wildcard
notation to keep the discussion as simple as possible.

We already observed in chapter 5 that theres another way to perform the same opera-
tion without using a collectorby mapping the stream of dishes into the number of
calories of each dish and then reducing this resulting stream with the same method
reference used in the previous version:
int totalcalories =
menu. Stream(). Map(dish: : getcalories). Reduce(integer: : sum). Get();

note that, like any one-argument reduce operation on a stream, the invocation
reduce(integer: : sum) doesnt return an int but an optional<integer> to manage
the case of a reduction operation over an empty stream in a null-safe way. Here you
extract the value inside the optional object using its get method. Note that in this
case using the get method is safe only because youre sure that the stream of dishes
isnt empty. In general, as youll learn in chapter 10, its safer to unwrap the value
eventually contained in an optional using a method that also allows you to provide a
default, such as orelse or orelseget. Finally, and even more concisely, you can
achieve the same result by mapping the stream to an intstream and then invoking
the sum method on it:

int totalcalories = menu. Stream(). Maptoint(dish: : getcalories). Sum();

choosing the best solution for your situation
once again, this demonstrates how functional programming in general (and the new
api based on functional-style principles added to the collections framework in java 8
in particular) often provides multiple ways to perform the same operation. This exam-
ple also shows that collectors are somewhat more complex to use than the methods
directly available on the streams interface, but in exchange they offer higher levels of
abstraction and generalization and are more reusable and customizable.
Our suggestion is to explore the largest number of solutions possible to the prob-
lem at hand, but always choose the most specialized one thats general enough to
solve it. This is often the best decision for both readability and performance reasons.
For instance, to calculate the total calories in our menu, wed prefer the last solution
(using intstream) because its the most concise and likely also the most readable one.
Reducing and summarizing 145

at the same time, its also the one that performs best, because intstream lets us avoid
all the auto-unboxing operations, or implicit conversions from integer to int, that are
useless in this case.
Next, test your understanding of how reducing can be used as a generalization of
other collectors by working through the exercise in quiz 6.1.

Quiz 6.1: joining strings with reducing
which of the following statements using the reducing collector are valid replace-
ments for this joining collector (as used in section 6.2.3)?
String shortmenu = menu. Stream(). Map(dish: : getname). Collect(joining());

1 string shortmenu = menu. Stream(). Map(dish: : getname)
. Collect( reducing( (s1, s2) -> s1 + s2 ) ). Get();

2 string shortmenu = menu. Stream()
. Collect( reducing( (d1, d2) -> d1. Getname() + d2. Getname() )
). Get();

3 string shortmenu = menu. Stream()
. Collect( reducing( "", dish: : getname, (s1, s2) -> s1 + s2 ) );

answer:
statements 1 and 3 are valid, whereas 2 doesnt compile.
1 this converts each dish in its name, as done by the original statement using
the joining collector, and then reduces the resulting stream of strings using
a string as accumulator and appending to it the names of the dishes one
by one.
2 this doesnt compile because the one argument that reducing accepts is a
binaryoperator<t> thats a bifunction<t, t, t>. This means that it wants
a function taking two arguments and returns a value of the same type, but
the lambda expression used there has two dishes as arguments but returns
a string.
3 this starts the reduction process with an empty string as the accumulator,
and when traversing the stream of dishes, it converts each dish to its name
and appends this name to the accumulator. Note that, as we mentioned,
reducing doesnt need the three arguments to return an optional because
in the case of an empty stream it can return a more meaningful value, which
is the empty string used as the initial accumulator value.
Note that even though statements 1 and 3 are valid replacements for the joining
collector, theyve been used here to demonstrate how the reducing one can be
seen, at least conceptually, as a generalization of all other collectors discussed in
this chapter. Nevertheless, for all practical purposes we always suggest using the
joining collector for both readability and performance reasons.
146 chapter 6 collecting data with streams

6.3 grouping
a common database operation is to group items in a set, based on one or more prop-
erties. As you saw in the earlier transactions-currency-grouping example, this opera-
tion can be cumbersome, verbose, and error-prone when implemented with an
imperative style. But it can be easily translated in a single, readable statement by
rewriting it in a more functional style as encouraged by java 8. As a second example of
how this feature works, suppose you want to classify the dishes in the menu according
to their type, putting the ones containing meat in a group, the ones with fish in
another group, and all others in a third group. You can easily perform this task using
a collector returned by the collectors. Groupingby factory method, as follows:
map<dish. Type, list<dish>> dishesbytype =
menu. Stream(). Collect(groupingby(dish: : gettype));

this will result in the following map:
{fish=[prawns, salmon], other=[french fries, rice, season fruit, pizza],
meat=[pork, beef, chicken]}

here, you pass to the groupingby method a function (expressed in the form of a
method reference) extracting the corresponding dish. Type for each dish in the
stream. We call this function a classification function specifically because its used to
classify the elements of the stream into different groups. The result of this grouping
operation, shown in figure 6.4, is a map having as map key the value returned by the clas-
sification function and as corresponding map value a list of all the items in the stream
having that classified value. In the menu-classification example, a key is the type of
dish, and its value is a list containing all the dishes of that type.

Stream
grouping map
next
item
apply key
prawns classification fish fish meat other
function

classify item into list salmon pork pizza
beef rice
chicken french fries

figure 6.4 classification of an item in the stream during the grouping process

but it isnt always possible to use a method reference as a classification function, because
you may wish to classify using something more complex than a simple property accessor.
For instance, you could decide to classify as diet all dishes with 400 calories or fewer,
grouping 147

set to normal the dishes having between 400 and 700 calories, and set to fat the
ones with more than 700 calories. Because the author of the dish class unhelpfully
didnt provide such an operation as a method, you cant use a method reference in
this case, but you can express this logic in a lambda expression:
public enum caloriclevel { diet, normal, fat }
map<caloriclevel, list<dish>> dishesbycaloriclevel = menu. Stream(). Collect(
groupingby(dish -> {
if (dish. Getcalories() <= 400) return caloriclevel. Diet;
else if (dish. Getcalories() <= 700) return caloriclevel. Normal;
else return caloriclevel. Fat;
} ));

now youve seen how to group the dishes in the menu, both by their type and by cal-
ories, but it could be also quite common that you may need to further manipulate
the results of the original grouping, and in the next section well show how you
achieve this.

6.3.1 manipulating grouped elements
frequently after performing a grouping operation you may need to manipulate the
elements in each resulting group. Suppose, for example, that you want to filter only
the caloric dishes, lets say the ones with more than 500 calories. You may argue that
in this case you could apply this filtering predicate before the grouping like the
following:
map<dish. Type, list<dish>> caloricdishesbytype =
menu. Stream(). Filter(dish -> dish. Getcalories() > 500)
. Collect(groupingby(dish: : gettype));

this solution works but has a possibly relevant drawback. If you try to use it on the
dishes in our menu, you will obtain a map like the following:

{other=[french fries, pizza], meat=[pork, beef]}

do you see the problem there? Because there is no dish of type fish satisfying our fil-
tering predicate, that key totally disappeared from the resulting map. To workaround
this problem the collectors class overloads the groupingby factory method, with one
variant also taking a second argument of type collector along with the usual classifi-
cation function. In this way, its possible to move the filtering predicate inside this sec-
ond collector, as follows:
map<dish. Type, list<dish>> caloricdishesbytype =
menu. Stream()
. Collect(groupingby(dish: : gettype,
filtering(dish -> dish. Getcalories() > 500, tolist())));

the filtering method is another static factory method of the collectors class
accepting a predicate to filter the elements in each group and a further collector
148 chapter 6 collecting data with streams

that is used to regroup the filtered elements. In this way, the resulting map will also
keep an entry for the fish type even if it maps an empty list:

{other=[french fries, pizza], meat=[pork, beef], fish=[]}

another even more common way in which it could be useful to manipulate the
grouped elements is transforming them through a mapping function. To this pur-
pose, similarly to what you have seen for the filtering collector, the collectors
class provides another collector through the mapping method that accepts a map-
ping function and another collector used to gather the elements resulting from the
application of that function to each of them. By using it you can, for instance, convert
each dish in the groups into their respective names in this way:
map<dish. Type, list<string>> dishnamesbytype =
menu. Stream()
. Collect(groupingby(dish: : gettype,
mapping(dish: : getname, tolist())));

note that in this case each group in the resulting map is a list of strings rather than
one of dishes as it was in the former examples. You could also use a third collector
in combination with the groupingby to perform a flatmap transformation instead of
a plain map. To demonstrate how this works lets suppose that we have a map associat-
ing to each dish a list of tags as it follows:
map<string, list<string>> dishtags = new hashmap<>();
dishtags. Put("pork", aslist("greasy", "salty"));
dishtags. Put("beef", aslist("salty", "roasted"));
dishtags. Put("chicken", aslist("fried", "crisp"));
dishtags. Put("french fries", aslist("greasy", "fried"));
dishtags. Put("rice", aslist("light", "natural"));
dishtags. Put("season fruit", aslist("fresh", "natural"));
dishtags. Put("pizza", aslist("tasty", "salty"));
dishtags. Put("prawns", aslist("tasty", "roasted"));
dishtags. Put("salmon", aslist("delicious", "fresh"));

in case you are required to extract these tags for each group of type of dishes you can
easily achieve this using the flatmapping collector:
map<dish. Type, set<string>> dishnamesbytype =
menu. Stream()
. Collect(groupingby(dish: : gettype,
flatmapping(dish -> dishtags. Get( dish. Getname() ). Stream(),
toset())));

here for each dish we are obtaining a list of tags. So analogously to what we have
already seen in the former chapter, we need to perform a flatmap in order to flatten
the resulting two-level list into a single one. Also note that this time we collected the
result of the flatmapping operations executed in each group into a set instead of
using a list as we did before, in order to avoid repetitions of same tags associated to
grouping 149

more than one dish in the same type. The map resulting from this operation is then
the following:

{meat=[salty, greasy, roasted, fried, crisp], fish=[roasted, tasty, fresh,
delicious], other=[salty, greasy, natural, light, tasty, fresh, fried]}

until this point we only used a single criterion to group the dishes in the menu, for
instance by their type or by calories, but what if you want to use more than one crite-
rion at the same time? Grouping is powerful because it composes effectively. Lets see
how to do this.

6.3.2 multilevel grouping
the two arguments collectors. Groupingby factory method that we used in a former
section to manipulate the elements in the groups resulting from the grouping opera-
tion can be used also to perform a two-level grouping. To achieve this you can pass to
it a second inner groupingby to the outer groupingby, defining a second-level crite-
rion to classify the streams items, as shown in the next listing.

Listing 6.2 multilevel grouping

map<dish. Type, map<caloriclevel, list<dish>>> dishesbytypecaloriclevel =
menu. Stream(). Collect(
groupingby(dish: : gettype, first-level
groupingby(dish -> { classification function
second-level
if (dish. Getcalories() <= 400) return caloriclevel. Diet;
classification
function else if (dish. Getcalories() <= 700) return caloriclevel. Normal;
else return caloriclevel. Fat;
} )
)
);

the result of this two-level grouping is a two-level map like the following:
{meat={diet=[chicken], normal=[beef], fat=[pork]},
fish={diet=[prawns], normal=[salmon]},
other={diet=[rice, seasonal fruit], normal=[french fries, pizza]}}

here the outer map has as keys the values generated by the first-level classification
function: fish, meat, other. The values of this map are in turn other maps, having as keys
the values generated by the second-level classification function: normal, diet, or fat.
Finally, the second-level maps have as values the list of the elements in the stream
returning the corresponding first- and second-level key values when applied respec-
tively to the first and second classification functions: salmon, pizza, and so on. This
multilevel grouping operation can be extended to any number of levels, and an n-level
grouping has as a result an n-level map, modeling an n-level tree structure.
Figure 6.5 shows how this structure is also equivalent to an n-dimensional table,
highlighting the classification purpose of the grouping operation.
150 chapter 6 collecting data with streams

first-level map

fish meat other second-level maps

normal diet normal diet

pizza fruit
salmon prawns
fries rice

fat normal diet type
fish meat other
calories
fruit
diet prawns chicken
rice
pork beef chicken
pizza
normal salmon beef
fries

fat pork

figure 6.5 equivalence between n-level nested map and n-dimensional classification table

in general, it helps to think that groupingby works in terms of buckets. The first
groupingby creates a bucket for each key. You then collect the elements in each
bucket with the downstream collector and so on to achieve n-level groupings!

6.3.3 collecting data in subgroups
in the previous section, you saw that its possible to pass a second groupingby collec-
tor to the outer one to achieve a multilevel grouping. But more generally, the second
collector passed to the first groupingby can be any type of collector, not just another
groupingby. For instance, its possible to count the number of dishes in the menu for
each type, by passing the counting collector as a second argument to the groupingby
collector:
map<dish. Type, long> typescount = menu. Stream(). Collect(
groupingby(dish: : gettype, counting()));

the result is the following map:

{meat=3, fish=2, other=4}

also note that the regular one-argument groupingby(f), where f is the classification
function is, in reality, shorthand for groupingby(f, tolist()).
To give another example, you could rework the collector you already used to find
the highest-calorie dish in the menu to achieve a similar result, but now classified by
the type of dish:
grouping 151

map<dish. Type, optional<dish>> mostcaloricbytype =
menu. Stream()
. Collect(groupingby(dish: : gettype,
maxby(comparingint(dish: : getcalories))));

the result of this grouping is then clearly a map, having as keys the available types of
dishes and as values the optional<dish>, wrapping the corresponding highest-calorie
dish for a given type:

{fish=optional[salmon], other=optional[pizza], meat=optional[pork]}

note the values in this map are optionals because this is the resulting type
of the collector generated by the maxby factory method, but in reality if
theres no dish in the menu for a given type, that type wont have an
optional. Empty() as value; it wont be present at all as a key in the map. The
groupingby collector lazily adds a new key in the grouping map only the first
time it finds an element in the stream, producing that key when applying on
it the grouping criteria being used. This means that in this case, the optional
wrapper isnt useful, because its not modeling a value that could be possibly
absent but is there incidentally, only because this is the type returned by the
reducing collector.
Adapting the collector result to a different type
because the optionals wrapping all the values in the map resulting from the last
grouping operation arent useful in this case, you may want to get rid of them. To
achieve this, or more generally, to adapt the result returned by a collector to a differ-
ent type, you could use the collector returned by the collectors. Collectingandthen
factory method, as shown in the following listing.

Listing 6.3 finding the highest-calorie dish in each subgroup

map<dish. Type, dish> mostcaloricbytype =
menu. Stream() classification
. Collect(groupingby(dish: : gettype, function
collectingandthen( wrapped
transformation maxby(comparingint(dish: : getcalories)), collector
function optional: : get)));

this factory method takes two argumentsthe collector to be adapted and a transfor-
mation functionand returns another collector. This additional collector acts as a
wrapper for the old one and maps the value it returns using the transformation func-
tion as the last step of the collect operation. In this case, the wrapped collector is the
one created with maxby, and the transformation function, optional: : get, extracts
the value contained in the optional returned. As weve said, here this is safe
because the reducing collector will never return an optional. Empty(). The result is
the following map:

{fish=salmon, other=pizza, meat=pork}
152 chapter 6 collecting data with streams

stream

groupingby
dish: : gettype
classification
function

the original
stream is divided substream substream substream
according to the
classification function.

Collectingandthen

each substream collectingandthen maxby collectingandthen
independently
processed by the result
second collector maxby maxby
optional[pork]
the reducing collector
returns the most
caloric dish wrapped
optional: : get
in an optional. Transformation
function

result result result result
collectingandthen
collector returns the grouping map
value extracted from
the former optional.

Fish meat other
the results of the
second-level collectors
become the values of
the grouping map. Salmon pork pizza

figure 6.6 combining the effect of multiple collectors by nesting one inside the other

its quite common to use multiple nested collectors, and at first the way they interact
may not always be obvious. Figure 6.6 helps you visualize how they work together.
From the outermost layer and moving inward, note the following:
 the collectors are represented by the dashed lines, so groupingby is the outer-
most one and groups the menu stream into three substreams according to the
different dishes types.
 The groupingby collector wraps the collectingandthen collector, so each sub-
stream resulting from the grouping operation is further reduced by this second
collector.
 The collectingandthen collector wraps in turn a third collector, the maxby one.
Grouping 153

 the reduction operation on the substreams is then performed by the reduc-
ing collector, but the collectingandthen collector containing it applies the
optional: : get transformation function to its result.
 The three transformed values, being the highest-calorie dishes for a given type
(resulting from the execution of this process on each of the three substreams),
will be the values associated with the respective classification keys, the types of
dishes, in themap returned by the groupingby collector.
Other examples of collectors used in conjunction with groupingby
more generally, the collector passed as second argument to the groupingby factory
method will be used to perform a further reduction operation on all the elements in
the stream classified into the same group. For example, you could also reuse the col-
lector created to sum the calories of all the dishes in the menu to obtain a similar
result, but this time for each group of dishes:

map<dish. Type, integer> totalcaloriesbytype =
menu. Stream(). Collect(groupingby(dish: : gettype,
summingint(dish: : getcalories)));

yet another collector, commonly used in conjunction with groupingby, is one gener-
ated by the mapping method. This method takes two arguments: a function transform-
ing the elements in a stream and a further collector accumulating the objects
resulting from this transformation. Its purpose is to adapt a collector accepting ele-
ments of a given type to one working on objects of a different type, by applying a map-
ping function to each input element before accumulating them. To see a practical
example of using this collector, suppose you want to know which caloriclevels are
available in the menu for each type of dish. You could achieve this result combining a
groupingby and a mapping collector, as follows:

map<dish. Type, set<caloriclevel>> caloriclevelsbytype =
menu. Stream(). Collect(
groupingby(dish: : gettype, mapping(dish -> {
if (dish. Getcalories() <= 400) return caloriclevel. Diet;
else if (dish. Getcalories() <= 700) return caloriclevel. Normal;
else return caloriclevel. Fat; },
toset() )));

here the transformation function passed to the mapping method maps a dish into its
caloriclevel, as youve seen before. The resulting stream of caloriclevels is then
passed to a toset collector, analogous to the tolist one, but accumulating the ele-
ments of a stream into a set instead of into a list, to keep only the distinct values. As
in earlier examples, this mapping collector will then be used to collect the elements in
each substream generated by the grouping function, allowing you to obtain as a result
the following map:

{other=[diet, normal], meat=[diet, normal, fat], fish=[diet, normal]}
154 chapter 6 collecting data with streams

from this you can easily figure out your choices. If youre in the mood for fish and
youre on a diet, you could easily find a dish; likewise, if youre hungry and want some-
thing with lots of calories, you could satisfy your robust appetite by choosing something
from the meat section of the menu. Note that in the previous example, there are no
guarantees about what type of set is returned. But by using tocollection, you can
have more control. For example, you can ask for a hashset by passing a constructor
reference to it:

map<dish. Type, set<caloriclevel>> caloriclevelsbytype =
menu. Stream(). Collect(
groupingby(dish: : gettype, mapping(dish -> {
if (dish. Getcalories() <= 400) return caloriclevel. Diet;
else if (dish. Getcalories() <= 700) return caloriclevel. Normal;
else return caloriclevel. Fat; },
tocollection(hashset: : new) )));

6.4 partitioning
partitioning is a special case of grouping: having a predicate called a partitioning func-
tion as a classification function. The fact that the partitioning function returns a bool-
ean means the resulting grouping map will have a boolean as a key type, and therefore,
there can be at most two different groupsone for true and one for false. For
instance, if youre vegetarian or have invited a vegetarian friend to have dinner with
you, you may be interested in partitioning the menu into vegetarian and nonvegetar-
ian dishes:
partitioning function
map<boolean, list<dish>> partitionedmenu =
menu. Stream(). Collect(partitioningby(dish: : isvegetarian));

this will return the following map:
{false=[pork, beef, chicken, prawns, salmon],
true=[french fries, rice, season fruit, pizza]}

so you could retrieve all the vegetarian dishes by getting from this map the value
indexed with the key true:

list<dish> vegetariandishes = partitionedmenu. Get(true);

note that you could achieve the same result by filtering the stream created from the
menu list with the same predicate used for partitioning and then collecting the
result in an additional list:

list<dish> vegetariandishes =
menu. Stream(). Filter(dish: : isvegetarian). Collect(tolist());
partitioning 155

6.4.1 advantages of partitioning
partitioning has the advantage of keeping both lists of the stream elements, for which
the application of the partitioning function returns true or false. In the previous
example, you can obtain the list of the nonvegetarian dishes by accessing the value
of the key false in the partitionedmenu map, using two separate filtering operations:
one with the predicate and one with its negation. Also, as you already saw for group-
ing, the partitioningby factory method has an overloaded version to which you can
pass a second collector, as shown here:
map<boolean, map<dish. Type, list<dish>>> vegetariandishesbytype =
menu. Stream(). Collect(
partitioningby(dish: : isvegetarian, partitioning function
groupingby(dish: : gettype)));
second collector
this will produce a two-level map:
{false={fish=[prawns, salmon], meat=[pork, beef, chicken]},
true={other=[french fries, rice, season fruit, pizza]}}

here the grouping of the dishes by their type is applied individually to both of the
substreams of vegetarian and nonvegetarian dishes resulting from the partitioning,
producing a two-level map thats similar to the one you obtained when you performed
the two-level grouping in section 6.3.1. As another example, you can reuse your ear-
lier code to find the most caloric dish among both vegetarian and nonvegetarian
dishes:
map<boolean, dish> mostcaloricpartitionedbyvegetarian =
menu. Stream(). Collect(
partitioningby(dish: : isvegetarian,
collectingandthen(maxby(comparingint(dish: : getcalories)),
optional: : get)));

that will produce the following result:

{false=pork, true=pizza}

we started this section by saying that you can think of partitioning as a special case
of grouping. Its worth also noting that the map implementation returned by
partitioningby is more compact and efficient as it only needs to contain two keys:
true and false. In fact, the internal implementation is a specialized map with two fields.
The analogies between the groupingby and partitioningby collectors dont end
here; as youll see in the next quiz, you can also perform multilevel partitioning in a
way similar to what you did for grouping in section 6.3.1.
To give one last example of how you can use the partitioningby collector, well
put aside the menu data model and look at something a bit more complex but also
more interesting: partitioning numbers into prime and nonprime.
156 chapter 6 collecting data with streams

quiz 6.2: using partitioningby
as youve seen, like the groupingby collector, the partitioningby collector can be
used in combination with other collectors. In particular it could be used with a second
partitioningby collector to achieve a multilevel partitioning. What will be the result
of the following multilevel partitionings?
1 menu. Stream(). Collect(partitioningby(dish: : isvegetarian,
partitioningby(d -> d. Getcalories() > 500)));

2 menu. Stream(). Collect(partitioningby(dish: : isvegetarian,
partitioningby(dish: : gettype)));

3 menu. Stream(). Collect(partitioningby(dish: : isvegetarian,
counting()));

answer:
1 this is a valid multilevel partitioning, producing the following two-level map:
{ false={false=[chicken, prawns, salmon], true=[pork, beef]},
true={false=[rice, season fruit], true=[french fries, pizza]}}

2 this wont compile because partitioningby requires a predicate, a function
returning a boolean. And the method reference dish: : gettype cant be used
as a predicate.
3 this counts the number of items in each partition, resulting in the following map:
{false=5, true=4}

6.4.2 partitioning numbers into prime and nonprime
suppose you want to write a method accepting as argument an int n and partitioning
the first n natural numbers into prime and nonprime. But first, it will be useful to
develop a predicate that tests to see if a given candidate number is prime or not:
generates a range of natural numbers
public boolean isprime(int candidate) { starting from and including 2, up to
return intstream. Range(2, candidate) but excluding candidate
. Nonematch(i -> candidate % i == 0);
}
returns true if the candidate isnt divisible
for any of the numbers in the stream

a simple optimization is to test only for factors less than or equal to the square root of
the candidate:
public boolean isprime(int candidate) {
int candidateroot = (int) math. Sqrt((double) candidate);
return intstream. Rangeclosed(2, candidateroot)
. Nonematch(i -> candidate % i == 0);
}
partitioning 157

now the biggest part of the job is done. To partition the first n numbers into prime
and nonprime, its enough to create a stream containing those n numbers, and
reduce it with a partitioningby collector using as predicate the isprime method you
just developed:
public map<boolean, list<integer>> partitionprimes(int n) {
return intstream. Rangeclosed(2, n). Boxed()
. Collect(
partitioningby(candidate -> isprime(candidate)));
}

weve now covered all the collectors that can be created using the static factory meth-
ods of the collectors class, showing practical examples of how they work. Table 6.1
brings them all together with the type they return when applied to a stream<t>, and a
practical example of their use on a stream<dish> named menustream.
Table 6.1 the main static factory methods of the collectors class

factory method returned type used to

tolist list<t> gather all the streams items in a list.

Example use: list<dish> dishes = menustream. Collect(tolist());

toset set<t> gather all the streams items in a set, elimi-
nating duplicates.

Example use: set<dish> dishes = menustream. Collect(toset());

tocollection collection<t> gather all the streams items in the collection
created by the provided supplier.

Example use: collection<dish> dishes =
menustream. Collect(tocollection(), arraylist: : new);

counting long count the number of items in the stream.

Example use: longhowmanydishes = menustream. Collect(counting());

summingint integer sum the values of an integer property of the
items in the stream.

Example use: int totalcalories =
menustream. Collect(summingint(dish: : getcalories));

averagingint double calculate the average value of an integer
property of the items in the stream.

Example use: double avgcalories =
menustream. Collect(averagingint(dish: : getcalories));

summarizingint intsummarystatistics collect statistics regarding an integer prop-
erty of the items in the stream, such as the
maximum, minimum, total, and average.

Example use: intsummarystatistics menustatistics =
menustream. Collect(summarizingint(dish: : getcalories));
158 chapter 6 collecting data with streams

table 6.1 the main static factory methods of the collectors class (continued)

factory method returned type used to

joining string concatenate the strings resulting from the
invocation of the tostring method on each
item of the stream.

Example use: string shortmenu =
menustream. Map(dish: : getname). Collect(joining(", "));

maxby optional<t> an optional wrapping the maximal element
in this stream according to the given compar-
ator or optional. Empty() if the stream is
empty.

Example use: optional<dish> fattest =
menustream. Collect(maxby(comparingint(dish: : getcalories)));

minby optional<t> an optional wrapping the minimal element
in this stream according to the given compar-
ator or optional. Empty() if the stream is
empty.

Example use: optional<dish> lightest =
menustream. Collect(minby(comparingint(dish: : getcalories)));

reducing the type produced by the reduce the stream to a single value starting
reduction operation from an initial value used as accumulator and
iteratively combining it with each item of the
stream using a binaryoperator.

Example use: int totalcalories =
menustream. Collect(reducing(0, dish: : getcalories, integer: : sum));

collectingandthen the type returned by the wrap another collector and apply a transfor-
transforming function mation function to its result.

Example use: int howmanydishes =
menustream. Collect(collectingandthen(tolist(), list: : size));

groupingby map<k, list<t>> group the items in the stream based on the
value of one of their properties and use those
values as keys in the resulting map.

Example use: map<dish. Type, list<dish>> dishesbytype =
menustream. Collect(groupingby(dish: : gettype));

partitioningby map<boolean, partition the items in the stream based on the
list<t>> result of the application of a predicate to each
of them.

Example use: map<boolean, list<dish>> vegetariandishes =
menustream. Collect(partitioningby(dish: : isvegetarian));

as we mentioned at the beginning of the chapter, all these collectors implement the
collector interface, so in the remaining part of the chapter we investigate this interface
the collector interface 159

in more detail. We investigate the methods in that interface and then explore how you
can implement your own collectors.

6.5 the collector interface
the collector interface consists of a set of methods that provide a blueprint for how
to implement specific reduction operations (collectors). Youve seen many collectors
that implement the collector interface, such as tolist or groupingby. This also
implies that youre free to create customized reduction operations by providing your
own implementation of the collector interface. In section 6.6 well show how you
can implement the collector interface to create a collector to partition a stream of
numbers into prime and nonprime more efficiently than what youve seen so far.
To get started with the collector interface, we focus on one of the first collectors
you encountered at the beginning of this chapter: the tolist factory method, which
gathers all the elements of a stream in a list. We said that youll frequently use this
collector in your day-to-day job, but its also one that, at least conceptually, is straight-
forward to develop. Investigating in more detail how this collector is implemented is a
good way to understand how the collector interface is defined and how the func-
tions returned by its methods are internally used by the collect method.
Lets start by taking a look at the definition of the collector interface in the next
listing, which shows the interface signature together with the five methods it declares.

Listing 6.4 the collector interface

public interface collector<t, a, r> {
supplier<a> supplier();
biconsumer<a, t> accumulator();
function<a, r> finisher();
binaryoperator<a> combiner();
set<characteristics> characteristics();
}

in this listing, the following definitions apply:
 t is the generic type of the items in the stream to be collected.
 A is the type of the accumulator, the object on which the partial result will be
accumulated during the collection process.
 R is the type of the object (typically, but not always, the collection) resulting
from the collect operation.
For instance, you could implement a tolistcollector<t> class that gathers all the
elements of a stream<t> into a list<t> having the following signature

public class tolistcollector<t> implements collector<t, list<t>, list<t>>

where, as well clarify shortly, the object used for the accumulation process will also be
the final result of the collection process.
160 chapter 6 collecting data with streams

6.5.1 making sense of the methods declared by collector interface
we can now analyze the five methods declared by the collector interface one by one.
When we do so, youll notice that each of the first four methods returns a function
that will be invoked by the collect method, whereas the fifth one, characteristics,
provides a set of characteristics thats a list of hints used by the collect method itself
to know which optimizations (for example, parallelization) its allowed to employ
while performing the reduction operation.
Making a new result container: the supplier method
the supplier method has to return a supplier of an empty accumulatora parame-
terless function that when invoked creates an instance of an empty accumulator used
during the collection process. Clearly, for a collector returning the accumulator itself
as result, like our tolistcollector, this empty accumulator will also represent the
result of the collection process when performed on an empty stream. In our tolist-
collector the supplier will then return an empty list, as follows:
public supplier<list<t>> supplier() {
return () -> new arraylist<t>();
}

note that you could also pass a constructor reference:
public supplier<list<t>> supplier() {
return arraylist: : new;
}

adding an element to a result container: the accumulator method
the accumulator method returns the function that performs the reduction opera-
tion. When traversing the nth element in the stream, this function is applied with two
arguments, the accumulator being the result of the reduction (after having collected
the first n1 items of the stream) and the nth element itself. The function returns
void because the accumulator is modified in place, meaning that its internal state is
changed by the function application to reflect the effect of the traversed element. For
tolistcollector, this function merely has to add the current item to the list contain-
ing the already traversed ones:
public biconsumer<list<t>, t> accumulator() {
return (list, item) -> list. Add(item);

you could instead use a method reference, which is more concise:
public biconsumer<list<t>, t> accumulator() {
return list: : add;
}

applying the final transformation to the result container: the finisher method
the finisher method has to return a function thats invoked at the end of the accu-
mulation process, after having completely traversed the stream, in order to transform
the collector interface 161

the accumulator object into the final result of the whole collection operation. Often,
as in the case of the tolistcollector, the accumulator object already coincides with
the final expected result. As a consequence, theres no need to perform a transforma-
tion, so the finisher method has to return the identity function:
public function<list<t>, list<t>> finisher() {
return function. Identity();
}

these first three methods are enough to execute a sequential reduction of the stream
that, at least from a logical point of view, could proceed as in figure 6.7. The implementa-
tion details are a bit more difficult in practice due to both the lazy nature of the stream,
which could require a pipeline of other intermediate operations to execute before the
collect operation, and the possibility, in theory, of performing the reduction in parallel.
Merging two result containers: the combiner method
the combiner method, the last of the four methods that return a function used by the
reduction operation, defines how the accumulators resulting from the reduction of
different subparts of the stream are combined when the subparts are processed in par-
allel. In the tolist case, the implementation of this method is simple; add the list

start

a accumulator = collector. Supplier(). Get();

collector. Accumulator(). Accept(accumulator, next)

are there yes
more items in the t next = fetch next stream's item
stream?

No

r result = collector. Finisher(). Apply(accumulator);

return result;

end

figure 6.7 logical steps of the sequential reduction process
162 chapter 6 collecting data with streams

containing the items gathered from the second subpart of the stream to the end of
the list obtained when traversing the first subpart:
public binaryoperator<list<t>> combiner() {
return (list1, list2) -> {
list1. Addall(list2);
return list1; }
}

split the stream
in 2 subparts

split the stream split the stream
in 2 subparts in 2 subparts

keep dividing the stream
until each subpart is
. . . .
. . Small enough . .
. . . .
. . . .
.
.
.
.

Process each substream in
parallel using the former
sequential algorithm

r r1 = collector. Combiner(). Apply(acc1, acc2); r r2 = collector. Combiner(). Apply(acc3, acc4);

a accumulator = collector. Combiner(). Apply(r1, r2);
combine the results of the
independent processing
of each substream

r result = collector. Finisher(). Apply(accumulator);

return result;

figure 6.8 parallelizing the reduction process using the combiner method

the addition of this fourth method allows a parallel reduction of the stream. This uses
the fork/join framework introduced in java 7 and the spliterator abstraction that
youll learn about in the next chapter. It follows a process similar to the one shown in
figure 6.8 and described in detail here.
The collector interface 163

 the original stream is recursively split in substreams until a condition defining
whether a stream needs to be further divided becomes false (parallel comput-
ing is often slower than sequential computing when the units of work being dis-
tributed are too small, and its pointless to generate many more parallel tasks
than you have processing cores).
 At this point, all substreams can be processed in parallel, each of them using the
sequential reduction algorithm shown in figure 6.7.
 Finally, all the partial results are combined pairwise using the function returned
by the combiner method of the collector. This is done by combining results cor-
responding to substreams associated with each split of the original stream.
The characteristics method
the last method, characteristics, returns an immutable set of characteristics,
defining the behavior of the collectorin particular providing hints about whether
the stream can be reduced in parallel and which optimizations are valid when doing
so. Characteristics is an enumeration containing three items:
 unorderedthe result of the reduction isnt affected by the order in which the
items in the stream are traversed and accumulated.
 Concurrentthe accumulator function can be called concurrently from mul-
tiple threads, and then this collector can perform a parallel reduction of the
stream. If the collector isnt also flagged as unordered, it can perform a parallel
reduction only when its applied to an unordered data source.
 Identity_finishthis indicates the function returned by the finisher method
is the identity one, and its application can be omitted. In this case, the accumu-
lator object is directly used as the final result of the reduction process. This also
implies that its safe to do an unchecked cast from the accumulator a to the
result r.
The tolistcollector developed so far is identity_finish, because the list used to
accumulate the elements in the stream is already the expected final result and doesnt
need any further transformation, but it isnt unordered because if you apply it to an
ordered stream you want this ordering to be preserved in the resulting list. Finally,
its concurrent, but following what we just said, the stream will be processed in paral-
lel only if its underlying data source is unordered.

6.5.2 putting them all together
the five methods analyzed in the preceding subsection are everything you need to
develop your own tolistcollector so you can implement it by putting all of them
together, as the next listing shows.

Listing 6.5 the tolistcollector

import java. Util. *;
import java. Util. Function. *;
164 chapter 6 collecting data with streams

import java. Util. Stream. Collector;
import static java. Util. Stream. Collector. Characteristics. *;
public class tolistcollector<t> implements collector<t, list<t>, list<t>> {
@override
public supplier<list<t>> supplier() { creates the collection
return arraylist: : new;
operation starting point
}
@override
public biconsumer<list<t>, t> accumulator() { accumulates the traversed
return list: : add; item, modifying the
} accumulator in place
@override
public function<list<t>, list<t>> finisher() {
return function. Identity();
identifies function
}
@override
public binaryoperator<list<t>> combiner() { modifies the first accumulator,
return (list1, list2) -> { combining it with the content
list1. Addall(list2);
of the second one
return list1;
}; returns the modified
}
first accumulator
@override
public set<characteristics> characteristics() {
return collections. Unmodifiableset(enumset. Of( flags the collector as
identity_finish, concurrent)); identity_finish and
} concurrent
}

note that this implementation isnt identical to the one returned by the collectors
. Tolist method, but it differs only in some minor optimizations. These optimizations
are mostly related to the fact that the collector provided by the java api uses the
collections. Emptylist() singleton when it has to return an empty list. This means
that it could be safely used in place of the original java as an example to gather a list of
all the dishes of a menu stream:

list<dish> dishes = menustream. Collect(new tolistcollector<dish>());

the remaining difference from this and the standard formulation

list<dish> dishes = menustream. Collect(tolist());

is that tolist is a factory, whereas you have to use new to instantiate your tolist-
collector.
Performing a custom collect without creating a collector implementation
in the case of an identity_finish collection operation, theres a further possibility of
obtaining the same result without developing a completely new implementation of
the collector interface. Streams has an overloaded collect method accepting the
three other functionssupplier, accumulator, and combinerhaving exactly the same
developing your own collector for better performance 165

semantics as the ones returned by the corresponding methods of the collector inter-
face. For instance, its possible to collect in a list all the items in a stream of dishes,
as follows:
list<dish> dishes = menustream. Collect( supplier
arraylist: : new,
list: : add, accumulator
list: : addall);
combiner

we believe that this second form, even if more compact and concise than the former
one, is rather less readable. Also, developing an implementation of your custom col-
lector in a proper class promotes its reuse and helps avoid code duplication. Its also
worth noting that youre not allowed to pass any characteristics to this second
collect method, so it always behaves as an identity_finish and concurrent but not
unordered collector.
In the next section, youll take your new knowledge of implementing collectors to
the next level. Youll develop your own custom collector for a more complex but
hopefully more specific and compelling use case.

6.6 developing your own collector for better performance
in section 6.4, where we discussed partitioning, you created a collector using one of
the many convenient factory methods provided by the collectors class, which
divides the first n natural numbers into primes and nonprimes, as shown in the fol-
lowing listing.

Listing 6.6 partitioning the first n natural numbers into primes and nonprimes

public map<boolean, list<integer>> partitionprimes(int n) {
return intstream. Rangeclosed(2, n). Boxed()
. Collect(partitioningby(candidate -> isprime(candidate));
}

there you achieved an improvement over the original isprime method by limiting
the number of divisors to be tested against the candidate prime to those not bigger
than the candidates square root:

public boolean isprime(int candidate) {
int candidateroot = (int) math. Sqrt((double) candidate);
return intstream. Rangeclosed(2, candidateroot)
. Nonematch(i -> candidate % i == 0);
}

is there a way to obtain even better performances? The answer is yes, but for this youll
have to develop a custom collector.
166 chapter 6 collecting data with streams

6.6.1 divide only by prime numbers
one possible optimization is to test only if the candidate number is divisible by prime
numbers. Its pointless to test it against a divisor thats not itself prime! You can limit
the test to only the prime numbers found before the current candidate. The problem
with the predefined collectors youve used so far, and the reason you have to develop a
custom one, is that during the collecting process you dont have access to the partial
result. This means that when testing whether a given candidate number is prime or
not, you dont have access to the list of the other prime numbers found so far.
Suppose you had this list; you could pass it to the isprime method and rewrite it
as follows:
public static boolean isprime(list<integer> primes, int candidate) {
return primes. Stream(). Nonematch(i -> candidate % i == 0);
}

also, you should implement the same optimization you used before and test only with
primes smaller than the square root of the candidate number. You need a way to stop
testing whether the candidate is divisible by a prime as soon as the next prime is
greater than the candidates root. You can easily do this by using the streams take-
while method:
public static boolean isprime(list<integer> primes, int candidate){
int candidateroot = (int) math. Sqrt((double) candidate);
return primes. Stream()
. Takewhile(i -> i <= candidateroot)
. Nonematch(i -> candidate % i == 0);
}

quiz 6.3: simulating takewhile in java 8
the takewhile method was introduced in java 9, so unfortunately you cannot use
this solution if you are still using java 8. How could you work around this limitation
and achieve something similar in java 8?
Answer:
you could implement your own takewhile method, which, given a sorted list and a
predicate, returns the longest prefix of this list whose elements satisfy the predicate:

public static <a> list<a> takewhile(list<a> list, predicate<a> p) {
int i = 0;
for (a item : list) { checks if the current item in
if (! P. Test(item)) {
the list satisfies the predicate
return list. Sublist(0, i);
} if it doesnt, returns the
i++;
sublist prefix until the
item before the tested one
} all the items in the list
return list; satisfy the predicate, so
} returns the list itself
developing your own collector for better performance 167

using this method, you can rewrite the isprime method and once again testing only
the candidate prime against only the primes that arent greater than its square root:
public static boolean isprime(list<integer> primes, int candidate){
int candidateroot = (int) math. Sqrt((double) candidate);
return takewhile(primes, i -> i <= candidateroot)
. Stream()
. Nonematch(p -> candidate % p == 0);
}

note that, unlike the one provided by the streams api, this implementation of take-
while is eager. When possible, always prefer the java 9 stream's lazy version of
takewhile so it can be merged with the nonematch operation.

With this new isprime method in hand, youre now ready to implement your own cus-
tom collector. First, you need to declare a new class that implements the collector
interface. Then, you need to develop the five methods required by the collector
interface.
Step 1: defining the collector class signature
lets start with the class signature, remembering that the collector interface is
defined as

public interface collector<t, a, r>

where t, a, and r are respectively the type of the elements in the stream, the type of
the object used to accumulate partial results, and the type of the final result of the
collect operation. In this case, you want to collect streams of integers while both
the accumulator and the result types are map<boolean, list<integer>> (the same
map you obtained as a result of the former partitioning operation in listing 6.6), hav-
ing as keys true and false and as values respectively the lists of prime and non-
prime numbers:
the type of the elements
public class primenumberscollector in the stream
implements collector<integer,
map<boolean, list<integer>>,
the type of the result of map<boolean, list<integer>>> the type of the
the collect operation accumulator

step 2: implementing the reduction process
next, you need to implement the five methods declared in the collector inter-
face. The supplier method has to return a function that when invoked creates
the accumulator:
public supplier<map<boolean, list<integer>>> supplier() {
return () -> new hashmap<boolean, list<integer>>() {{
put(true, new arraylist<integer>());
168 chapter 6 collecting data with streams

put(false, new arraylist<integer>());
}};
}

here youre not only creating the map that youll use as the accumulator, but youre
also initializing it with two empty lists under the true and false keys. This is where
youll add respectively the prime and nonprime numbers during the collection pro-
cess. The most important method of your collector is the accumulator method,
because it contains the logic defining how the elements of the stream have to be col-
lected. In this case, its also the key to implementing the optimization we described
previously. At any given iteration you can now access the partial result of the collection
process, which is the accumulator containing the prime numbers found so far:

public biconsumer<map<boolean, list<integer>>, integer> accumulator() {
return (map<boolean, list<integer>> acc, integer candidate) -> {
acc. Get( isprime(acc. Get(true), candidate) )
. Add(candidate); gets the list of prime
}; or nonprime numbers
adds the candidate to depending on the
}
the appropriate list result of isprime

in this method, you invoke the isprime method, passing to it (together with the num-
ber for which you want to test whether its prime or not) the list of the prime numbers
found so far. (these are the values indexed by the true key in the accumulating map. )
the result of this invocation is then used as the key to get the list of either the prime
or nonprime numbers, so you can add the new candidate to the right list.
Step 3: making the collector work in parallel (if possible)
the next method has to combine two partial accumulators in the case of a parallel col-
lection process, so in this case it has to merge the two maps by adding all the numbers
in the prime and nonprime lists of the second map to the corresponding lists in the
first map:

public binaryoperator<map<boolean, list<integer>>> combiner() {
return (map<boolean, list<integer>> map1,
map<boolean, list<integer>> map2) -> {
map1. Get(true). Addall(map2. Get(true));
map1. Get(false). Addall(map2. Get(false));
return map1;
};
}

note that in reality this collector cant be used in parallel, because the algorithm is
inherently sequential. This means the combiner method wont ever be invoked, and
you could leave its implementation empty (or better, throw an unsupportedoperation-
exception). We decided to implement it anyway only for completeness.
Developing your own collector for better performance 169

step 4: the finisher method and the collectors characteristic method
the implementation of the last two methods is quite straightforward. As we said, the
accumulator coincides with the collectors result so it wont need any further transfor-
mation, and the finisher method returns the identity function:
public function<map<boolean, list<integer>>,
map<boolean, list<integer>>> finisher() {
return function. Identity();
}

as for the characteristic method, we already said that its neither concurrent nor
unordered but is identity_finish:
public set<characteristics> characteristics() {
return collections. Unmodifiableset(enumset. Of(identity_finish));
}

the following listing shows the final implementation of primenumberscollector.

Listing 6.7 the primenumberscollector

public class primenumberscollector
implements collector<integer,
map<boolean, list<integer>>, starts the
map<boolean, list<integer>>> { collection
@override process with a
public supplier<map<boolean, list<integer>>> supplier() { map containing
return () -> new hashmap<boolean, list<integer>>() {{ two empty lists
put(true, new arraylist<integer>());
put(false, new arraylist<integer>());
}};
}
@override
public biconsumer<map<boolean, list<integer>>, integer> accumulator() {
return (map<boolean, list<integer>> acc, integer candidate) -> {
acc. Get( isprime( acc. Get(true),
passes to
the isprime candidate) ) gets from the map the list of prime or
method the . Add(candidate); nonprime numbers, according to what
list of already }; the isprime method returned, and adds
found primes } to it the current candidate
@override
public binaryoperator<map<boolean, list<integer>>> combiner() {
return (map<boolean, list<integer>> map1,
map<boolean, list<integer>> map2) -> {
merges the
second map into map1. Get(true). Addall(map2. Get(true));
the first one map1. Get(false). Addall(map2. Get(false));
return map1;
}; no transformation necessary
}
at the end of the collection
process, so terminate it with
@override
the identity function
public function<map<boolean, list<integer>>,
map<boolean, list<integer>>> finisher() {
return function. Identity();
170 chapter 6 collecting data with streams

}
@override
public set<characteristics> characteristics() {
return collections. Unmodifiableset(enumset. Of(identity_finish));
}
} this collector is identity_finish but neither unordered
nor concurrent because it relies on the fact that prime
numbers are discovered in sequence.

You can now use this new custom collector in place of the former one created with the
partitioningby factory method in section 6.4 and obtain exactly the same result:

public map<boolean, list<integer>>
partitionprimeswithcustomcollector(int n) {
return intstream. Rangeclosed(2, n). Boxed()
. Collect(new primenumberscollector());
}

6.6.2 comparing collectors performances
the collector created with the partitioningby factory method and the custom one
you just developed are functionally identical, but did you achieve your goal of improv-
ing the performance of the partitioningby collector with your custom one? Lets
write a quick harness to check this:

public class collectorharness {
public static void main(string[] args) { partitions the
long fastest = long. Max_value; runs the test first million
for (int i = 0; i < 10; i++) { 10 times natural numbers
long start = system. Nanotime(); into primes and
partitionprimes(1_000_000); nonprimes
long duration = (system. Nanotime() - start) / 1_000_000;
the duration in
milliseconds if (duration < fastest) fastest = duration;
} checks if this
system. Out. Println(
execution is
the fastest one
"fastest execution done in " + fastest + " msecs");
}
}

note that a more scientific benchmarking approach would be to use a framework
such as java microbenchmark harness (jmh), but we didnt want to add the com-
plexity of using such a framework here and, for this use case, the results provided by
this small benchmarking class are accurate enough. This class partitions the first
million natural numbers into primes and nonprimes, invoking the method using
the collector created with the partitioningby factory method 10 times and regis-
tering the fastest execution. Running it on an intel i5 2.4 ghz, it prints the follow-
ing result:

fastest execution done in 4716 msecs
summary 171

now replace partitionprimes with partitionprimeswithcustomcollector in the
harness, in order to test the performances of the custom collector you developed.
Now the program prints
fastest execution done in 3201 msecs

not bad! This means you didnt waste your time developing this custom collector for
two reasons: first, you learned how to implement your own collector when you need
it. And second, you achieved a performance improvement of around 32%.
Finally, its important to note that, as you did for the tolistcollector in listing 6.5,
its possible to obtain the same result by passing the three functions implementing the
core logic of primenumberscollector to the overloaded version of the collect
method, taking them as arguments:
public map<boolean, list<integer>> partitionprimeswithcustomcollector
(int n) {
intstream. Rangeclosed(2, n). Boxed() supplier
. Collect(
() -> new hashmap<boolean, list<integer>>() {{
put(true, new arraylist<integer>());
put(false, new arraylist<integer>());
}},
(acc, candidate) -> { accumulator
acc. Get( isprime(acc. Get(true), candidate) )
. Add(candidate);
},
(map1, map2) -> { combiner
map1. Get(true). Addall(map2. Get(true));
map1. Get(false). Addall(map2. Get(false));
});
}

as you can see, in this way you can avoid creating a completely new class that imple-
ments the collector interface; the resulting code is more compact, even if its also
probably less readable and certainly less reusable.

Summary
 collect is a terminal operation that takes as argument various recipes (called
collectors) for accumulating the elements of a stream into a summary result.
 Predefined collectors include reducing and summarizing stream elements into
a single value, such as calculating the minimum, maximum, or average. Those
collectors are summarized in table 6.1.
 Predefined collectors let you group elements of a stream with groupingby and
partition elements of a stream with partitioningby.
 Collectors compose effectively to create multilevel groupings, partitions, and
reductions.
 You can develop your own collectors by implementing the methods defined in
the collector interface.
Parallel data processing
and performance

this chapter covers
 processing data in parallel with parallel streams
 performance analysis of parallel streams
 the fork/join framework
 splitting a stream of data using a spliterator

in the last three chapters, youve seen how the new streams interface lets you
manipulate collections of data in a declarative way. We also explained that the shift
from external to internal iteration enables the native java library to gain control
over processing the elements of a stream. This approach relieves java developers
from explicitly implementing optimizations necessary to speed up the processing
of collections of data. By far the most important benefit is the possibility of execut-
ing a pipeline of operations on these collections that automatically makes use of
the multiple cores on your computer.
For instance, before java 7, processing a collection of data in parallel was
extremely cumbersome. First, you needed to explicitly split the data structure con-
taining your data into subparts. Second, you needed to assign each of these sub-
parts to a different thread. Third, you needed to synchronize them opportunely to
avoid unwanted race conditions, wait for the completion of all threads, and finally

172
parallel streams 173

combine the partial results. Java 7 introduced a framework called fork/join to perform
these operations more consistently and in a less error-prone way. Well explore this
framework in section 7.2.
In this chapter, youll discover how the streams interface gives you the opportunity
to execute operations in parallel on a collection of data without much effort. It lets you
declaratively turn a sequential stream into a parallel one. Moreover, youll see how java
can make this magic happen or, more practically, how parallel streams work under the
hood by employing the fork/join framework introduced in java 7. Youll also discover
that its important to know how parallel streams work internally, because if you ignore
this aspect, you could obtain unexpected (and likely wrong) results by misusing them.
In particular, well demonstrate that the way a parallel stream gets divided into
chunks, before processing the different chunks in parallel, can in some cases be the
origin of these incorrect and apparently unexplainable results. For this reason, youll
learn how to take control of this splitting process by implementing and using your
own spliterator.

7.1 parallel streams
in chapter 4, we briefly mentioned that the streams interface allows you to process its
elements in parallel in a convenient way: its possible to turn a collection into a paral-
lel stream by invoking the method parallelstream on the collection source. A parallel
stream is a stream that splits its elements into multiple chunks, processing each chunk
with a different thread. Thus, you can automatically partition the workload of a given
operation on all the cores of your multicore processor and keep all of them equally
busy. Lets experiment with this idea by using a simple example.
Lets suppose you need to write a method accepting a number n as argument and
returning the sum of the numbers from one to n. A straightforward (perhaps naive)
approach is to generate an infinite stream of numbers, limiting it to the passed num-
bers, and then reduce the resulting stream with a binaryoperator that sums two num-
bers, as follows:
generates the infinite
public long sequentialsum(long n) { stream of natural numbers
return stream. Iterate(1l, i -> i + 1)
limits it to the
. Limit(n)
first n numbers
. Reduce(0l, long: : sum);
} reduces the stream by
summing all the numbers

in more traditional java terms, this code is equivalent to its iterative counterpart:
public long iterativesum(long n) {
long result = 0;
for (long i = 1l; i <= n; i++) {
result += i;
}
return result;
}
174 chapter 7 parallel data processing and performance

this operation seems to be a good candidate to use parallelization, especially for large
values of n. But where do you start? Do you synchronize on the result variable? How
many threads do you use? Who does the generation of numbers? Who adds them up?
Dont worry about all of this. Its a much simpler problem to solve if you adopt par-
allel streams!

7.1.1 turning a sequential stream into a parallel one
you can make the former functional reduction process (summing) run in parallel by turn-
ing the stream into a parallel one; call the method parallel on the sequential stream:
public long parallelsum(long n) {
return stream. Iterate(1l, i -> i + 1)
. Limit(n) turns the stream
. Parallel() into a parallel one
. Reduce(0l, long: : sum);
}

in the previous code, the reduction process used to sum all the numbers in the stream
works in a way thats similar to whats described in section 5.4.1. The difference is that
the stream is now internally divided into multiple chunks. As a result, the reduction
operation can work on the various chunks independently and in parallel, as shown in
figure 7.1. Finally, the same reduction operation combines the values resulting from

first chunk second chunk

stream 1 2 3 4 5 6 7 8

0 + 0 +

1 + 5 +

3 + 11 +

6 + 18 +

10 + 26

36

figure 7.1 a parallel reduction operation
parallel streams 175

the partial reductions of each substream, producing the result of the reduction pro-
cess on the whole initial stream.
Note that, in reality, calling the method parallel on a sequential stream doesnt
imply any concrete transformation on the stream itself. Internally, a boolean flag is set
to signal that you want to run in parallel all the operations that follow the invocation
to parallel. Similarly, you can turn a parallel stream into a sequential one by invok-
ing the method sequential on it. Note that you might think that by combining these
two methods you could achieve finer-grained control over which operations you want
to perform in parallel and which ones sequentially while traversing the stream. For
example, you could do something like the following:
stream. Parallel()
. Filter(... )
. Sequential()
. Map(... )
. Parallel()
. Reduce();

but the last call to parallel or sequential wins and affects the pipeline globally. In
this example, the pipeline will be executed in parallel because thats the last call in the
pipeline.

Configuring the thread pool used by parallel streams
looking at the streams parallel method, you may wonder where the threads used
by the parallel stream come from, how many there are, and how you can customize
the process.
Parallel streams internally use the default forkjoinpool (youll learn more about
the fork/join framework in section 7.2), which by default has as many threads
as you have processors, as returned by runtime. Getruntime(). Available-
processors().
But you can change the size of this pool using the system property java. Util
. Concurrent. Forkjoinpool. Common. Parallelism, as in the following example:

system. Setproperty("java. Util. Concurrent. Forkjoinpool. Common. Parallelism",
"12");

this is a global setting, so it will affect all the parallel streams in your code. Con-
versely, it currently isnt possible to specify this value for a single parallel stream. In
general, having the size of the forkjoinpool equal to the number of processors on
your machine is a meaningful default, and we strongly suggest that you not modify it
unless you have a good reason for doing so.

Returning to the number-summing exercise, we said that you can expect a significant
performance improvement in its parallel version when running it on a multicore
processor. You now have three methods executing exactly the same operation in three
176 chapter 7 parallel data processing and performance

different ways (iterative style, sequential reduction, and parallel reduction), so lets
see which is the fastest one!

7.1.2 measuring stream performance
we claimed that the parallelized summing method should perform better than the
sequential and the iterative methods. Nevertheless, in software engineering, guessing
is never a good idea! When optimizing performance, you should always follow three
golden rules: measure, measure, measure. To this purpose we will implement a micro-
benchmark using a library called java microbenchmark harness (jmh). This is a tool-
kit that helps to create, in a simple, annotation-based way, reliable microbenchmarks
for java programs and for any other language targeting the java virtual machine
(jvm). In fact, developing correct and meaningful benchmarks for programs running
on the jvm is not an easy task, because there are many factors to consider like the
warm-up time required by hotspot to optimize the bytecode and the overhead intro-
duced by the garbage collector. If youre using maven as your build tool, then to start
using jmh in your project you add a couple of dependencies to your pom. Xml file
(which defines the maven build process).
<dependency>
<groupid>org. Openjdk. Jmh</groupid>
<artifactid>jmh-core</artifactid>
<version>1.17.4</version>
</dependency>
<dependency>
<groupid>org. Openjdk. Jmh</groupid>
<artifactid>jmh-generator-annprocess</artifactid>
<version>1.17.4</version>
</dependency>

the first library is the core jmh implementation while the second contains an annota-
tion processor that helps to generate a java archive (jar) file through which you can
conveniently run your benchmark once you have also added the following plugin to
your maven configuration:
<build>
<plugin>
<groupid>org. Apache. Maven. Plugins</groupid>
<artifactid>maven-shade-plugin</artifactid>
<executions>
<execution>
<phase>package</phase>
<goals><goal>shade</goal></goals>
<configuration>
<finalname>benchmarks</finalname>
<transformers>
<transformer implementation="org. Apache. Maven. Plugins. Shade.
Resource. Manifestresourcetransformer">
<mainclass>org. Openjdk. Jmh. Main</mainclass>
</transformer>
parallel streams 177

</transformers>
</configuration>
</execution>
</executions>
</plugin>
</plugins>
</build>

having done this, you can benchmark the sequentialsum method introduced at the
beginning of this section in this simple way, as shown in the next listing.

Listing 7.1 measuring performance of a function summing the first n numbers

measures the average time taken
@benchmarkmode(mode. Averagetime) to the benchmarked method
@outputtimeunit(timeunit. Milliseconds)
prints benchmark results using
@fork(2, jvmargs={"-xms4g", "-xmx4g"}) milliseconds as time unit
public class parallelstreambenchmark {
private static final long n= 10_000_000l; executes the benchmark
2 times to increase the
reliability of results, with
@benchmark 4gb of heap space
the
method to be public long sequentialsum() {
benchmarked return stream. Iterate(1l, i -> i + 1). Limit(n)
. Reduce( 0l, long: : sum);
}

@teardown(level. Invocation)
tries to run the garbage
public void teardown() { collector after each iteration
system. Gc(); of the benchmark
}
}

when you compile this class, the maven plugin configured before generates a second
jar file named benchmarks. Jar that you can run as follows:

java -jar . /target/benchmarks. Jar parallelstreambenchmark

we configured the benchmark to use an oversized heap to avoid any influence of the
garbage collector as much as possible, and for the same reason, we tried to enforce
the garbage collector to run after each iteration of our benchmark. Despite all these
precautions, it has to be noted that the results should be taken with a grain of salt.
Many factors will influence the execution time, such as how many cores your machine
supports! You can try this on your own machine by running the code available on the
books repository.
When you launch the former, command jmh to execute 20 warm-up iterations of
the benchmarked method to allow hotspot to optimize the code, and then 20 more
iterations that are used to calculate the final result. These 20+20 iterations are the
default behavior of jmh, but you can change both values either through other jmh
specific annotations or, even more conveniently, by adding them to the command line
178 chapter 7 parallel data processing and performance

using the -w and -i flags. Executing it on a computer equipped with an intel i7-4600u
2.1 ghz quad-core, it prints the following result:
benchmark mode cnt score error units
parallelstreambenchmark. Sequentialsum avgt 40 121.843  3.062 ms/op

you should expect that the iterative version using a traditional for loop runs much
faster because it works at a much lower level and, more important, doesnt need to
perform any boxing or unboxing of the primitive values. We can check this intuition
by adding a second method to the benchmarking class of listing 7.1 and also annotate
it with @benchmark:
@benchmark
public long iterativesum() {
long result = 0;
for (long i = 1l; i <= n; i++) {
result += i;
}
return result;
}

running this second benchmark (possibly having commented out the first one to
avoid running it again) on our testing machine, we obtained the following result:
benchmark mode cnt score error units
parallelstreambenchmark. Iterativesum avgt 40 3.278  0.192 ms/op

this confirmed our expectations: the iterative version is almost 40 times faster than
the one using the sequential stream for the reasons we anticipated. Now lets do the
same with the version using the parallel stream, also adding that method to our
benchmarking class. We obtained the following outcome:
benchmark mode cnt score error units
parallelstreambenchmark. Parallelsum avgt 40 604.059  55.288 ms/op

this is quite disappointing: the parallel version of the summing method isnt taking
any advantage of our quad-core cpu and is around five times slower than the sequen-
tial one. How can you explain this unexpected result? Two issues are mixed together:
 iterate generates boxed objects, which have to be unboxed to numbers before
they can be added.
 Iterate is difficult to divide into independent chunks to execute in parallel.

The second issue is particularly interesting because you need to keep a mental model
that some stream operations are more parallelizable than others. Specifically, the
iterate operation is hard to split into chunks that can be executed independently,
because the input of one function application always depends on the result of the pre-
vious application, as illustrated in figure 7.2.
Parallel streams 179

output

function iterate

input

figure 7.2 iterate is
inherently sequential.

This means that in this specific case the reduction process isnt proceeding as depicted
in figure 7.1: the whole list of numbers isnt available at the beginning of the reduction
process, making it impossible to efficiently partition the stream in chunks to be pro-
cessed in parallel. By flagging the stream as parallel, youre adding the overhead of allo-
cating each sum operation on a different thread to the sequential processing.
This demonstrates how parallel programming can be tricky and sometimes coun-
terintuitive. When misused (for example, using an operation thats not parallel-
friendly, like iterate) it can worsen the overall performance of your programs, so its
mandatory to understand what happens behind the scenes when you invoke that
apparently magic parallel method.
Using more specialized methods
so how can you use your multicore processors and use the stream to perform a paral-
lel sum in an effective way? We discussed a method called longstream. Rangeclosed in
chapter 5. This method has two benefits compared to iterate:
 longstream. Rangeclosed works on primitive long numbers directly so theres
no boxing and unboxing overhead.
 Longstream. Rangeclosed produces ranges of numbers, which can be easily
split into independent chunks. For example, the range 120 can be split into 15,
610, 1115, and 1620.
Lets first see how it performs on a sequential stream by adding the following method to
our benchmarking class to check if the overhead associated with unboxing is relevant:
@benchmark
public long rangedsum() {
return longstream. Rangeclosed(1, n)
. Reduce(0l, long: : sum);
}

this time the output is
benchmark mode cnt score error units
parallelstreambenchmark. Rangedsum avgt 40 5.315  0.285 ms/op
180 chapter 7 parallel data processing and performance

the numeric stream is much faster than the earlier sequential version, generated with
the iterate factory method, because the numeric stream avoids all the overhead
caused by all the unnecessary autoboxing and auto-unboxing operations performed
by the nonspecialized stream. This is evidence that choosing the right data structures
is often more important than parallelizing the algorithm that uses them. But what
happens if you try to use a parallel stream in this new version that follows?
@benchmark
public long parallelrangedsum() {
return longstream. Rangeclosed(1, n)
. Parallel()
. Reduce(0l, long: : sum);
}

now, adding this method to our benchmarking class we obtained
benchmark mode cnt score error units
parallelstreambenchmark. Parallelrangedsum avgt 40 2.677  0.214 ms/op

finally, we got a parallel reduction thats faster than its sequential counterpart,
because this time the reduction operation can be executed as shown in figure 7.1.
This also demonstrates that using the right data structure and then making it work in
parallel guarantees the best performance. Note that this latest version is also around
20% faster than the original iterative one, demonstrating that, when used correctly,
the functional-programming style allows us to use the parallelism of modern multi-
core cpus in a simpler and more straightforward way than its imperative counterpart.
Nevertheless, keep in mind that parallelization doesnt come for free. The paral-
lelization process itself requires you to recursively partition the stream, assign the
reduction operation of each substream to a different thread, and then combine the
results of these operations in a single value. But moving data between multiple cores is
also more expensive than you might expect, so its important that work to be done in
parallel on another core takes longer than the time required to transfer the data from
one core to another. In general, there are many cases where it isnt possible or conve-
nient to use parallelization. But before you use a parallel stream to make your code
faster, you have to be sure that youre using it correctly; its not helpful to produce a
result in less time if the result will be wrong. Lets look at a common pitfall.

7.1.3 using parallel streams correctly
the main cause of errors generated by misuse of parallel streams is the use of algo-
rithms that mutate some shared state. Heres a way to implement the sum of the first n
natural numbers by mutating a shared accumulator:
public long sideeffectsum(long n) {
accumulator accumulator = new accumulator();
longstream. Rangeclosed(1, n). Foreach(accumulator: : add);
return accumulator. Total;
}
parallel streams 181

public class accumulator {
public long total = 0;
public void add(long value) { total += value; }
}

its quite common to write this sort of code, especially for developers who are famil-
iar with imperative programming paradigms. This code closely resembles what
youre used to doing when iterating imperatively a list of numbers: you initialize an
accumulator and traverse the elements in the list one by one, adding them on the
accumulator.
Whats wrong with this code? Unfortunately, its irretrievably broken because its
fundamentally sequential. You have a data race on every access of total. And if you try
to fix that with synchronization, youll lose all your parallelism. To understand this,
lets try to turn the stream into a parallel one:
public long sideeffectparallelsum(long n) {
accumulator accumulator = new accumulator();
longstream. Rangeclosed(1, n). Parallel(). Foreach(accumulator: : add);
return accumulator. Total;
}

try to run this last method with the harness of listing 7.1, also printing the result of
each execution:
system. Out. Println("sideeffect parallel sum done in: " +
measureperf(parallelstreams: : sideeffectparallelsum, 10_000_000l) + "
msecs" );

you could obtain something like the following:
result: 5959989000692
result: 7425264100768
result: 6827235020033
result: 7192970417739
result: 6714157975331
result: 7497810541907
result: 6435348440385
result: 6999349840672
result: 7435914379978
result: 7715125932481
sideeffect parallel sum done in: 49 msecs

this time the performance of your method isnt important. The only relevant thing is
that each execution returns a different result, all distant from the correct value of
50000005000000. This is caused by the fact that multiple threads are concurrently
accessing the accumulator and, in particular, executing total += value, which,
despite its appearance, isnt an atomic operation. The origin of the problem is that
the method invoked inside the foreach block has the side effect of changing the
mutable state of an object shared among multiple threads. Its mandatory to avoid
182 chapter 7 parallel data processing and performance

these kinds of situations if you want to use parallel streams without incurring similar
bad surprises.
Now you know that a shared mutable state doesnt play well with parallel streams and
with parallel computations in general. Well come back to this idea of avoiding mutation
in chapters 18 and 19 when discussing functional programming in more detail. For now,
keep in mind that avoiding a shared mutable state ensures that your parallel stream will
produce the right result. Next, well look at some practical advice you can use to figure
out when its appropriate to use parallel streams to gain performance.

7.1.4 using parallel streams effectively
in general, its impossible (and pointless) to try to give any quantitative hint on when to
use a parallel stream, because any specific criterion such as only when the stream con-
tains more than a thousand elements could be correct for a specific operation running
on a specific machine, but completely wrong in a marginally different context. Nonethe-
less, its at least possible to provide some qualitative advice that could be useful when
deciding whether it makes sense to use a parallel stream in a certain situation:
 if in doubt, measure. Turning a sequential stream into a parallel one is trivial
but not always the right thing to do. As we already demonstrated in this section,
a parallel stream isnt always faster than the corresponding sequential version.
Moreover, parallel streams can sometimes work in a counterintuitive way, so the
first and most important suggestion when choosing between sequential and
parallel streams is to always check their performance with an appropriate
benchmark.
 Watch out for boxing. Automatic boxing and unboxing operations can dramat-
ically hurt performance. Java 8 includes primitive streams (intstream, long-
stream, and doublestream) to avoid such operations, so use them when possible.
 Some operations naturally perform worse on a parallel stream than on a
sequential stream. In particular, operations such as limit and findfirst that
rely on the order of the elements are expensive in a parallel stream. For exam-
ple, findany will perform better than findfirst because it isnt constrained to
operate in the encounter order. You can always turn an ordered stream into an
unordered stream by invoking the method unordered on it. For instance, if you
need n elements of your stream and youre not necessarily interested in the first
n ones, calling limit on an unordered parallel stream may execute more effi-
ciently than on a stream with an encounter order (for example, when the
source is a list).
 Consider the total computational cost of the pipeline of operations performed
by the stream. With n being the number of elements to be processed and q the
approximate cost of processing one of these elements through the stream pipe-
line, the product of n*q gives a rough qualitative estimation of this cost. A
higher value for q implies a better chance of good performance when using a
parallel stream.
Parallel streams 183

 for a small amount of data, choosing a parallel stream is almost never a winning
decision. The advantages of processing in parallel only a few elements arent
enough to compensate for the additional cost introduced by the parallelization
process.
 Take into account how well the data structure underlying the stream decom-
poses. For instance, an arraylist can be split much more efficiently than a
linkedlist, because the first can be evenly divided without traversing it, as its
necessary to do with the second. Also, the primitive streams created with the
range factory method can be decomposed quickly. Finally, as youll learn in sec-
tion 7.3, you can get full control of this decomposition process by implement-
ing your own spliterator.
 The characteristics of a stream, and how the intermediate operations through
the pipeline modify them, can change the performance of the decomposition
process. For example, a sized stream can be divided into two equal parts, and
then each part can be processed in parallel more effectively, but a filter opera-
tion can throw away an unpredictable number of elements, making the size of
the stream itself unknown.
 Consider whether a terminal operation has a cheap or expensive merge step
(for example, the combiner method in a collector). If this is expensive, then
the cost caused by the combination of the partial results generated by each sub-
stream can outweigh the performance benefits of a parallel stream.
Table 7.1 gives a summary of the parallel-friendliness of certain stream sources in
terms of their decomposability.

Table 7.1 stream sources and decomposability

source decomposability

arraylist excellent

linkedlist poor

intstream. Range excellent

stream. Iterate poor

hashset good

treeset good

finally, we need to emphasize that the infrastructure used behind the scenes by paral-
lel streams to execute operations in parallel is the fork/join framework introduced in
java 7. The parallel summing example proved that its vital to have a good understand-
ing of the parallel stream internals in order to use them correctly, so well investigate
in detail the fork/join framework in the next section.
184 chapter 7 parallel data processing and performance

7.2 the fork/join framework
the fork/join framework was designed to recursively split a parallelizable task into
smaller tasks and then combine the results of each subtask to produce the overall
result. Its an implementation of the executorservice interface, which distributes
those subtasks to worker threads in a thread pool, called forkjoinpool. Lets start by
exploring how to define a task and subtasks.

7.2.1 working with recursivetask
to submit tasks to this pool, you have to create a subclass of recursivetask<r>, where
r is the type of the result produced by the parallelized task (and each of its subtasks)
or of recursiveaction if the task returns no result (it could be updating other nonlo-
cal structures, though). To define recursivetasks you need only implement its single
abstract method, compute:

protected abstract r compute();

this method defines both the logic of splitting the task at hand into subtasks and the
algorithm to produce the result of a single subtask when its no longer possible or con-
venient to further divide it. For this reason an implementation of this method often
resembles the following pseudocode:

if (task is small enough or no longer divisible) {
compute task sequentially
} else {
split task in two subtasks
call this method recursively possibly further splitting each subtask
wait for the completion of all subtasks
combine the results of each subtask
}

in general, there are no precise criteria for deciding whether a given task should be
further divided or not, but there are various heuristics that you can follow to help you
with this decision. We clarify them in more detail in section 7.2.2. The recursive task-
splitting process is visually synthesized by figure 7.3.
As you might have noticed, this is nothing more than the parallel version of the
well-known divide-and-conquer algorithm. To demonstrate a practical example of how
to use the fork/join framework and to build on our previous examples, lets try to cal-
culate the sum of a range of numbers (here represented by an array of numbers
long[]) using this framework. As explained, you need to first provide an implemen-
tation for the recursivetask class, as shown by the forkjoinsumcalculator in
listing 7.2.
The fork/join framework 185

fork recursively a task
in smaller subtask fork
until each subtask
is small enough

fork fork

sequential sequential sequential sequential
evaluation evaluation evaluation evaluation
evaluate all
subtasks in
parallel

join join

recombine
the partial
results
join

figure 7.3 the fork/join process

listing 7.2 executing a parallel sum using the fork/join framework

the initial and final positions of the extends recursivetask to
subarray processed by this subtask create a task usable with
the fork/join framework
public class forkjoinsumcalculator
extends java. Util. Concurrent. Recursivetask<long> {
private final long[] numbers;
private final int start; the array of numbers
private final int end; to be summed
public static final long threshold = 10_000;
the size
threshold for public forkjoinsumcalculator(long[] numbers) {
public constructor to
splitting into this(numbers, 0, numbers. Length); create the main task
subtasks }
private forkjoinsumcalculator(long[] numbers, int start, int end) {
this. Numbers = numbers;
this. Start = start; private constructor to create
this. End = end; subtasks of the main task
}
@override override the abstract
protected long compute() { method of recursivetask
186 chapter 7 parallel data processing and performance

int length = end - start;
the size of
the subarray if (length <= threshold) { if the size is less than or equal
summed by return computesequentially(); to the threshold, computes
this task } the result sequentially
forkjoinsumcalculator lefttask =
new forkjoinsumcalculator(numbers, start, start + length/2);
creates a
subtask to lefttask. Fork();
sum the forkjoinsumcalculator righttask =
first half of new forkjoinsumcalculator(numbers, start + length/2, end);
the array long rightresult = righttask. Compute();
long leftresult = lefttask. Join(); creates a subtask
return leftresult + rightresult; to sum the second
asynchronously } half of the array
executes the
private long computesequentially() {
newly created executes this second
long sum = 0;
subtask using subtask synchronously,
another for (int i = start; i < end; i++) {
sum += numbers[i];
potentially allowing
thread of further recursive splits
forkjoinpool }
return sum; a simple sequential
} algorithm for sizes reads the result of the
} below the threshold first subtaskwaiting
if it isnt ready
combines the results
of the two subtasks

writing a method performing a parallel sum of the first n natural numbers is now
straightforward. You need to pass the desired array of numbers to the constructor of
forkjoinsumcalculator:
public static long forkjoinsum(long n) {
long[] numbers = longstream. Rangeclosed(1, n). Toarray();
forkjointask<long> task = new forkjoinsumcalculator(numbers);
return new forkjoinpool(). Invoke(task);
}

here, you generate an array containing the first n natural numbers using a long-
stream. Then you create a forkjointask (the superclass of recursivetask), passing
this array to the public constructor of the forkjoinsumcalculator shown in listing 7.2.
Finally, you create a new forkjoinpool and pass that task to its invoke method. The
value returned by this last method is the result of the task defined by the forkjoin-
sumcalculator class when executed inside the forkjoinpool.
Note that in a real-world application, it doesnt make sense to use more than one
forkjoinpool. For this reason, what you typically should do is instantiate it only once
and keep this instance in a static field, making it a singleton, so it could be conve-
niently reused by any part of your software. Here, to create it youre using its default
no-argument constructor, meaning that you want to allow the pool to use all the proces-
sors available to the jvm. More precisely, this constructor will use the value returned by
runtime. Availableprocessors to determine the number of threads used by the pool.
Note that the availableprocessors method, despite its name, in reality returns the
number of available cores, including any virtual ones due to hyperthreading.
The fork/join framework 187

running the forkjoinsumcalculator
when you pass the forkjoinsumcalculator task to the forkjoinpool, this task is exe-
cuted by a thread of the pool that in turn calls the compute method of the task. This
method checks to see if the task is small enough to be performed sequentially; other-
wise, it splits the array of numbers to be summed into two halves and assigns them to
two new forkjoinsumcalculators that are scheduled to be executed by the fork-
joinpool. As a result, this process can be recursively repeated, allowing the original
task to be divided into smaller tasks, until the condition used to check if its no longer
convenient or no longer possible to further split it is met (in this case, if the number
of items to be summed is less than or equal to 10,000). At this point, the result of
each subtask is computed sequentially, and the (implicit) binary tree of tasks cre-
ated by the forking process is traversed back toward its root. The result of the task is
then computed, combining the partial results of each subtask. This process is shown
in figure 7.4.

Final result

c = f = g
split

a + b = c d + e = f
split split

sequential sequential sequential sequential
reduction reduction reduction reduction

result = a result = b result = d result = e

figure 7.4 the fork/join algorithm

once again you can check the performance of the summing method explicitly using
the fork/join framework with the harness developed at the beginning of this chapter:

system. Out. Println("forkjoin sum done in: " + measuresumperf(
forkjoinsumcalculator: : forkjoinsum, 10_000_000) + " msecs" );

in this case it produces the following output:

forkjoin sum done in: 41 msecs
188 chapter 7 parallel data processing and performance

here, the performance is worse than the version using the parallel stream, but only
because youre obliged to put the whole stream of numbers into a long[] before
being allowed to use it in the forkjoinsumcalculator task.

7.2.2 best practices for using the fork/join framework
even though the fork/join framework is relatively easy to use, unfortunately its also
easy to misuse. Here are a few best practices to use it effectively:
 invoking the join method on a task blocks the caller until the result produced
by that task is ready. For this reason, its necessary to call it after the computa-
tion of both subtasks has been started. Otherwise, youll end up with a slower
and more complex version of your original sequential algorithm because every
subtask will have to wait for the other one to complete before starting.
 The invoke method of a forkjoinpool shouldnt be used from within a
recursivetask. Instead, you should always call the methods compute or fork
directly; only sequential code should use invoke to begin parallel computation.
 Calling the fork method on a subtask is the way to schedule it on the fork-
joinpool. It might seem natural to invoke it on both the left and right sub-
tasks, but this is less efficient than directly calling compute on one of them.
Doing this allows you to reuse the same thread for one of the two subtasks and
avoid the overhead caused by the unnecessary allocation of a further task on
the pool.
 Debugging a parallel computation using the fork/join framework can be tricky.
In particular, its ordinarily quite common to browse a stack trace in your favor-
ite ide to discover the cause of a problem, but this cant work with a fork/join
computation because the call to compute occurs in a different thread than the
conceptual caller, which is the code that called fork.
 As youve discovered with parallel streams, you should never take for granted
that a computation using the fork/join framework on a multicore processor is
faster than the sequential counterpart. We already said that a task should be
decomposable into several independent subtasks in order to be parallelizable
with a relevant performance gain. All of these subtasks should take longer to
execute than forking a new task; one idiom is to put i/o into one subtask and
computation into another, thereby overlapping computation with i/o. More-
over, you should consider other things when comparing the performance of
the sequential and parallel versions of the same algorithm. Like any other
java code, the fork/join framework needs to be warmed up, or executed, a
few times before being optimized by the jit compiler. This is why its always
important to run the program multiple times before to measure its perfor-
mance, as we did in our harness. Also be aware that optimizations built into
the compiler could unfairly give an advantage to the sequential version (for
example, by performing dead code analysisremoving a computation thats
never used).
The fork/join framework 189

the fork/join splitting strategy deserves one last note: you must choose the criteria
used to decide if a given subtask should be further split or is small enough to be evalu-
ated sequentially. Well give some hints about this in the next section.

7.2.3 work stealing
in our forkjoinsumcalculator example we decided to stop creating more subtasks
when the array of numbers to be summed contained at most 10,000 items. This is an
arbitrary choice, but in most cases its difficult to find a good heuristic, other than try-
ing to optimize it by making several attempts with different inputs. In our test case, we
started with an array of 10 million items, meaning that the forkjoinsumcalculator
will fork at least 1,000 subtasks. This might seem like a waste of resources because we
ran it on a machine that has only four cores. In this specific case, thats probably true
because all tasks are cpu bound and are expected to take a similar amount of time.
But forking a quite large number of fine-grained tasks is in general a winning
choice. This is because ideally you want to partition the workload of a parallelized task
in such a way that each subtask takes exactly the same amount of time, keeping all the
cores of your cpu equally busy. Unfortunately, especially in cases closer to real-world
scenarios than the straightforward example we presented here, the time taken by each
subtask can dramatically vary either due to the use of an inefficient partition strategy
or because of unpredictable causes like slow access to the disk or the need to coordi-
nate the execution with external services.
The fork/join framework works around this problem with a technique called work
stealing. In practice, this means that the tasks are more or less evenly divided on all the
threads in the forkjoinpool. Each of these threads holds a doubly linked queue of
the tasks assigned to it, and as soon as it completes a task it pulls another one from the
head of the queue and starts executing it. For the reasons we listed previously, one
thread might complete all the tasks assigned to it much faster than the others, which
means its queue will become empty while the other threads are still pretty busy. In this
case, instead of becoming idle, the thread randomly chooses a queue of a different
thread and steals a task, taking it from the tail of the queue. This process continues
until all the tasks are executed, and then all the queues become empty. Thats why
having many smaller tasks, instead of only a few bigger ones, can help in better balanc-
ing the workload among the worker threads.
More generally, this work-stealing algorithm is used to redistribute and balance the
tasks among the worker threads in the pool. Figure 7.5 shows how this process occurs.
When a task in the queue of a worker is divided into two subtasks, one of the two sub-
tasks is stolen by another idle worker. As described previously, this process can con-
tinue recursively until the condition used to define that a given subtask should be
executed sequentially becomes true.
It should now be clear how a stream can use the fork/join framework to process its
items in parallel, but theres still one missing ingredient. In this section, we analyzed
an example where you explicitly developed the logic to split an array of numbers into
190 chapter 7 parallel data processing and performance

split split

worker 1 4 2 2 1 1 1 running

split

worker 2 steal 2 1 1 1 running

worker 3 steal 1 1 running

worker 4 steal 1 1 running

figure 7.5 the work-stealing algorithm used by the fork/join framework

multiple tasks. Nevertheless, you didnt have to do anything similar when you used the
parallel streams at the beginning of this chapter, and this means that there must be an
automatic mechanism splitting the stream for you. This new automatic mechanism is
called the spliterator, and well explore it in the next section.

7.3 spliterator
the spliterator is another new interface added to java 8; its name stands for split-
able iterator. Like iterators, spliterators are used to traverse the elements of a
source, but theyre also designed to do this in parallel. Although you may not have to
develop your own spliterator in practice, understanding how to do so will give you a
wider understanding about how parallel streams work. Java 8 already provides a default
spliterator implementation for all the data structures included in its collections
framework. The collection interface now provides a default method spliterator()
(you will learn more about default methods in chapter 13) which returns a spliterator
object. The spliterator interface defines several methods, as shown in the following
listing.

Listing 7.3 the spliterator interface

public interface spliterator<t> {
boolean tryadvance(consumer<? Super t> action);
spliterator<t> trysplit();
long estimatesize();
int characteristics();
}
spliterator 191

as usual, t is the type of the elements traversed by the spliterator. The tryadvance
method behaves in a way similar to a normal iterator in the sense that its used to
sequentially consume the elements of the spliterator one by one, returning true if
there are still other elements to be traversed. But the trysplit method is more spe-
cific to the spliterator interface because its used to partition off some of its ele-
ments to a second spliterator (the one returned by the method), allowing the two
to be processed in parallel. A spliterator may also provide an estimation of the num-
ber of the elements remaining to be traversed via its estimatesize method, because
even an inaccurate but quick-to-compute value can be useful to split the structure
more or less evenly.
Its important to understand how this splitting process is performed internally in
order to take control of it when required. Therefore, well analyze it in more detail in
the next section.

7.3.1 the splitting process
the algorithm that splits a stream into multiple parts is a recursive process and pro-
ceeds as shown in figure 7.6. In the first step, trysplit is invoked on the first
spliterator and generates a second one. Then in step two, its called again on these
two spliterators, which results in a total of four. The framework keeps invoking the
method trysplit on a spliterator until it returns null to signal that the data

step 1 step 2
spliterator1 spliterator1

trysplit() spliterator2 trysplit()

spliterator2 trysplit() spliterator3

spliterator4

step 3 step 4
spliterator1 spliterator1

spliterator2 spliterator3 trysplit() spliterator2 spliterator3

spliterator4 trysplit() trysplit() null spliterator4 spliterator5 trysplit()

trysplit() null trysplit() null

spliterator5
null null

figure 7.6 the recursive splitting process
192 chapter 7 parallel data processing and performance

structure that its processing is no longer divisible, as shown in step 3. Finally, this
recursive splitting process terminates in step 4 when all spliterators have returned
null to a trysplit invocation.
This splitting process can also be influenced by the characteristics of the spliterator
itself, which are declared via the characteristics method.
The spliterator characteristics
the last abstract method declared by the spliterator interface is characteristics,
which returns an int encoding the set of characteristics of the spliterator itself. The
spliterator clients can use these characteristics to better control and optimize its
usage. Table 7.2 summarizes them. (unfortunately, although these conceptually over-
lap with characteristics of a collector, theyre coded differently. ) the characteristics
are int constants defined in the spliterator interface.

Table 7.2 spliterators characteristics

characteristic meaning

ordered elements have a defined order (for example, a list), so the spliterator
enforces this order when traversing and partitioning them.

Distinct for each pair of traversed elements x and y, x. Equals(y) returns false.

Sorted the traversed elements follow a predefined sort order.

Sized this spliterator has been created from a source with a known size (for example,
a set), so the value returned by estimatedsize() is precise.

Non-null its guaranteed that the traversed elements wont be null.

Immutable the source of this spliterator cant be modified. This implies that no elements
can be added, removed, or modified during their traversal.

Concurrent the source of this spliterator may be safely, concurrently modified by other
threads without any synchronization.

Subsized both this spliterator and all further spliterators resulting from its split are
sized.

Now that youve seen what the spliterator interface is and which methods it defines,
you can try to develop your own implementation of a spliterator.

7.3.2 implementing your own spliterator
lets look at a practical example of where you might need to implement your own
spliterator. Well develop a simple method that counts the number of words in a
string. An iterative version of this method could be written as shown in the follow-
ing listing.
Spliterator 193

listing 7.4 an iterative word counter method

public int countwordsiteratively(string s) {
int counter = 0;
boolean lastspace = true; traverses all the characters
for (char c : s. Tochararray()) { in the string one by one
if (character. Iswhitespace(c)) {
lastspace = true;
} else {
if (lastspace) counter++;
increases the word counter when
lastspace = false;
the last character is a space and
} the currently traversed one isnt
}
return counter;
}

lets put this method to work on the first sentence of dantes inferno (see http: //en
. Wikipedia. Org/wiki/inferno_(dante). ):
final string sentence =
" nel mezzo del cammin di nostra vita " +
"mi ritrovai in una selva oscura" +
" che la dritta via era smarrita ";
system. Out. Println("found " + countwordsiteratively(sentence) + " words");

note that we added some additional random spaces in the sentence to demonstrate
that the iterative implementation is working correctly even in the presence of multiple
spaces between two words. As expected, this code prints out the following:

found 19 words

ideally youd like to achieve the same result in a more functional style because this way
youll be able, as shown previously, to parallelize this process using a parallel stream
without having to explicitly deal with threads and their synchronization.
Rewriting the wordcounter in functional style
first, you need to convert the string into a stream. Unfortunately, there are primitive
streams only for int, long, and double, so youll have to use a stream<character>:
stream<character> stream = intstream. Range(0, sentence. Length())
. Maptoobj(sentence: : charat);

you can calculate the number of words by performing a reduction on this stream.
While reducing the stream, youll have to carry a state consisting of two variables: an
int counting the number of words found so far and a boolean to remember if the last-
encountered character was a space or not. Because java doesnt have tuples (a con-
struct to represent an ordered list of heterogeneous elements without the need of a
wrapper object), youll have to create a new class, wordcounter, which will encapsulate
this state as shown in the following listing.
194 chapter 7 parallel data processing and performance

listing 7.5 a class to count words while traversing a stream of characters
class wordcounter {
private final int counter;
private final boolean lastspace;
public wordcounter(int counter, boolean lastspace) {
this. Counter = counter; accumulate method
this. Lastspace = lastspace; traverses characters one
} by one as done by the
public wordcounter accumulate(character c) { iterative algorithm
if (character. Iswhitespace(c)) {
return lastspace ?
This : increases the word
new wordcounter(counter, true); counter when the last
} else { character is a space
return lastspace ? And the currently
new wordcounter(counter+1, false) : traversed one isnt
this;
} combines two
} wordcounters by
public wordcounter combine(wordcounter wordcounter) { summing their
return new wordcounter(counter + wordcounter. Counter, counters
wordcounter. Lastspace);
} uses only the sum
public int getcounter() { of the counters so
return counter; you dont care
} about lastspace
}

in this listing, the accumulate method defines how to change the state of the word-
counter, or, more precisely, with which state to create a new wordcounter because its
an immutable class. This is important to understand. We are accumulating state with
an immutable class specifically so that the process can be parallelized in the next step.
The method accumulate is called whenever a new character of the stream is tra-
versed. In particular, as you did in the countwordsiteratively method in listing 7.4,
the counter is incremented when a new nonspace is met, and the last character
encountered is a space. Figure 7.7 shows the state transitions of the wordcounter
when a new character is traversed by the accumulate method.

C is space

c is space
wordcounter wordcounter
lastspace == false lastspace == true
c is not space
increment counter

c is not space

figure 7.7 the state transitions of the wordcounter when a new character
c is traversed
spliterator 195

the second method, combine, is invoked to aggregate the partial results of two word-
counters operating on two different subparts of the stream of characters, so it com-
bines two wordcounters by summing their internal counters.
Now that youve encoded the logic of how to accumulate characters on a word-
counter and how to combine them in the wordcounter itself, writing a method that
will reduce the stream of characters is straightforward:
private int countwords(stream<character> stream) {
wordcounter wordcounter = stream. Reduce(new wordcounter(0, true),
wordcounter: : accumulate,
wordcounter: : combine);
return wordcounter. Getcounter();
}

now you can try this method with the stream created from the string containing the
first sentence of dantes inferno:
stream<character> stream = intstream. Range(0, sentence. Length())
. Maptoobj(sentence: : charat);
system. Out. Println("found " + countwords(stream) + " words");

you can check that its output corresponds with the one generated by the iterative
version:

found 19 words

so far, so good, but we said that one of the main reasons for implementing the word-
counter in functional terms was to be able to easily parallelize this operation, so lets
see how this works.
Making the wordcounter work in parallel
you could try to speed up the word-counting operation using a parallel stream, as
follows:

system. Out. Println("found " + countwords(stream. Parallel()) + " words");

unfortunately, this time the output is

found 25 words

evidently something has gone wrong, but what? The problem isnt hard to discover.
Because the original string is split at arbitrary positions, sometimes a word is divided
in two and then counted twice. In general, this demonstrates that going from a
sequential stream to a parallel one can lead to a wrong result if this result may be
affected by the position where the stream is split.
How can you fix this issue? The solution consists of ensuring that the string isnt
split at a random position but only at the end of a word. To do this, youll have to
196 chapter 7 parallel data processing and performance

implement a spliterator of character that splits a string only between two words
(as shown in the following listing) and then creates the parallel stream from it.

Listing 7.6 the wordcounterspliterator

class wordcounterspliterator implements spliterator<character> {
private final string string;
private int currentchar = 0;
public wordcounterspliterator(string string) {
this. String = string; consumes the
} current character
@override
public boolean tryadvance(consumer<? Super character> action) {
action. Accept(string. Charat(currentchar++));
return currentchar < string. Length();
returns true if there
}
are further characters
@override to be consumed
public spliterator<character> trysplit() {
int currentsize = string. Length() - currentchar; returns null to signal that
sets the if (currentsize < 10) {
candidate split the string to be parsed
return null; is small enough to be
position to be }
half of the string processed sequentially
for (int splitpos = currentsize / 2 + currentchar;
to be parsed splitpos < string. Length(); splitpos++) {
if (character. Iswhitespace(string. Charat(splitpos))) {
advances the
split position spliterator<character> spliterator =
until the new wordcounterspliterator(string. Substring(currentchar,
next space splitpos));
currentchar = splitpos;
return spliterator; sets the start position
} of the current word-
creates a new } counterspliterator to
wordcounter- the split position
return null;
spliterator parsing
}
the string from the
start to the split @override found a space and created
position public long estimatesize() { the new spliterator, so
return string. Length() - currentchar;
exit the loop
}
@override
public int characteristics() {
return ordered + sized + subsized + non-null + immutable;
}
}

this spliterator is created from the string to be parsed and iterates over its
characters by holding the index of the one currently being traversed. Lets quickly
revisit the methods of the wordcounterspliterator implementing the spliterator
interface:
 the tryadvance method feeds the consumer with the character in the string
at the current index position and increments this position. The consumer passed
as its argument is an internal java class forwarding the consumed character to
the set of functions that have to be applied to it while traversing the stream,
spliterator 197

which in this case is only a reducing function, namely, the accumulate method of
the wordcounter class. The tryadvance method returns true if the new cursor
position is less than the total string length and there are further characters to
be iterated.
 The trysplit method is the most important one in a spliterator, because its
the one defining the logic used to split the data structure to be iterated. As you
did in the compute method of the recursivetask implemented in listing 7.1,
the first thing you have to do here is set a limit under which you dont want to
perform further splits. Here, you use a low limit of 10 characters only to make
sure that your program will perform some splits with the relatively short string
youre parsing. But in real-world applications youll have to use a higher limit,
as you did in the fork/join example, to avoid creating too many tasks. If the
number of remaining characters to be traversed is under this limit, you return
null to signal that no further split is necessary. Conversely, if you need to per-
form a split, you set the candidate split position to the half of the string chunk
remaining to be parsed. But you dont use this split position directly because
you want to avoid splitting in the middle of a word, so you move forward until
you find a blank character. Once you find an opportune split position, you cre-
ate a new spliterator that will traverse the substring chunk going from the
current position to the split one; you set the current position of this to the split
one, because the part before it will be managed by the new spliterator, and
then you return it.
 The estimatedsize of elements still to be traversed is the difference between
the total length of the string parsed by this spliterator and the position cur-
rently iterated.
 Finally, the characteristics method signals to the framework that this
spliterator is ordered (the order is the sequence of characters in the
string), sized (the value returned by the estimatedsize method is exact),
subsized (the other spliterators created by the trysplit method also have
an exact size), non-null (there can be no null characters in the string), and
immutable (no further characters can be added while parsing the string
because the string itself is an immutable class).
Putting the wordcounterspliterator to work
you can now use a parallel stream with this new wordcounterspliterator as follows:
spliterator<character> spliterator = new wordcounterspliterator(sentence);
stream<character> stream = streamsupport. Stream(spliterator, true);

the second boolean argument passed to the streamsupport. Stream factory method
means that you want to create a parallel stream. Passing this parallel stream to the
countwords method

system. Out. Println("found " + countwords(stream) + " words");
198 chapter 7 parallel data processing and performance

produces the correct output, as expected:

found 19 words

youve seen how a spliterator can let you to gain control over the policy used to split
a data structure. One last notable feature of spliterators is the possibility of binding
the source of the elements to be traversed at the point of first traversal, first split, or
first query for estimated size, rather than at the time of its creation. When this hap-
pens, its called a late-binding spliterator. Weve dedicated appendix c to showing
how you can develop a utility class capable of performing multiple operations on the
same stream in parallel using this feature.

Summary
 internal iteration allows you to process a stream in parallel without the need to
explicitly use and coordinate different threads in your code.
 Even if processing a stream in parallel is so easy, theres no guarantee that doing
so will make your programs run faster under all circumstances. Behavior and
performance of parallel software can sometimes be counterintuitive, and for
this reason its always necessary to measure them and be sure that youre not
slowing your programs down.
 Parallel execution of an operation on a set of data, as done by a parallel stream,
can provide a performance boost, especially when the number of elements to
be processed is huge or the processing of each single element is particularly
time consuming.
 From a performance point of view, using the right data structure, for instance,
employing primitive streams instead of nonspecialized ones whenever possible,
is almost always more important than trying to parallelize some operations.
 The fork/join framework lets you recursively split a parallelizable task into
smaller tasks, execute them on different threads, and then combine the results
of each subtask in order to produce the overall result.
 Spliterators define how a parallel stream can split the data it traverses.
Part 3

effective programming
with streams and lambdas

t he third part of this book explores various java 8 and java 9 topics that will
make you more effective at using java and enhance your codebase with modern
idioms. Because its oriented toward more advanced programming ideas, nothing
later in the book depends on the techniques described here.
Chapter 8 is a new chapter for the second edition and explores the collec-
tion api enhancements of java 8 and java 9 and covers using collection factories
and learning new idiomatic patterns to work with list and set collections along
with idiomatic patterns involving map.
Chapter 9 explores how you can improve your existing code using new java 8
features and a few recipes. In addition, it explores vital software development
techniques such as design patterns, refactoring, testing, and debugging.
Chapter 10 is again new for the second edition. It explores the idea of bas-
ing an api on a domain-specific language (dsl). This is not only a powerful
way of designing apis but also one that is both becoming increasingly popular
and already visible in java, such as in the comparator, stream, and collector
interfaces.
Collection api
enhancements

this chapter covers
 using collection factories
 learning new idiomatic patterns to use with list
and set
 learning idiomatic patterns to work with map

your life as a java developer would be rather lonely without the collection api. Col-
lections are used in every java applications. In previous chapters, you saw how use-
ful the combination of collections with the streams api is for expressing data
processing queries. Nonetheless, the collection api had various deficiencies, which
made it verbose and error-prone to use at times.
In this chapter, you will learn about new additions to the collection api in java
8 and java 9 that will make your life easier. First, you learn about the collections fac-
tories in java 9new additions that simplify the process of creating small lists, sets,
and maps. Next, you learn how to apply idiomatic removal and replacement pat-
terns in lists and sets thanks to java 8 enhancements. Finally, you learn about new
convenience operations that are available to work with maps.
Chapter 9 explores a wider range of techniques for refactoring old-style java
code.

201
202 chapter 8 collection api enhancements

8.1 collection factories
java 9 introduced a few convenient ways to create small collection objects. First, well
review why programmers needed a better way to do things; then well show you how to
use the new factory methods.
How would you create a small list of elements in java? You might want to group the
names of your friends who are going on a holiday, for example. Heres one way:
list<string> friends = new arraylist<>();
friends. Add("raphael");
friends. Add("olivia");
friends. Add("thibaut");

but thats quite a few lines to write for storing three strings! A more convenient way to
write this code is to use the arrays. Aslist() factory method:
list<string> friends
= arrays. Aslist("raphael", "olivia", "thibaut");

you get a fixed-sized list that you can update, but not add elements to or remove ele-
ments from. Attempting to add elements, for example, results in an unsupported-
modificationexception, but updating by using the method set is allowed:

list<string> friends = arrays. Aslist("raphael", "olivia");
friends. Set(0, "richard");
friends. Add("thibaut");
throws an
unsupportedoperationexception

this behavior seems slightly surprising because the underlying list is backed by a
mutable array of fixed size.
How about a set? Unfortunately, theres no arrays. Asset() factory method, so
you need another trick. You can use the hashset constructor, which accepts a list:
set<string> friends "
= new hashset<>(arrays. Aslist("raphael", "olivia", thibaut"));

alternatively you could use the streams api:
set<string> friends
= stream. Of("raphael", "olivia", "thibaut")
. Collect(collectors. Toset());

both solutions, however, are far from elegant and involve unnecessary object alloca-
tions behind the scenes. Also note that you get a mutable set as a result.
How about map? Theres no elegant way of creating small maps, but dont worry;
java 9 added factory methods to make your life simpler when you need to create small
lists, sets, and maps.
We begin the tour of new ways of creating collections in java by showing you whats
new with lists.
Collection factories 203

collection literals
some languages, including python and groovy, support collection literals, which let
you create collections by using special syntax, such as [42, 1, 5] to create a list of
three numbers. Java doesnt provide syntactic support because language changes
come with a high maintenance cost and restrict future use of a possible syntax.
Instead, java 9 adds support by enhancing the collection api.

8.1.1 list factory
you can create a list simply by calling the factory method list. Of:
list<string> friends = list. Of("raphael", "olivia", "thibaut");
system. Out. Println(friends);
[raphael, olivia, thibaut]

youll notice something strange, however. Try to add an element to your list of friends:
list<string> friends = list. Of("raphael", "olivia", "thibaut");
friends. Add("chih-chun");

running this code results in a java. Lang. Unsupportedoperationexception. In fact,
the list thats produced is immutable. Replacing an item with the set() method throws
a similar exception. You wont be able to modify it by using the set method either.
This restriction is a good thing, however, as it protects you from unwanted mutations
of the collections. Nothing is stopping you from having elements that are mutable
themselves. If you need a mutable list, you can still instantiate one manually. Finally,
note that to prevent unexpected bugs and enable a more-compact internal represen-
tation, null elements are disallowed.

Overloading vs. Varargs
if you further inspect the list interface, you notice several overloaded variants of
list. Of:
static <e> list<e> of(e e1, e e2, e e3, e e4)
static <e> list<e> of(e e1, e e2, e e3, e e4, e e5)

you may wonder why the java api didnt have one method that uses varargs to accept
an arbitrary number of elements in the following style:
static <e> list<e> of(e... Elements)

under the hood, the varargs version allocates an extra array, which is wrapped up
inside a list. You pay the cost for allocating an array, initializing it, and having it gar-
bage-collected later. By providing a fixed number of elements (up to ten) through an
api, you dont pay this cost. Note that you can still create list. Of using more than
ten elements, but in that case the varargs signature is invoked. You also see this
pattern appearing with set. Of and map. Of.
204 chapter 8 collection api enhancements

you may wonder whether you should use the streams api instead of the new collec-
tion factory methods to create such lists. After all, you saw in previous chapters that
you can use the collectors. Tolist() collector to transform a stream into a list.
Unless you need to set up some form of data processing and transformation of the
data, we recommend that you use the factory methods; theyre simpler to use, and the
implementation of the factory methods is simpler and more adequate.
Now that youve learned about a new factory method for list, in the next section,
you will work with sets.

8.1.2 set factory
as with list. Of, you can create an immutable set out of a list of elements:

set<string> friends = set. Of("raphael", "olivia", "thibaut"); [raphael, olivia,
system. Out. Println(friends); thibaut]

if you try to create a set by providing a duplicated element, you receive an illegal-
argumentexception. This exception reflects the contract that sets enforce uniqueness
of the elements they contain:

set<string> friends = set. Of("raphael", "olivia", "olivia");
java. Lang. Illegalargumentexception:
duplicate element: olivia
another popular data structure in java is map. In the next section, you learn about new
ways of creating maps.

8.1.3 map factories
creating a map is a bit more complicated than creating lists and sets because you have
to include both the key and the value. You have two ways to initialize an immutable
map in java 9. You can use the factory method map. Of, which alternates between keys
and values:

map<string, integer> ageoffriends {olivia=25,
= map. Of("raphael", 30, "olivia", 25, "thibaut", 26); raphael=30,
system. Out. Println(ageoffriends); thibaut=26}

this method is convenient if you want to create a small map of up to ten keys and val-
ues. To go beyond this, use the alternative factory method called map. Ofentries,
which takes map. Entry<k, v> objects but is implemented with varargs. This method
requires additional object allocations to wrap up a key and a value:
import static java. Util. Map. Entry;
map<string, integer> ageoffriends
= map. Ofentries(entry("raphael", 30),
entry("olivia", 25), {olivia=25,
entry("thibaut", 26)); raphael=30,
system. Out. Println(ageoffriends); thibaut=26}
working with list and set 205

map. Entry is a new factory method to create map. Entry objects.

Quiz 8.1
what do you think is the output of the following snippet?
List<string> actors = list. Of("keanu", "jessica")
actors. Set(0, "brad");
system. Out. Println(actors)

answer:
an unsupportedoperationexception is thrown. The collection produced by list. Of
is immutable.

So far, youve seen that the new java 9 factory methods allow you to create collections
more simply. But in practice, you have to process the collections. In the next section,
you learn about a few new enhancements to list and set that implement common
processing patterns out of the box.

8.2 working with list and set
java 8 introduced a couple of methods into the list and set interfaces:
 removeif removes element matching a predicate. Its available on all classes
that implement list or set (and is inherited from the collection interface).
 Replaceall is available on list and replaces elements using a (unaryoperator)
function.
 Sort is also available on the list interface and sorts the list itself.

All these methods mutate the collections on which theyre invoked. In other words,
they change the collection itself, unlike stream operations, which produce a new (cop-
ied) result. Why would such methods be added? Modifying collections can be error-
prone and verbose. So java 8 added removeif and replaceall to help.

8.2.1 removeif
consider the following code, which tries to remove transactions that have a reference
code starting with a digit:
for (transaction transaction : transactions) {
if(character. Isdigit(transaction. Getreferencecode(). Charat(0))) {
transactions. Remove(transaction);
}
}

can you see the problem? Unfortunately, this code may result in a concurrent-
modificationexception. Why? Under the hood, the for-each loop uses an iterator
object, so the code executed is as follows:
206 chapter 8 collection api enhancements

for (iterator<transaction> iterator = transactions. Iterator();
iterator. Hasnext(); ) {
transaction transaction = iterator. Next();
if(character. Isdigit(transaction. Getreferencecode(). Charat(0))) {
transactions. Remove(transaction);
problem we are iterating and
} modifying the collection through
} two separate objects

notice that two separate objects manage the collection:
 the iterator object, which is querying the source by using next() and has-
next()
 the collection object itself, which is removing the element by calling remove()

as a result, the state of the iterator is no longer synced with the state of the collection,
and vice versa. To solve this problem, you have to use the iterator object explicitly
and call its remove() method:
for (iterator<transaction> iterator = transactions. Iterator();
iterator. Hasnext(); ) {
transaction transaction = iterator. Next();
if(character. Isdigit(transaction. Getreferencecode(). Charat(0))) {
iterator. Remove();
}
}

this code has become fairly verbose to write. This code pattern is now directly
expressible with the java 8 removeif method, which is not only simpler but also pro-
tects you from these bugs. It takes a predicate indicating which elements to remove:

transactions. Removeif(transaction ->
character. Isdigit(transaction. Getreferencecode(). Charat(0)));

sometimes, though, instead of removing an element, you want to replace it. For this
purpose, java 8 added replaceall.

8.2.2 replaceall
the replaceall method on the list interface lets you replace each element in a list
with a new one. Using the streams api, you could solve this problem as follows:

referencecodes. Stream() [a12, c14, b13]
. Map(code -> character. Touppercase(code. Charat(0)) +
code. Substring(1))
. Collect(collectors. Tolist()) outputs a12,
. Foreach(system. Out: : println); c14, b13

this code results in a new collection of strings, however. You want a way to update the
existing collection. You can use a listiterator object as follows (supporting a set()
method to replace an element):
working with map 207

for (listiterator<string> iterator = referencecodes. Listiterator();
iterator. Hasnext(); ) {
string code = iterator. Next();
iterator. Set(character. Touppercase(code. Charat(0)) + code. Substring(1));
}

as you can see, this code is fairly verbose. In addition, as we explained earlier, using
iterator objects in conjunction with collection objects can be error-prone by mixing
iteration and modification of the collection. In java 8, you can simply write

referencecodes. Replaceall(code -> character. Touppercase(code. Charat(0)) +
code. Substring(1));

youve learned whats new with list and set, but dont forget about map. New addi-
tions to the map interface are covered in the next section.

8.3 working with map
java 8 introduced several default methods supported by the map interface. (default
methods are covered in detail in chapter 13, but here you can think of them as being
preimplemented methods in an interface. ) the purpose of these new operations is to
help you write more concise code by using a readily available idiomatic pattern
instead of implementing it yourself. We look at these operations in the following sec-
tions, starting with the shiny new foreach.

8.3.1 foreach
iterating over the keys and values of a map has traditionally been awkward. In fact, you
needed to use an iterator of a map. Entry<k, v> over the entry set of a map:

for(map. Entry<string, integer> entry: ageoffriends. Entryset()) {
string friend = entry. Getkey();
integer age = entry. Getvalue();
system. Out. Println(friend + " is " + age + " years old");
}

since java 8, the map interface has supported the foreach method, which accepts a
biconsumer, taking the key and value as arguments. Using foreach makes your code
more concise:

ageoffriends. Foreach((friend, age) -> system. Out. Println(friend + " is " +
age + " years old"));

a concern related to iterating over date is sorting it. Java 8 introduced a couple of con-
venient ways to compare entries in a map.
208 chapter 8 collection api enhancements

8.3.2 sorting
two new utilities let you sort the entries of a map by values or keys:
 entry. Comparingbyvalue
 entry. Comparingbykey

the code
map<string, string> favouritemovies
= map. Ofentries(entry("raphael", "star wars"),
entry("cristina", "matrix"),
entry("olivia",
"james bond"));

favouritemovies
. Entryset() processes the elements of
. Stream() the stream in alphabetic
. Sorted(entry. Comparingbykey()) order based on the
. Foreachordered(system. Out: : println); persons name

outputs, in order:
cristina=matrix
olivia=james bond
raphael=star wars

hashmap and performance
the internal structure of a hashmap was updated in java 8 to improve performance.
Entries of a map typically are stored in buckets accessed by the generated hashcode
of the key. But if many keys return the same hashcode, performance deteriorates
because buckets are implemented as linkedlists with o(n) retrieval. Nowadays,
when the buckets become too big, theyre replaced dynamically with sorted trees,
which have o(log(n)) retrieval and improve the lookup of colliding elements. Note
that this use of sorted trees is possible only when the keys are comparable (such
as string or number classes).

Another common pattern is how to act when the key youre looking up in the map isnt
present. The new getordefault method can help.

8.3.3 getordefault
when the key youre looking up isnt present, you receive a null reference that you have
to check against to prevent a nullpointerexception. A common design style is to pro-
vide a default value instead. Now you can encode this idea more simply by using the
getordefault method. This method takes the key as the first argument and a default
value (to be used when the key is absent from the map) as the second argument:
map<string, string> favouritemovies
= map. Ofentries(entry("raphael", "star wars"),
entry("olivia", "james bond"));
working with map 209

system. Out. Println(favouritemovies. Getordefault("olivia", "matrix"));
system. Out. Println(favouritemovies. Getordefault("thibaut", "matrix"));

outputs matrix outputs james bond

note that if the key existed in the map but was accidentally associated with a null value,
getordefault can still return null. Also note that the expression you pass as a fallback
is always evaluated, whether the key exists or not.
Java 8 also included a few more advanced patterns to deal with the presence and
absence of values for a given key. You will learn about these new methods in the next
section.

8.3.4 compute patterns
sometimes, you want to perform an operation conditionally and store its result,
depending on whether a key is present or absent in a map. You may want to cache the
result of an expensive operation given a key, for example. If the key is present, theres
no need to recalculate the result. Three new operations can help:
 computeifabsentif theres no specified value for the given key (its absent or
its value is null), calculate a new value by using the key and add it to the map.
 Computeifpresentif the specified key is present, calculate a new value for it
and add it to the map.
 Computethis operation calculates a new value for a given key and stores it in
the map.
One use of computeifabsent is for caching information. Suppose that you parse each
line of a set of files and calculate their sha-256 representation. If youve processed
the data previously, theres no need to recalculate it.
Now suppose that you implement a cache by using a map, and you use an instance
of messagedigest to calculate sha-256 hashes:
map<string, byte[]> datatohash = new hashmap<>();
messagedigest messagedigest = messagedigest. Getinstance("sha-256");

then you can iterate through the data and cache the results:
line is the key to look
lines. Foreach(line -> up in the map. The operation to
datatohash. Computeifabsent(line, execute if the key
this: : calculatedigest)); is absent

private byte[] calculatedigest(string key) {
return messagedigest. Digest(key. Getbytes(standardcharsets. Utf_8));
}
the helper that will calculate
a hash for the given key

this pattern is also useful for conveniently dealing with maps that store multiple val-
ues. If you need to add an element to a map<k, list<v>>, you need to ensure that the
210 chapter 8 collection api enhancements

entry has been initialized. This pattern is a verbose one to put in place. Suppose that
you want to build up a list of movies for your friend raphael:

string friend = "raphael";
list<string> movies = friendstomovies. Get(friend); check that the list
if(movies == null) { was initialized.
Movies = new arraylist<>();
friendstomovies. Put(friend, movies);
} add the movie.
Movies. Add("star wars");
{raphael:
system. Out. Println(friendstomovies); [star wars]}

how can you use computeifabsent instead? It returns the calculated value after add-
ing it to the map if the key wasnt found; otherwise, it returns the existing value. You
can use it as follows:
friendstomovies. Computeifabsent("raphael", name -> new arraylist<>())
. Add("star wars");
{raphael: [star wars]}

the computeifpresent method calculates a new value if the current value associated
with the key is present in the map and non-null. Note a subtlety: if the function that
produces the value returns null, the current mapping is removed from the map. If you
need to remove a mapping, however, an overloaded version of the remove method is
better suited to the task. You learn about this method in the next section.

8.3.5 remove patterns
you already know about the remove method that lets you remove a map entry for a
given key. Since java 8, an overloaded version removes an entry only if the key is
associated with a specific value. Previously, this code is how youd implement this
behavior (we have nothing against tom cruise, but jack reacher 2 received poor
reviews):
string key = "raphael";
string value = "jack reacher 2";
if (favouritemovies. Containskey(key) &&
objects. Equals(favouritemovies. Get(key), value)) {
favouritemovies. Remove(key);
return true;
}
else {
return false;
}

here is how you can do the same thing now, which you have to admit is much more to
the point:

favouritemovies. Remove(key, value);
working with map 211

in the next section, you learn about ways of replacing elements in and removing ele-
ments from a map.

8.3.6 replacement patterns
map has two new methods that let you replace the entries inside a map:
 replaceallreplaces each entrys value with the result of applying a
bifunction. This method works similarly to replaceall on a list, which you
saw earlier.
 Replacelets you replace a value in the map if a key is present. An additional
overload replaces the value only if it the key is mapped to a certain value.
You could format all the values in a map as follows:
we have to use a
map<string, string> favouritemovies = new hashmap<>(); mutable map since we
favouritemovies. Put("raphael", "star wars"); will be using replaceall
favouritemovies. Put("olivia", "james bond");
favouritemovies. Replaceall((friend, movie) -> movie. Touppercase());
system. Out. Println(favouritemovies);
{olivia=james bond,
raphael=star wars}

the replace patterns youve learned work with a single map. But what if you have to com-
bine and replace values from two maps? You can use a new merge method for that task.

8.3.7 merge
suppose that youd like to merge two intermediate maps, perhaps two separate maps for
two groups of contacts. You can use putall as follows:

map<string, string> family = map. Ofentries(
entry("teo", "star wars"), entry("cristina", "james bond"));
map<string, string> friends = map. Ofentries(
entry("raphael", "star wars")); copies all the
map<string, string> everyone = new hashmap<>(family); entries from friends
everyone. Putall(friends); into everyone
system. Out. Println(everyone);
{cristina=james bond, raphael=
star wars, teo=star wars}

this code works as expected as long as you dont have duplicate keys. If you require
more flexibility in how values are combined, you can use the new merge method. This
method takes a bifunction to merge values that have a duplicate key. Suppose that
cristina is in both the family and friends maps but with different associated movies:

map<string, string> family = map. Ofentries(
entry("teo", "star wars"), entry("cristina", "james bond"));
map<string, string> friends = map. Ofentries(
entry("raphael", "star wars"), entry("cristina", "matrix"));
212 chapter 8 collection api enhancements

then you could use the merge method in combination with foreach to provide a way
to deal with the conflict. The following code concatenates the string names of the two
movies:
given a duplicate
map<string, string> everyone = new hashmap<>(family); key, concatenates
friends. Foreach((k, v) ->
the two values
everyone. Merge(k, v, (movie1, movie2) -> movie1 + " & " + movie2));
system. Out. Println(everyone);
outputs {raphael=star wars, cristina=
james bond & matrix, teo=star wars}

note that the merge method has a fairly complex way to deal with nulls, as specified in
the javadoc:

if the specified key is not already associated with a value or is associated with null,
[merge] associates it with the given non-null value. Otherwise, [merge] replaces the
associated value with the [result] of the given remapping function, or removes [it] if the
result is null.

You can also use merge to implement initialization checks. Suppose that you have a
map for recording how many times a movie is watched. You need to check that the key
representing the movie is in the map before you can increment its value:

map<string, long> moviestocount = new hashmap<>();
string moviename = "jamesbond";
long count = moviestocount. Get(moviename);
if(count == null) {
moviestocount. Put(moviename, 1);
}
else {
moviestocount. Put(moviename, count + 1);
}

this code can be rewritten as

moviestocount. Merge(moviename, 1l, (key, count) -> count + 1l);

the second argument to merge in this case is 1l. The javadoc specifies that this argu-
ment is the non-null value to be merged with the existing value associated with the
key or, if no existing value or a null value is associated with the key, to be associated
with the key. Because the value returned for that key is null, the value 1 is provided
the first time around. The next time around, because the value for the key was initial-
ized to the value of 1, the bifunction is applied to increment the count.
Youve learned about the additions to the map interface. New enhancements were
added to a cousin: concurrenthashmap which you will learn about next.
Improved concurrenthashmap 213

quiz 8.2
figure out what the following code does, and think of what idiomatic operation you
could use to simplify what its doing:

map<string, integer> movies = new hashmap<>();
movies. Put("jamesbond", 20);
movies. Put("matrix", 15);
movies. Put("harry potter", 5);
iterator<map. Entry<string, integer>> iterator =
movies. Entryset(). Iterator();
while(iterator. Hasnext()) {
map. Entry<string, integer> entry = iterator. Next();
if(entry. Getvalue() < 10) {
iterator. Remove();
}
} {matrix=15,
system. Out. Println(movies); jamesbond=20}

answer:
you can use the removeif method on the maps entry set, which takes a predicate
and removes the elements:

movies. Entryset(). Removeif(entry -> entry. Getvalue() < 10);

8.4 improved concurrenthashmap
the concurrenthashmap class was introduced to provide a more modern hashmap,
which is also concurrency friendly. Concurrenthashmap allows concurrent add and
update operations that lock only certain parts of the internal data structure. Thus,
read and write operations have improved performance compared with the synchro-
nized hashtable alternative. (note that the standard hashmap is unsynchronized. )

8.4.1 reduce and search
concurrenthashmap supports three new kinds of operations, reminiscent of what you
saw with streams:
 foreachperforms a given action for each (key, value)
 reducecombines all (key, value) given a reduction function into a result
 searchapplies a function on each (key, value) until the function produces a
non-null result
each kind of operation supports four forms, accepting functions with keys, values,
map. Entry, and (key, value) arguments:
 operates with keys and values (foreach, reduce, search)
 operates with keys (foreachkey, reducekeys, searchkeys)
214 chapter 8 collection api enhancements

 operates with values (foreachvalue, reducevalues, searchvalues)
 operates with map. Entry objects (foreachentry, reduceentries, search-
entries)
note that these operations dont lock the state of the concurrenthashmap; they oper-
ate on the elements as they go along. The functions supplied to these operations
shouldnt depend on any ordering or on any other objects or values that may change
while computation is in progress.
In addition, you need to specify a parallelism threshold for all these operations.
The operations execute sequentially if the current size of the map is less than the
given threshold. A value of 1 enables maximal parallelism using the common thread
pool. A threshold value of long. Max_value runs the operation on a single thread. You
generally should stick to these values unless your software architecture has advanced
resource-use optimization.
In this example, you use the reducevalues method to find the maximum value in
the map:
a concurrenthashmap, presumed to be
updated to contain several keys and values
concurrenthashmap<string, long> map = new concurrenthashmap<>();
long parallelismthreshold = 1;
optional<integer> maxvalue =
optional. Ofnullable(map. Reducevalues(parallelismthreshold, long: : max));

note the primitive specializations for int, long, and double for each reduce operation
(reducevaluestoint, reducekeystolong, and so on), which are more efficient, as
they prevent boxing.

8.4.2 counting
the concurrenthashmap class provides a new method called mappingcount, which
returns the number of mappings in the map as a long. You should use it for new code
in preference to the size method, which returns an int. Doing so future proofs your
code for use when the number of mappings no longer fits in an int.

8.4.3 set views
the concurrenthashmap class provides a new method called keyset that returns a
view of the concurrenthashmap as a set. (changes in the map are reflected in the set,
and vice versa. ) you can also create a set backed by a concurrenthashmap by using the
new static method newkeyset.

Summary
 java 9 supports collection factories, which let you create small immutable lists,
sets, and maps by using list. Of, set. Of, map. Of, and map. Ofentries.
 The objects returned by these collection factories are immutable, which means
that you cant change their state after creation.
Summary 215

 the list interface supports the default methods removeif, replaceall, and
sort.
 The set interface supports the default method removeif.
 The map interface includes several new default methods for common patterns
and reduces the scope for bugs.
 Concurrenthashmap supports the new default methods inherited from map but
provides thread-safe implementations for them.
Refactoring, testing,
and debugging

this chapter covers
 refactoring code to use lambda expressions
 appreciating the impact of lambda expressions
on object-oriented design patterns
 testing lambda expressions
 debugging code that uses lambda expressions
and the streams api

in the first eight chapters of this book, you saw the expressive power of lambdas and
the streams api. You were mainly creating new code that used these features. If you
have to start a new java project, you can use lambdas and streams immediately.
Unfortunately, you dont always get to start a new project from scratch. Most
of the time you have to deal with an existing code base written in an older version
of java.
This chapter presents several recipes that show you how to refactor existing
code to use lambda expressions to gain readability and flexibility. In addition, we
discuss how several object-oriented design patterns (including strategy, template
method, observer, chain of responsibility, and factory) can be made more concise

216
refactoring for improved readability and flexibility 217

thanks to lambda expressions. Finally, we explore how you can test and debug code
that uses lambda expressions and the streams api.
In chapter 10, we explore a more wide-ranging way of refactoring code to make
the application logic more readable: creating a domain-specific language.

9.1 refactoring for improved readability and flexibility
from the start of this book, weve argued that lambda expressions let you write more
concise and flexible code. The code is more concise because lambda expressions let
you represent a piece of behavior in a more compact form compared with using anon-
ymous classes. We also showed you in chapter 3 that method references let you write
even more concise code when all you want to do is pass an existing method as an argu-
ment to another method.
Your code is more flexible because lambda expressions encourage the style of
behavior parameterization that we introduced in chapter 2. Your code can use and
execute multiple behaviors passed as arguments to cope with requirement changes.
In this section, we bring everything together and show you simple steps for refac-
toring code to gain readability and flexibility, using the features you learned in previ-
ous chapters: lambdas, method references, and streams.

9.1.1 improving code readability
what does it mean to improve the readability of code? Defining good readability can
be subjective. The general view is that the term means how easily this code can be
understood by another human. Improving code readability ensures that your code is
understandable and maintainable by people other than you. You can take a few steps
to make sure that your code is understandable to other people, such as making sure
that your code is well documented and follows coding standards.
Using features introduced in java 8 can also improve code readability compared
with previous versions. You can reduce the verbosity of your code, making it easier to
understand. Also, you can better show the intent of your code by using method refer-
ences and the streams api.
In this chapter, we describe three simple refactorings that use lambdas, method
references, and streams, which you can apply to your code to improve its readability:
 refactoring anonymous classes to lambda expressions
 refactoring lambda expressions to method references
 refactoring imperative-style data processing to streams

9.1.2 from anonymous classes to lambda expressions
the first simple refactoring you should consider is converting uses of anonymous
classes implementing one single abstract method to lambda expressions. Why? We
hope that in earlier chapters, we convinced you that anonymous classes are verbose
and error-prone. By adopting lambda expressions, you produce code thats more
218 chapter 9 refactoring, testing, and debugging

succinct and readable. As shown in chapter 3, heres an anonymous class for creating
a runnable object and its lambda-expression counterpart:
runnable r1 = new runnable() {
before, using an
public void run(){ anonymous class
system. Out. Println("hello");
}
}; after, using a
runnable r2 = () -> system. Out. Println("hello"); lambda expression

but converting anonymous classes to lambda expressions can be a difficult process in
certain situations.1 first, the meanings of this and super are different for anonymous
classes and lambda expressions. Inside an anonymous class, this refers to the anony-
mous class itself, but inside a lambda, it refers to the enclosing class. Second, anony-
mous classes are allowed to shadow variables from the enclosing class. Lambda
expressions cant (theyll cause a compile error), as shown in the following code:
int a = 10;
runnable r1 = () -> {
int a = 2; compile error
system. Out. Println(a);
};
runnable r2 = new runnable(){
public void run(){ everything
int a = 2; is fine!
System. Out. Println(a);
}
};

finally, converting an anonymous class to a lambda expression can make the resulting
code ambiguous in the context of overloading. Indeed, the type of anonymous class is
explicit at instantiation, but the type of the lambda depends on its context. Heres an
example of how this situation can be problematic. Suppose that youve declared a
functional interface with the same signature as runnable, here called task (as might
occur when you need more-meaningful interface names in your domain model):
interface task {
public void execute();
}
public static void dosomething(runnable r){ r. Run(); }
public static void dosomething(task a){ r. Execute(); }

now you can pass an anonymous class implementing task without a problem:
dosomething(new task() {
public void execute() {
system. Out. Println("danger danger! ! ");
}
});

1
this excellent paper describes the process in more detail: http: //dig. Cs. Illinois. Edu/papers/
lambdarefactoring. Pdf.
Refactoring for improved readability and flexibility 219

but converting this anonymous class to a lambda expression results in an ambiguous
method call, because both runnable and task are valid target types:

dosomething(() -> system. Out. Println("danger danger! ! "));
problem; both dosomething(runnable)
and dosomething(task) match.

You can solve the ambiguity by providing an explicit cast (task):

dosomething((task)() -> system. Out. Println("danger danger! ! "));

dont be turned off by these issues, though; theres good news! Most integrated devel-
opment environments (ides)such as netbeans, eclipse, and intellijsupport this
refactoring and automatically ensure that these gotchas dont arise.

9.1.3 from lambda expressions to method references
lambda expressions are great for short code that needs to be passed around. But con-
sider using method references whenever possible to improve code readability. A
method name states the intent of your code more clearly. In chapter 6, for example,
we showed you the following code to group dishes by caloric levels:
map<caloriclevel, list<dish>> dishesbycaloriclevel =
menu. Stream()
. Collect(
groupingby(dish -> {
if (dish. Getcalories() <= 400) return caloriclevel. Diet;
else if (dish. Getcalories() <= 700) return caloriclevel. Normal;
else return caloriclevel. Fat;
}));

you can extract the lambda expression into a separate method and pass it as an argu-
ment to groupingby. The code becomes more concise, and its intent is more explicit:

map<caloriclevel, list<dish>> dishesbycaloriclevel =
menu. Stream(). Collect(groupingby(dish: : getcaloriclevel));
the lambda expression is
extracted into a method.

You need to add the method getcaloriclevel inside the dish class itself for this code
to work:
public class dish{
...
Public caloriclevel getcaloriclevel() {
if (this. Getcalories() <= 400) return caloriclevel. Diet;
else if (this. Getcalories() <= 700) return caloriclevel. Normal;
else return caloriclevel. Fat;
}
}
220 chapter 9 refactoring, testing, and debugging

in addition, consider using helper static methods such as comparing and maxby when-
ever possible. These methods were designed for use with method references! Indeed,
this code states much more clearly its intent than its counterpart using a lambda
expression, as we showed you in chapter 3:
you need to think about the
inventory. Sort( implementation of comparison.
(apple a1, apple a2) -> a1. Getweight(). Compareto(a2. Getweight()));
inventory. Sort(comparing(apple: : getweight));
reads like the
problem statement

moreover, for many common reduction operations such as sum, maximum, there are
built-in helper methods that can be combined with method references. We showed
you, for example, that by using the collectors api, you can find the maximum or
sum in a clearer way than by using a combination of a lambda expression and a lower-
level reduce operation. Instead of writing

int totalcalories =
menu. Stream(). Map(dish: : getcalories)
. Reduce(0, (c1, c2) -> c1 + c2);

try using alternative built-in collectors, which state the problem statement more
clearly. Here, we use the collector summingint (names go a long way in documenting
your code):

int totalcalories = menu. Stream(). Collect(summingint(dish: : getcalories));

9.1.4 from imperative data processing to streams
ideally, you should try to convert all code that processes a collection with typical
data processing patterns with an iterator to use the streams api instead. Why? The
streams api expresses more clearly the intent of a data processing pipeline. In addi-
tion, streams can be optimized behind the scenes, making use of short-circuiting
and laziness as well as leveraging your multicore architecture, as we explained in
chapter 7.
The following imperative code expresses two patterns (filtering and extracting)
that are mangled together, forcing the programmer to carefully figure out the whole
implementation before figuring out what the code does. In addition, an implementa-
tion that executes in parallel would be a lot more difficult to write. See chapter 7 (par-
ticularly section 7.2) to get an idea of the work involved:

list<string> dishnames = new arraylist<>();
for(dish dish: menu) {
if(dish. Getcalories() > 300){
dishnames. Add(dish. Getname());
}
}
refactoring for improved readability and flexibility 221

the alternative, which uses the streams api, reads more like the problem statement,
and it can be easily parallelized:

menu. Parallelstream()
. Filter(d -> d. Getcalories() > 300)
. Map(dish: : getname)
. Collect(tolist());

unfortunately, converting imperative code to the streams api can be a difficult task,
because you need to think about control-flow statements such as break, continue, and
return and then infer the right stream operations to use. The good news is that some
tools can help you with this task as well. The good news is that some tools (e. G. , lambda-
ficator, https: //ieeexplore. Ieee. Org/document/6606699) can help you with this task
as well.

9.1.5 improving code flexibility
we argued in chapters 2 and 3 that lambda expressions encourage the style of behav-
ior parameterization. You can represent multiple behaviors with different lambdas
that you can then pass around to execute. This style lets you cope with requirement
changes (creating multiple ways of filtering with a predicate or comparing with a
comparator, for example). In the next section, we look at a couple of patterns that you
can apply to your code base to benefit immediately from lambda expressions.
Adopting functional interfaces
first, you cant use lambda expressions without functional interfaces; therefore, you
should start introducing them in your code base. But in which situations should you
introduce them? In this chapter, we discuss two common code patterns that can be
refactored to leverage lambda expressions: conditional deferred execution and exe-
cute around. Also, in the next section, we show you how various object-oriented
design patternssuch as the strategy and template-method design patternscan be
rewritten more concisely with lambda expressions.
Conditional deferred execution
its common to see control-flow statements mangled inside business-logic code. Typi-
cal scenarios include security checks and logging. Consider the following code, which
uses the built-in java logger class:
if (logger. Isloggable(log. Finer)) {
logger. Finer("problem: " + generatediagnostic());
}

whats wrong with it? A couple of things:
 the state of the logger (what level it supports) is exposed in the client code
through the method isloggable.
 Why should you have to query the state of the logger object every time before
you log a message? It clutters your code.
222 chapter 9 refactoring, testing, and debugging

a better alternative is to use the log method, which checks internally to see whether
the logger object is set to the right level before logging the message:

logger. Log(level. Finer, "problem: " + generatediagnostic());

this approach is better because your code isnt cluttered with if checks, and the state
of the logger is no longer exposed. Unfortunately, this code still has an issue: the log-
ging message is always evaluated, even if the logger isnt enabled for the message level
passed as an argument.
Lambda expressions can help. What you need is a way to defer the construction of
the message so that it can be generated only under a given condition (here, when the
logger level is set to finer). It turns out that the java 8 api designers knew about this
problem and introduced an overloaded alternative to log that takes a supplier as an
argument. This alternative log method has the following signature:

public void log(level level, supplier<string> msgsupplier)

now you can call it as follows:

logger. Log(level. Finer, () -> "problem: " + generatediagnostic());

the log method internally executes the lambda passed as an argument only if the
logger is of the right level. The internal implementation of the log method is along
these lines:
public void log(level level, supplier<string> msgsupplier) {
if(logger. Isloggable(level)){
log(level, msgsupplier. Get());
executing
} the lambda
}

whats the takeaway from the story? If you see yourself querying the state of an
object (such as the state of the logger) many times in client code, only to call some
method on this object with arguments (such as to log a message), consider introduc-
ing a new method that calls that method, passed as a lambda or method reference,
only after internally checking the state of the object. Your code will be more read-
able (less cluttered) and better encapsulated, without exposing the state of the
object in client code.
Execute around
in chapter 3, we discussed another pattern that you can adopt: execute around. If
you find yourself surrounding different code with the same preparation and
cleanup phases, you can often pull that code into a lambda. The benefit is that you
can reuse the logic dealing with the preparation and cleanup phases, thus reducing
code duplication.
Heres the code that you saw in chapter 3. It reuses the same logic to open and
close a file but can be parameterized with different lambdas to process the file:
refactoring object-oriented design patterns with lambdas 223

string oneline =
processfile((bufferedreader b) -> b. Readline()); pass a lambda.
String twolines =
processfile((bufferedreader b) -> b. Readline() + b. Readline());
pass a
public static string processfile(bufferedreaderprocessor p) throws different
ioexception { lambda.
Try(bufferedreader br = new bufferedreader(new
filereader("modernjavainaction/chap9/data. Txt"))) {
return p. Process(br);
execute the buffered-
} readerprocessor passed
} as an argument.
Public interface bufferedreaderprocessor {
string process(bufferedreader b) throws ioexception; a functional interface
} for a lambda, which can
throw an ioexception

this code was made possible by introducing the functional interface bufferedreader-
processor, which lets you pass different lambdas to work with a bufferedreader
object.
In this section, youve seen how to apply various recipes to improve the readability
and flexibility of your code. In the next section, you see how lambda expressions can
remove boilerplate code associated with common object-oriented design patterns.

9.2 refactoring object-oriented design patterns
with lambdas
new language features often make existing code patterns or idioms less popular. The
introduction of the for-each loop in java 5, for example, has replaced many uses of
explicit iterators because its less error-prone and more concise. The introduction of
the diamond operator <> in java 7 reduced the use of explicit generics at instance cre-
ation (and slowly pushed java programmers to embrace type inference).
A specific class of patterns is called design patterns.2 design patterns are reusable
blueprints, if you will, for common problems in designing software. They are rather
like how construction engineers have a set of reusable solutions to construct bridges
for specific scenarios (suspension bridge, arch bridge, and so on). The visitor design
pattern, for example, is a common solution for separating an algorithm from a struc-
ture on which it needs to operate. The singleton pattern is a common solution to restrict
the instantiation of a class to one object.
Lambda expressions provide yet another new tool in the programmers toolbox.
They can provide alternative solutions to the problems that the design patterns are
tackling, but often with less work and in a simpler way. Many existing object-oriented
design patterns can be made redundant or written in a more concise way with lambda
expressions.

2
see design patterns: elements of reusable object-oriented software, by erich gamma, richard helm, ralph john-
son, and john vlissides; isbn 978-0201633610, isbn 0-201-63361-2
224 chapter 9 refactoring, testing, and debugging

in this section, we explore five design patterns:
 strategy
 template method
 observer
 chain of responsibility
 factory

we show you how lambda expressions can provide an alternative way to solve the prob-
lem that each design pattern is intended to solve.

9.2.1 strategy
the strategy pattern is a common solution for representing a family of algorithms and
letting you choose among them at runtime. You saw this pattern briefly in chapter 2
when we showed you how to filter an inventory with different predicates (such as
heavy apples or green apples). You can apply this pattern to a multitude of scenarios,
such as validating an input with different criteria, using different ways of parsing, or
formatting an input.
The strategy pattern consists of three parts, as illustrated in figure 9.1:

concretestrategyb
strategy
client
+ execute()
concretestrategya

figure 9.1 the strategy design pattern

 an interface to represent some algorithm (the interface strategy)
 one or more concrete implementations of that interface to represent multiple
algorithms (the concrete classes concretestrategya, concretestrategyb)
 one or more clients that use the strategy objects

suppose that youd like to validate whether a text input is properly formatted for dif-
ferent criteria (consists of only lowercase letters or is numeric, for example). You start
by defining an interface to validate the text (represented as a string):
public interface validationstrategy {
boolean execute(string s);
}

second, you define one or more implementation(s) of that interface:
public class isalllowercase implements validationstrategy {
public boolean execute(string s){
return s. Matches("[a-z]+");
}
}
refactoring object-oriented design patterns with lambdas 225

public class isnumeric implements validationstrategy {
public boolean execute(string s){
return s. Matches("\\d+");
}
}

then you can use these different validation strategies in your program:
public class validator {
private final validationstrategy strategy;
public validator(validationstrategy v) {
this. Strategy = v;
}
public boolean validate(string s) {
return strategy. Execute(s);
}
}
validator numericvalidator = new validator(new isnumeric()); returns false
boolean b1 = numericvalidator. Validate("aaaa");
validator lowercasevalidator = new validator(new isalllowercase ());
boolean b2 = lowercasevalidator. Validate("bbbb");
returns true

using lambda expressions
by now, you should recognize that validationstrategy is a functional interface. In
addition, it has the same function descriptor as predicate<string>. As a result,
instead of declaring new classes to implement different strategies, you can pass more
concise lambda expressions directly:
validator numericvalidator =
new validator((string s) -> s. Matches("[a-z]+"));
boolean b1 = numericvalidator. Validate("aaaa"); passing a
validator lowercasevalidator =
lambda
directly
new validator((string s) -> s. Matches("\\d+"));
boolean b2 = lowercasevalidator. Validate("bbbb");

as you can see, lambda expressions remove the boilerplate code thats inherent to the
strategy design pattern. If you think about it, lambda expressions encapsulate a piece
of code (or strategy), which is what the strategy design pattern was created for, so we
recommend that you use lambda expressions instead for similar problems.

9.2.2 template method
the template method design pattern is a common solution when you need to repre-
sent the outline of an algorithm and have the additional flexibility to change certain
parts of it. Okay, this pattern sounds a bit abstract. In other words, the template
method pattern is useful when you find yourself saying id love to use this algorithm,
but i need to change a few lines so it does what i want.
Heres an example of how this pattern works. Suppose that you need to write a sim-
ple online banking application. Users typically enter a customer id; the application
fetches the customers details from the banks database and does something to make
226 chapter 9 refactoring, testing, and debugging

the customer happy. Different online banking applications for different banking
branches may have different ways of making a customer happy (such as adding a
bonus to his account or sending him less paperwork). You can write the following
abstract class to represent the online banking application:
abstract class onlinebanking {
public void processcustomer(int id){
customer c = database. Getcustomerwithid(id);
makecustomerhappy(c);
}
abstract void makecustomerhappy(customer c);
}

the processcustomer method provides a sketch for the online banking algorithm:
fetch the customer given its id and make the customer happy. Now different branches
can provide different implementations of the makecustomerhappy method by sub-
classing the onlinebanking class.
Using lambda expressions
you can tackle the same problem (creating an outline of an algorithm and letting
implementers plug in some parts) by using your favorite lambdas. The components of
the algorithms you want to plug in can be represented by lambda expressions or
method references.
Here, we introduce a second argument to the processcustomer method of type
consumer<customer> because it matches the signature of the method makecustomer-
happy defined earlier:
public void processcustomer(int id, consumer<customer> makecustomerhappy) {
customer c = database. Getcustomerwithid(id);
makecustomerhappy. Accept(c);
}

now you can plug in different behaviors directly without subclassing the online-
banking class by passing lambda expressions:

new onlinebankinglambda(). Processcustomer(1337, (customer c) ->
system. Out. Println("hello " + c. Getname());

this example shows how lambda expressions can help you remove the boilerplate
inherent to design patterns.

9.2.3 observer
the observer design pattern is a common solution when an object (called the subject)
needs to automatically notify a list of other objects (called observers) when some event
happens (such as a state change). You typically come across this pattern when working
with gui applications. You register a set of observers on a gui component such as a
button. If the button is clicked, the observers are notified and can execute a specific
action. But the observer pattern isnt limited to guis. The observer design pattern is
refactoring object-oriented design patterns with lambdas 227

also suitable in a situation in which several traders (observers) want to react to the
change of price of a stock (subject). Figure 9.2 illustrates the uml diagram of the
observer pattern.

Concreteobserverb
subject observer

+ notifyobserver() + notify()
concreteobservera

figure 9.2 the observer design pattern

now write some code to see how useful the observer pattern is in practice. Youll
design and implement a customized notification system for an application such as
twitter. The concept is simple: several newspaper agencies (the new york times, the
guardian, and le monde) are subscribed to a feed of news tweets and may want to
receive a notification if a tweet contains a particular keyword.
First, you need an observer interface that groups the observers. It has one
method, called notify, that will be called by the subject (feed) when a new tweet is
available:
interface observer {
void notify(string tweet);
}

now you can declare different observers (here, the three newspapers) that produce a
different action for each different keyword contained in a tweet:
class nytimes implements observer {
public void notify(string tweet) {
if(tweet ! = null && tweet. Contains("money")){
system. Out. Println("breaking news in ny! " + tweet);
}
}
}
class guardian implements observer {
public void notify(string tweet) {
if(tweet ! = null && tweet. Contains("queen")){
system. Out. Println("yet more news from london... " + tweet);
}
}
}
class lemonde implements observer {
public void notify(string tweet) {
if(tweet ! = null && tweet. Contains("wine")){
system. Out. Println("today cheese, wine and news! " + tweet);
}
}
}
228 chapter 9 refactoring, testing, and debugging

youre still missing the crucial part: the subject. Define an interface for the subject:
interface subject {
void registerobserver(observer o);
void notifyobservers(string tweet);
}

the subject can register a new observer using the registerobserver method and
notify his observers of a tweet with the notifyobservers method. Now implement the
feed class:

class feed implements subject {
private final list<observer> observers = new arraylist<>();
public void registerobserver(observer o) {
this. Observers. Add(o);
}
public void notifyobservers(string tweet) {
observers. Foreach(o -> o. Notify(tweet));
}
}

this implementation is straightforward: the feed keeps an internal list of observers
that it can notify when a tweet arrives. You can create a demo application to wire up
the subject and observers:
feed f = new feed();
f. Registerobserver(new nytimes());
f. Registerobserver(new guardian());
f. Registerobserver(new lemonde());
f. Notifyobservers("the queen said her favourite book is modern java in action! ");

unsurprisingly, the guardian picks up this tweet.
Using lambda expressions
you may be wondering how to use lambda expressions with the observer design pat-
tern. Notice that the various classes that implement the observer interface all provide
implementation for a single method: notify. Theyre wrapping a piece of behavior to
execute when a tweet arrives. Lambda expressions are designed specifically to remove
that boilerplate. Instead of instantiating three observer objects explicitly, you can pass
a lambda expression directly to represent the behavior to execute:
f. Registerobserver((string tweet) -> {
if(tweet ! = null && tweet. Contains("money")){
system. Out. Println("breaking news in ny! " + tweet);
}
});
f. Registerobserver((string tweet) -> {
if(tweet ! = null && tweet. Contains("queen")){
system. Out. Println("yet more news from london... " + tweet);
}
});
refactoring object-oriented design patterns with lambdas 229

should you use lambda expressions all the time? The answer is no. In the example we
described, lambda expressions work great because the behavior to execute is simple,
so theyre helpful for removing boilerplate code. But the observers may be more com-
plex; they could have state, define several methods, and the like. In those situations,
you should stick with classes.

9.2.4 chain of responsibility
the chain of responsibility pattern is a common solution to create a chain of process-
ing objects (such as a chain of operations). One processing object may do some work
and pass the result to another object, which also does some work and passes it on to
yet another processing object, and so on.
Generally, this pattern is implemented by defining an abstract class representing
a processing object that defines a field to keep track of a successor. When it finishes
its work, the processing object hands over its work to its successor. The code looks
like this:
public abstract class processingobject<t> {
protected processingobject<t> successor;
public void setsuccessor(processingobject<t> successor){
this. Successor = successor;
}
public t handle(t input) {
t r = handlework(input);
if(successor ! = null){
return successor. Handle(r);
}
return r;
}
abstract protected t handlework(t input);
}

figure 9.3 illustrates the chain of responsibility pattern in uml.

Concreteprocessingobject client

successor

processingobject
figure 9.3 the chain of
+ handle()
responsibility design pattern

here, you may recognize the template method design pattern, which we discussed in
section 9.2.2. The handle method provides an outline for dealing with a piece of work.
You can create different kinds of processing objects by subclassing the processing-
object class and by providing an implementation for the handlework method.
230 chapter 9 refactoring, testing, and debugging

heres an example of how to use this pattern. You can create two processing
objects doing some text processing:
public class headertextprocessing extends processingobject<string> {
public string handlework(string text) {
return "from raoul, mario and alan: " + text;
}
}
public class spellcheckerprocessing extends processingobject<string> {
public string handlework(string text) {
return text. Replaceall("labda", "lambda");
oopswe forgot the
} m in lambda!
}

now you can connect two processing objects to construct a chain of operations:

processingobject<string> p1 = new headertextprocessing(); chaining two
processingobject<string> p2 = new spellcheckerprocessing(); processing
p1. Setsuccessor(p2); objects
string result = p1. Handle("aren't labdas really sexy? ! ! ");
system. Out. Println(result);
prints from raoul, mario and alan:
arent lambdas really sexy? ! !
Using lambda expressions
wait a minutethis pattern looks like chaining (that is, composing) functions. We
discussed composing lambda expressions in chapter 3. You can represent the process-
ing objects as an instance of function<string, string>, or (more precisely) a
unaryoperator<string>. To chain them, compose these functions by using the
andthen method:
the first
unaryoperator<string> headerprocessing = processing
(string text) -> "from raoul, mario and alan: " + text; object
unaryoperator<string> spellcheckerprocessing =
the second
(string text) -> text. Replaceall("labda", "lambda");
processing object
function<string, string> pipeline =
headerprocessing. Andthen(spellcheckerprocessing);
string result = pipeline. Apply("aren't labdas really sexy? ! ! ");

compose the two functions,
resulting in a chain of operations.
9.2.5 factory
the factory design pattern lets you create objects without exposing the instantiation
logic to the client. Suppose that youre working for a bank that needs a way of creating
different financial products: loans, bonds, stocks, and so on.
Typically, youd create a factory class with a method thats responsible for the cre-
ation of different objects, as shown here:

public class productfactory {
public static product createproduct(string name) {
switch(name){
refactoring object-oriented design patterns with lambdas 231

case "loan": return new loan();
case "stock": return new stock();
case "bond": return new bond();
default: throw new runtimeexception("no such product " + name);
}
}
}

here, loan, stock, and bond are subtypes of product. The createproduct method
could have additional logic to configure each created product. But the benefit is
that you can create these objects without exposing the constructor and the configu-
ration to the client, which makes the creation of products simpler for the client,
as follows:

product p = productfactory. Createproduct("loan");

using lambda expressions
you saw in chapter 3 that you can refer to constructors the way that you refer to meth-
ods: by using method references. Heres how to refer to the loan constructor:

supplier<product> loansupplier = loan: : new;
loan loan = loansupplier. Get();

using this technique, you could rewrite the preceding code by creating a map that
maps a product name to its constructor:

final static map<string, supplier<product>> map = new hashmap<>();
static {
map. Put("loan", loan: : new);
map. Put("stock", stock: : new);
map. Put("bond", bond: : new);
}

you can use this map to instantiate different products, as you did with the factory
design pattern:
public static product createproduct(string name){
supplier<product> p = map. Get(name);
if(p ! = null) return p. Get();
throw new illegalargumentexception("no such product " + name);
}

this technique is a neat way to use this java 8 feature to achieve the same intent as the
factory pattern. But this technique doesnt scale well if the factory method create-
product needs to take multiple arguments to pass to the product constructors. Youd
have to provide a functional interface other than a simple supplier.
Suppose that you want to refer to constructors for products that take three argu-
ments (two integers and a string); you need to create a special functional interface
232 chapter 9 refactoring, testing, and debugging

trifunction to support such constructors. As a result, the signature of the map becomes
more complex:
public interface trifunction<t, u, v, r> {
r apply(t t, u u, v v);
}
map<string, trifunction<integer, integer, string, product>> map
= new hashmap<>();

youve seen how to write and refactor code by using lambda expressions. In the next
section, you see how to ensure that your new code is correct.

9.3 testing lambdas
youve sprinkled your code with lambda expressions, and it looks nice and concise.
But in most developer jobs, youre paid not for writing nice code, but for writing code
thats correct.
Generally, good software engineering practice involves using unit testing to ensure
that your program behaves as intended. You write test cases, which assert that small
individual parts of your source code are producing the expected results. Consider a
simple point class for a graphical application:
public class point {
private final int x;
private final int y;
private point(int x, int y) {
this. X = x;
this. Y = y;
}
public int getx() { return x; }
public int gety() { return y; }
public point moverightby(int x) {
return new point(this. X + x, this. Y);
}
}

the following unit test checks whether the method moverightby behaves as expected:
@test
public void testmoverightby() throws exception {
point p1 = new point(5, 5);
point p2 = p1. Moverightby(10);
assertequals(15, p2. Getx());
assertequals(5, p2. Gety());
}

9.3.1 testing the behavior of a visible lambda
this code works nicely because the moverightby method is public and, therefore, can
be tested inside the test case. But lambdas dont have names (theyre anonymous
functions, after all), and testing them in your code is tricky because you cant refer to
them by name.
Testing lambdas 233

sometimes, you have access to a lambda via a field so that you can reuse it, and
youd like to test the logic encapsulated in that lambda. What can you do? You could
test the lambda as you do when calling methods. Suppose that you add a static field
comparebyxandtheny in the point class that gives you access to a comparator object
generated from method references:
public class point {
public final static comparator<point> comparebyxandtheny =
comparing(point: : getx). Thencomparing(point: : gety);
...
}

remember that lambda expressions generate an instance of a functional interface. As
a result, you can test the behavior of that instance. Here, you can call the compare
method on the comparator object comparebyxandtheny with different arguments to
test whether its behavior is as intended:
@test
public void testcomparingtwopoints() throws exception {
point p1 = new point(10, 15);
point p2 = new point(10, 20);
int result = point. Comparebyxandtheny. Compare(p1 , p2);
asserttrue(result < 0);
}

9.3.2 focusing on the behavior of the method using a lambda
but the purpose of lambdas is to encapsulate a one-off piece of behavior to be used by
another method. In that case, you shouldnt make lambda expressions available pub-
licly; theyre only implementation details. Instead, we argue that you should test the
behavior of the method that uses a lambda expression. Consider the moveallpoints-
rightby method shown here:

public static list<point> moveallpointsrightby(list<point> points, int x) {
return points. Stream()
. Map(p -> new point(p. Getx() + x, p. Gety()))
. Collect(tolist());
}

theres no point (pun intended) in testing the lambda p -> new point(p. Getx() + x,
p. Gety()); its only an implementation detail for the moveallpointsrightby method.
Instead, you should focus on testing the behavior of the moveallpointsrightby
method:
@test
public void testmoveallpointsrightby() throws exception {
list<point> points =
arrays. Aslist(new point(5, 5), new point(10, 5));
list<point> expectedpoints =
arrays. Aslist(new point(15, 5), new point(20, 5));
234 chapter 9 refactoring, testing, and debugging

list<point> newpoints = point. Moveallpointsrightby(points, 10);
assertequals(expectedpoints, newpoints);
}

note that in the unit test, its important for the point class to implement the equals
method appropriately; otherwise, it relies on the default implementation from object.

9.3.3 pulling complex lambdas into separate methods
perhaps you come across a really complicated lambda expression that contains a lot of
logic (such as a technical pricing algorithm with corner cases). What do you do,
because you cant refer to the lambda expression inside your test? One strategy is to
convert the lambda expression to a method reference (which involves declaring a new
regular method), as we explained in section 9.1.3. Then you can test the behavior of
the new method as you would that of any regular method.

9.3.4 testing high-order functions
methods that take a function as an argument or return another function (so-called
higher-order functions, explained in chapter 19) are a little harder to deal with. One
thing you can do if a method takes a lambda as an argument is test its behavior with
different lambdas. You can test the filter method that you created in chapter 2 with
different predicates:
@test
public void testfilter() throws exception {
list<integer> numbers = arrays. Aslist(1, 2, 3, 4);
list<integer> even = filter(numbers, i -> i % 2 == 0);
list<integer> smallerthanthree = filter(numbers, i -> i < 3);
assertequals(arrays. Aslist(2, 4), even);
assertequals(arrays. Aslist(1, 2), smallerthanthree);
}

what if the method that needs to be tested returns another function? You can test the
behavior of that function by treating it as an instance of a functional interface, as we
showed you earlier with a comparator.
Unfortunately, not everything works the first time, and your tests may report some
errors related to your use of lambda expressions. So, in the next section we turn to
debugging.

9.4 debugging
a developers arsenal has two main old-school weapons for debugging problematic
code:
 examining the stack trace
 logging

lambda expressions and streams can bring new challenges to your typical debugging
routine. We explore both in this section.
Debugging 235

9.4.1 examining the stack trace
when your program has stopped (because an exception was thrown, for example),
the first thing you need to know is where the program stopped and how it got there.
Stack frames are useful for this purpose. Each time your program performs a method
call, information about the call is generated, including the location of the call in your
program, the arguments of the call, and the local variables of the method being
called. This information is stored on a stack frame.
When your program fails, you get a stack trace, which is a summary of how your pro-
gram got to that failure, stack frame by stack frame. In other words, you get a valuable
list of method calls up to when the failure appeared. This list helps you understand
how the problem occurred.
Using lambda expressions
unfortunately, due to the fact that lambda expressions dont have names, stack traces
can be slightly puzzling. Consider the following simple code, which is made to fail on
purpose:
import java. Util. *;
public class debugging{
public static void main(string[] args) {
list<point> points = arrays. Aslist(new point(12, 2), null);
points. Stream(). Map(p -> p. Getx()). Foreach(system. Out: : println);
}
}

running this code produces a stack trace along the lines of the following (depending
on your javac version; you may not have the same stack trace):

exception in thread "main" java. Lang. Nullpointerexception what does $0 in
at debugging. Lambda$main$0(debugging. Java:6) this line mean?
At debugging$$lambda$5/284720968. Apply(unknown source)
at java. Util. Stream. Referencepipeline$3$1. Accept(referencepipeline
. Java:193)
at java. Util. Spliterators$arrayspliterator. Foreachremaining(spliterators
. Java:948)
...

Yuck! Whats going on? The program fails, of course, because the second element of
the list of points is null. You try to process a null reference. Because the error
occurs in a stream pipeline, the whole sequence of method calls that make a stream
pipeline work is exposed to you. But notice that the stack trace produces the follow-
ing cryptic lines:
at debugging. Lambda$main$0(debugging. Java:6)
at debugging$$lambda$5/284720968. Apply(unknown source)

these lines mean that the error occurred inside a lambda expression. Unfortunately,
because lambda expressions dont have names, the compiler has to make up a name
236 chapter 9 refactoring, testing, and debugging

to refer to them. In this case, the name is lambda$main$0, which isnt intuitive and can
be problematic if you have large classes containing several lambda expressions.
Even if you use method references, its still possible that the stack wont show you
the name of the method you used. Changing the preceding lambda p -> p. Getx() to
the method reference point: : getx also results in a problematic stack trace:

points. Stream(). Map(point: : getx). Foreach(system. Out: : println); what does this
exception in thread "main" java. Lang. Nullpointerexception line mean?
At debugging$$lambda$5/284720968. Apply(unknown source)
at java. Util. Stream. Referencepipeline$3$1. Accept(referencepipeline
. Java:193)
...

Note that if a method reference refers to a method declared in the same class where
its used, it appears in the stack trace. In the following example:
import java. Util. *;
public class debugging{
public static void main(string[] args) {
list<integer> numbers = arrays. Aslist(1, 2, 3);
numbers. Stream(). Map(debugging: : dividebyzero). Foreach(system
. Out: : println);
}
public static int dividebyzero(int n){
return n / 0;
}
}

the dividebyzero method is reported correctly in the stack trace:
exception in thread "main" java. Lang. Arithmeticexception: / by zero
at debugging. Dividebyzero(debugging. Java:10)
at debugging$$lambda$1/999966131. Apply(unknown source)
at java. Util. Stream. Referencepipeline$3$1. Accept(referencepipeline
. Java:193)
...
Dividebyzero appears
in the stack trace.

In general, keep in mind that stack traces involving lambda expressions may be more
difficult to understand. This area is one in which the compiler can be improved in a
future version of java.

9.4.2 logging information
suppose that youre trying to debug a pipeline of operations in a stream. What can
you do? You could use foreach to print or log the result of a stream as follows:
list<integer> numbers = arrays. Aslist(2, 3, 4, 5);
numbers. Stream()
. Map(x -> x + 17)
. Filter(x -> x % 2 == 0)
. Limit(3)
. Foreach(system. Out: : println);
debugging 237

this code produces the following output:
20
22

unfortunately, after you call foreach, the whole stream is consumed. It would be use-
ful to understand what each operation (map, filter, limit) produces in the pipeline
of a stream.
The stream operation peek can help. The purpose of peek is to execute an action
on each element of a stream as its consumed. It doesnt consume the whole stream
the way foreach does, however; it forwards the element on which it performed an
action to the next operation in the pipeline. Figure 9.4 illustrates the peek operation.

Peek peek peek peek

numbers map filter limit collect

figure 9.4 examining values flowing in a stream pipeline with peek

in the following code, you use peek to print the intermediate values before and after
each operation in the stream pipeline:

print the current element
list<integer> result = consumed from the source.
Numbers. Stream() print the
. Peek(x -> system. Out. Println("from stream: " + x)) result of
. Map(x -> x + 17) the map
. Peek(x -> system. Out. Println("after map: " + x)) operation.
. Filter(x -> x % 2 == 0)
. Peek(x -> system. Out. Println("after filter: " + x))
. Limit(3)
. Peek(x -> system. Out. Println("after limit: " + x))
. Collect(tolist());
print the number
print the number selected selected after the
after the limit operation. Filter operation.

This code produces useful output at each step of the pipeline:
from stream: 2
after map: 19
from stream: 3
after map: 20
after filter: 20
after limit: 20
from stream: 4
after map: 21
from stream: 5
238 chapter 9 refactoring, testing, and debugging

after map: 22
after filter: 22
after limit: 22

summary
 lambda expressions can make your code more readable and flexible.
 Consider converting anonymous classes to lambda expressions, but be wary of
subtle semantic differences such as the meaning of the keyword this and shad-
owing of variables.
 Method references can make your code more readable compared with lambda
expressions.
 Consider converting iterative collection processing to use the streams api.
 Lambda expressions can remove boilerplate code associated with several object-
oriented design patterns, such as strategy, template method, observer, chain of
responsibility, and factory.
 Lambda expressions can be unit-tested, but in general, you should focus on test-
ing the behavior of the methods in which the lambda expressions appear.
 Consider extracting complex lambda expressions into regular methods.
 Lambda expressions can make stack traces less readable.
 The peek method of a stream is useful for logging intermediate values as they
flow past certain points of a stream pipeline.
Domain-specific
languages using lambdas

this chapter covers
 what domain-specific languages (dsls) and their
forms are
 the pros and cons of adding a dsl to your api
 the alternatives available on the jvm to a plain
java-based dsl
 learning from the dsls present in modern java
interfaces and classes
 patterns and techniques to implement effective
java-based dsls
 how commonly used java libraries and tools use
these patterns

developers often forget that a programming language is first of all a language. The
main purpose of any language is to convey a message in the clearest, most under-
standable way. Perhaps the most important characteristic of well-written software is
clear communication of its intentionsor, as famous computer scientist harold
abelson stated, programs must be written for people to read and only incidentally
for machines to execute.

239
240 chapter 10 domain-specific languages using lambdas

readability and understandability are even more important in the parts of the soft-
ware that are intended to model the core business of your application. Writing code
that can be shared and understood by both the development team and the domain
experts is helpful for productivity. Domain experts can be involved in the software
development process and verify the correctness of the software from a business point
of view. As a result, bugs and misunderstandings can be caught early on.
To achieve this result, its common to express the applications business logic
through a domain-specific language (dsl). A dsl is a small, usually non-general-
purpose programming language explicitly tailored for a specific domain. The dsl
uses the terminology characteristic of that domain. You may be familiar with maven
and ant, for example. You can see them as dsls for expressing build processes. Youre
also familiar with html, which is a language tailored to define the structure of a web
page. Historically, due to its rigidity and excessive verbosity, java has never been popu-
lar for implementing a compact dsl thats also suitable to be read by non-technical
people. Now that java supports lambda expressions, however, you have new tools in
your toolbox! In fact, you learned in chapter 3 that lambda expressions help reduce
code verbosity and improve the signal/noise ratio of your programs.
Think about a database implemented in java. Deep down in the database, theres
likely to be lots of elaborate code determining where on disk to store a given record,
constructing indexes for tables, and dealing with concurrent transactions. This data-
base is likely to be programmed by relatively expert programmers. Suppose that
now you want to program a query similar to those we explored in chapters 4 and 5:
find all menu entries on a given menu that have fewer than 400 calories.
Historically, such expert programmers might have quickly written low-level code in
this style and thought that the task was easy:

while (block ! = null) {
read(block, buffer)
for (every record in buffer) {
if (record. Calorie < 400) {
system. Out. Println (record. Name);
}
}
block = buffer. Next();
}

this solution has two main problems: its hard for a less-experienced programmer to
create (it may need subtle details of locking, i/o, or disc allocation), and more
important, it deals in system-level concepts, not application-level concepts.
A new-joining user-facing programmer might say, why cant you provide me an
sql interface so i can write select name from menu where calorie < 400, where menu
holds the restaurant menu expressed as an sql table? Now i can program far more
effectively than all this system-level nonsense! Its hard to argue with this statement!
In essence the programmer has asked for a dsl to interact with the database instead
a specific language for your domain 241

of writing pure java code. Technically, this type of dsl is called external because it
expects the database to have an api that can parse and evaluate sql expressions writ-
ten in text. You learn more about the distinction between external and internal dsl
later in this chapter.
But if you think back to chapters 4 and 5, you notice that this code could also be
written more concisely in java using the stream api , such as the following:

menu. Stream()
. Filter(d -> d. Getcalories() < 400)
. Map(dish: : getname)
. Foreach(system. Out: : println)

this use of chaining methods, which is so characteristic of the stream api, is often
called fluent style in that its easy to understand quickly, in contrast to complex control
flow in java loops.
This style effectively captures a dsl. In this case, this dsl isnt external, but inter-
nal. In an internal dsl, the application-level primitives are exposed as java methods to
use on one or more class types that represent the database, in contrast to the non-java
syntax for primitives in an external dsl, such as select from in the sql discus-
sion above.
In essence, designing a dsl consists of deciding what operations the application-
level programmer needs to manipulate (carefully avoiding any unnecessary pollution
caused by system-level concepts) and providing these operations to the programmer.
For an internal dsl, this process means exposing appropriate classes and methods
so that code can be written fluently. An external dsl takes more effort; you must not
only design the dsl syntax, but also implement a parser and evaluator for the dsl. If
you get the design right, however, perhaps lower-skilled programmers can write code
quickly and effectively (thus making the money that keeps your company in business)
without having to program directly within your beautiful (but hard for non-experts to
understand) system-level code!
In this chapter, you learn what a dsl is through several examples and use cases;
you learn when you should consider implementing one and what the benefits are.
Then you explore some of the small dsls that were introduced in the java 8 api. You
also learn how you could employ the same patterns to create your own dsls. Finally,
you investigate how some widely used java libraries and frameworks have adopted
these techniques to offer their functionalities through a set of dsls, making their
apis more accessible and easy to use.

10.1 a specific language for your domain
a dsl is a custom-built language designed to solve a problem for a specific business
domain. You may be developing a software application for accounting, for example.
Your business domain includes concepts such as bank statements and operations such
as reconciling. You could create a custom dsl to express problems in that domain. In
242 chapter 10 domain-specific languages using lambdas

java, you need to come up with a set of classes and methods to represent that domain.
In a way, you can see the dsl as an api created to interface with a specific business
domain.
A dsl isnt a general-purpose programming language; it restricts the operations
and vocabulary available to a specific domain, which means that you have less to worry
about and can invest more attention in solving the business problem at hand. Your
dsl should allow its users to deal only with the complexities of that domain. Other
lower-level implementation details should be hidden  just like making lower-level
implementation-detail methods of a class private. This results in a user-friendly dsl.
What isnt a dsl? A dsl isnt plain english. Its also not a language that lets
domain experts implement low-level business logic. Two reasons should drive you
toward the development of a dsl:
 communication is king. Your code should clearly communicate its intentions
and be understandable even by a non-programmer. This way, this person can
contribute to validating whether the code matches the business requirements.
 Code is written once but read many times. Readability is vital for maintainability. In
other words, you should always code in a way that your colleagues thank you for
rather than hate you for!
A well-designed dsl offers many benefits. Nonetheless, developing and using a bespoke
dsl has pros and cons. In section 10.1.1, we explore the pros and cons in more detail so
that you can decide when a dsl is appropriate (or not) for a particular scenario.

10.1.1 pros and cons of dsls
dsls, like other technologies and solutions in software development, arent silver bul-
lets. Using a dsl to work with your domain can be both an asset and a liability. A dsl
can be an asset because it raises the level of abstraction with which you can clarify the
business intention of the code and makes the code more readable. But it can also be a
liability because the implementation of the dsl is code in its own right that needs to
be tested and maintained. For this reason, its useful to investigate the benefits and
costs of dsls so that you can evaluate whether adding one to your project will result in
a positive return of investment.
Dsls offer the following benefits:
 concisenessan api that conveniently encapsulates the business logic allows
you to avoid repetition, resulting in code thats less verbose.
 Readabilityusing words that belong to the vocabulary of the domain makes
the code understandable even by domain non-experts. Consequently, code and
domain knowledge can be shared across a wider range of members of the orga-
nization.
 Maintainabilitycode written against a well-designed dsl is easier to maintain
and modify. Maintainability is especially important for business-related code,
which is the part of the application that may change most frequently.
A specific language for your domain 243

 higher level of abstractionthe operations available in a dsl work at the same
level of abstraction as the domain, thus hiding the details that arent strictly
related to the domains problems.
 Focushaving a language designed for the sole purpose of expressing the rules
of the business domain helps programmers stay focused on that specific part of
the code. The result is increased productivity.
 Separation of concernsexpressing the business logic in a dedicated language
makes it easier to keep the business-related code isolated from the infrastruc-
tural part of the application. The result is code thats easier to maintain.
Conversely, introducing a dsl into your code base can have a few disadvantages:
 difficulty of dsl designits hard to capture domain knowledge in a concise lim-
ited language.
 Development costadding a dsl to your code base is a long-term investment
with a high up-front cost, which could delay your project in its early stages. In
addition, the maintenance of the dsl and its evolution add further engineer-
ing overhead.
 Additional indirection layer a dsl wraps your domain model in an additional
layer that has to be as thin as possible to avoid incurring performance
problems.
 Another language to learn nowadays, developers are used to employing multi-
ple languages. Adding a dsl to your project, however, implicitly implies that
you and your team have one more language to learn. Worse, if you decide to
have multiple dsls covering different areas of your business domain, com-
bining them in a seamless way could be hard, because dsls tend to evolve
independently.
 Hosting-language limitationssome general-purpose programming languages
(java is one of them) are known for being verbose and having rigid syntax.
These languages make it difficult to design a user-friendly dsl. In fact, dsls
developed on top of a verbose programming language are constrained by the
cumbersome syntax and may not be nice to read. The introduction of lambda
expression in java 8 offers a powerful new tool to mitigate this problem.
Given these lists of positive and negative arguments, deciding whether to develop a
dsl for your project isnt easy. Moreover, you have alternatives to java for implement-
ing your own dsl. Before investigating which patterns and strategies you could
employ to develop a readable easy-to-use dsl in java 8 and beyond, we quickly
explore these alternatives and describe the circumstances under which they could be
appropriate solutions.
244 chapter 10 domain-specific languages using lambdas

10.1.2 different dsl solutions available on the jvm
in this section, you learn the categories of dsls. You also learn that you have many
choices besides java for implementing dsls. In later sections, we focus on how to
implement dsls by using java features.
The most common way to categorize dsls, introduced by martin fowler, is to dis-
tinguish between internal and external dsls. Internal dsls (also known as embed-
ded dsls) are implemented on top of the existing hosting language (which could be
plain java code), whereas external dsls are called stand-alone because theyre devel-
oped from scratch with a syntax thats independent of the hosting language.
Moreover, the jvm gives you a third possibility that falls between an internal and
an external dsl: another general-purpose programming language that also runs on
the jvm but is more flexible and expressive than java, such as scala or groovy. We
refer to this third alternative as a polyglot dsl.
In the following sections, we look at these three types of dsls in order.
Internal dsl
because this book is about java, when we speak about an internal dsl, we clearly
mean a dsl written in java. Historically, java hasnt been considered to be a dsl-
friendly language because its cumbersome, inflexible syntax makes it difficult to pro-
duce an easy-to-read, concise, expressive dsl. This issue has been largely mitigated by
the introduction of lambda expressions. As you saw in chapter 3, lambdas are useful
for using behavior parameterization in a concise manner. In fact, using lambdas
extensively results in a dsl with a more acceptable signal/noise ratio by reducing the
verbosity that you get with anonymous inner classes. To demonstrate the signal/noise
ratio, try to print a list of strings with java 7 syntax, but use java 8s new foreach
method:
list<string> numbers = arrays. Aslist("one", "two", "three");
numbers. Foreach( new consumer<string>() {
@override
public void accept( string s ) {
system. Out. Println(s);
}
} );

in this snippet, the part that is bold is carrying the signal of the code. All the remain-
ing code is syntactic noise that provides no additional benefit and (even better) is no
longer necessary in java 8. The anonymous inner class can be replaced by the lambda
expression

numbers. Foreach(s -> system. Out. Println(s));

or even more concisely by a method reference:

numbers. Foreach(system. Out: : println);
a specific language for your domain 245

you may be happy to build your dsl with java when you expect users to be somewhat
technically minded. If the java syntax isnt an issue, choosing to develop your dsl in
plain java has many advantages:
 the effort of learning the patterns and techniques necessary to implement a
good java dsl is modest compared with the effort required to learn a new pro-
gramming language and the tools normally used to develop an external dsl.
 Your dsl is written in plain java, so its compiled with the rest of your code.
Theres no additional building cost caused by the integration of a second lan-
guage compiler or of the tool employed to generate the external dsl.
 Your development team wont need to get familiar with a different language or
with a potentially unfamiliar and complex external tool.
 The users of your dsl will have all the features normally provided by your
favorite java ide, such as autocompletion and refactoring facilities. Modern
ides are improving their support for other popular jvm languages, but still
dont have support comparable to what they offer java developers.
 If you need to implement more than one dsl to cover various parts of your
domain or multiple domains, you wont have any problem composing them if
theyre written in plain java.

Another possibility is combining dsls that use the same java bytecode by combining
jvm-based programming languages. We call these dsls polyglot and describe them in
the next section.
Polyglot dsl
nowadays, probably more than 100 languages run on the jvm. Some of these lan-
guages, such as scala and groovy, are quite popular, and it isnt difficult to find devel-
opers who are skilled in them. Other languages, including jruby and jython, are ports
of other well-known programming languages to the jvm. Finally, other emerging lan-
guages, such as kotlin and ceylon, are gaining traction mostly because they claim to
have features comparable with those of scala, but with lower intrinsic complexity and
a gentle learning curve. All these languages are younger than java and have been
designed with less-constrained, less-verbose syntax. This characteristic is important
because it helps implement a dsl that has less inherent verbosity due to the program-
ming language in which its embedded.
Scala in particular has several features, such as currying and implicit conversion,
that are convenient in developing a dsl. You get an overview of scala and how it com-
pares to java in chapter 20. For now, we want to give you a feeling for what you can do
with these features by giving you a small example.
Suppose that you want to build a utility function that repeats the execution of
another function, f, a given number of times. As a first attempt, you could end up
with the following recursive implementation in scala. (dont worry about the syntax;
the overall idea is whats important. )
246 chapter 10 domain-specific languages using lambdas

def times(i: int, f: => unit): unit = {
execute the
f f function.
If (i > 1) times(i - 1, f)
} if the counter i is positive,
decrement it and recursively
invoke the times function.

Note that in scala, invoking this function with large values of i wont cause a stack over-
flow, as would happen in java, because scala has the tail call optimization, which means
that the recursive invocation to the times function wont be added to the stack. You
learn more about this topic in chapters 18 and 19. You can use this function to execute
another function repeatedly (one that prints "hello world" three times) as follows:

times(3, println("hello world"))

if you curry the times function, or put its arguments in two groups (we cover currying
in detail in chapter 19),
def times(i: int)(f: => unit): unit = {
f
if (i > 1 times(i - 1)(f)
}

you can achieve the same result by passing the function to be executed multiple times
in curly braces:
times(3) {
println("hello world")
}

finally, in scala you can define an implicit conversion from an int to an anonymous
class by having only one function that in turn has as argument the function to be
repeated. Again, dont worry about the syntax and details. The objective of this exam-
ple is to give you an idea of whats possible beyond java.
Defines an implicit conversion from
implicit def inttotimes(i: int) = new { an int to an anonymous class
def times(f: => unit): unit = {
def times(i: int, f: => unit): unit = { the class has only a times
f function accepting another
if (i > 1) times(i - 1, f)
function f as argument.
} a second times function takes
times(i, f) two arguments and is defined
invokes the inner
} times function in the scope of the first one.
}

in this way the user of your small scala-embedded dsl can execute a function that
prints "hello world" three times as follows:
3 times {
println("hello world")
}
a specific language for your domain 247

as you can see, the result has no syntactic noise, and its easily understandable even by
a non-developer. Here, the number 3 is automatically converted by the compiler in an
instance of a class that stores the number in its i field. Then the times function is
invoked with dotless notation, taking as an argument the function to be repeated.
Obtaining a similar result in java is impossible, so the advantages of using a more
dsl-friendly language are obvious. This choice also has some clear inconveniences,
however:
 you have to learn a new programming language or have somebody on your
team whos already skilled in it. Because developing a nice dsl in these lan-
guages generally requires the use of relatively advanced features, superficial
knowledge of the new language normally isnt enough.
 You need to complicate your build process a bit by integrating multiple compil-
ers to build the source written with two or more languages.
 Finally, although the majority of languages running on the jvm claim to be 100
percent java-compatible, making them interoperate with java often requires
awkward tricks and compromises. Also, this interoperation sometimes causes a
performance loss. Scala and java collections arent compatible, for example, so
when a scala collection has to be passed to a java function or vice versa, the
original collection has to be converted to one that belongs to the native api of
the target language.
External dsl
the third option for adding a dsl to your project is implementing an external one.
In this case, you have to design a new language from the ground up, with its own syn-
tax and semantics. You also need to set up a separate infrastructure to parse the new
language, analyze the output of the parser, and generate the code to execute your
external dsl. This is a lot of work! The skills required to perform these tasks are nei-
ther common nor easy to acquire. If you do want to go down this road, antlr is a
parser generator thats commonly used to help and that goes hand in hand with java.
Moreover, even designing a coherent programming language from scratch isnt a
trivial task. Another common problem is that its easy for an external dsl to grow out
of control and to cover areas and purposes for which it wasnt designed.
The biggest advantage in developing an external dsl is the practically unlimited
degree of flexibility that it provides. Its possible for you to design a language that per-
fectly fits the needs and peculiarities of your domain. If you do a good job, the result is
an extremely readable language specifically tailored to describe and solve the prob-
lems of your business. The other positive outcome is the clear separation between the
infrastructural code developed in java and the business code written with the external
dsl. This separation is a double-edged sword, however, because it also creates an arti-
ficial layer between the dsl and the host language.
In the remainder of this chapter, you learn about patterns and techniques that can
help you develop effective modern-java-based internal dsls. You start by exploring
248 chapter 10 domain-specific languages using lambdas

how these ideas have been used in the design of the native java api, especially the api
additions in java 8 and beyond.

10.2 small dsls in modern java apis
the first apis to take advantage of the new functional capabilities of java are the
native java apis themselves. Before java 8, the native java api already had a few inter-
faces with a single abstract method, but as you saw in section 10.1, their use required
the implementation of an anonymous inner class with a bulky syntax. The addition of
lambdas and (maybe even more important from a dsl point of view) method refer-
ences changed the rules of the game, making functional interfaces a cornerstone of
java api design.
The comparator interface in java 8 has been updated with new methods. You learn
in chapter 13 that an interface can include both static method and default methods.
For now, the comparator interface serves as a good example of how lambdas improve
the reusability and composability of methods in native java api.
Suppose that you have a list of objects representing people (persons), and you
want to sort these objects based on the peoples ages. Before lambdas, you had to
implement the comparator interface by using an inner class:
collections. Sort(persons, new comparator<person>() {
public int compare(person p1, person p2) {
return p1. Getage() - p2. Getage();
}
});

as youve seen in many other examples in this book, now you can replace the inner
class with a more compact lambda expression:

collections. Sort(people, (p1, p2) -> p1. Getage() - p2. Getage());

this technique greatly increases the signal/noise ratio of your code. Java, however,
also has a set of static utility methods that let you create comparator objects in a more
readable manner. These static methods are included in the comparator interface. By
statically importing the comparator. Comparing method, you can rewrite the preced-
ing sorting example as follows:

collections. Sort(persons, comparing(p -> p. Getage()));

even better, you can replace the lambda with a method reference:

collections. Sort(persons, comparing(person: : getage));

the benefit of this approach can be pushed even further. If you want to sort the peo-
ple by age, but in reverse order, you can exploit the instance method reverse (also
added in java 8):

collections. Sort(persons, comparing(person: : getage). Reverse());
small dsls in modern java apis 249

moreover, if you want the people of the same age to be sorted alphabetically, you can
compose that comparator with one that performs the comparison on the names:

collections. Sort(persons, comparing(person: : getage)
. Thencomparing(person: : getname));

finally, you could use the new sort method added on the list interface to tidy
things further:

persons. Sort(comparing(person: : getage)
. Thencomparing(person: : getname));

this small api is a minimal dsl for the domain of collection sorting. Despite its lim-
ited scope, this dsl already shows you how a well-designed use of lambdas and
method reference can improve the readability, reusability, and composability of
your code.
In the next section, we explore a richer and more widely used java 8 class in which
the readability improvement is even more evident: the stream api.

10.2.1 the stream api seen as a dsl to manipulate collections
the stream interface is a great example of a small internal dsl introduced into the
native java api. In fact, a stream can be seen as a compact but powerful dsl that fil-
ters, sorts, transforms, groups, and manipulates the items of a collection. Suppose that
youre required to read a log file and collect the first 40 lines, starting with the word
error in a list<string>. You could perform this task in an imperative style, as
shown in the following listing.

Listing 10.1 reading the error lines in a log file in imperative style

list<string> errors = new arraylist<>();
int errorcount = 0;
bufferedreader bufferedreader
= new bufferedreader(new filereader(filename));
string line = bufferedreader. Readline();
while (errorcount < 40 && line ! = null) {
if (line. Startswith("error")) {
errors. Add(line);
errorcount++;
}
line = bufferedreader. Readline();
}

here, we omitted the error-handling part of the code for brevity. Despite this fact, the
code is excessively verbose, and its intention isnt immediately evident. The other
aspect that harms both readability and maintainability is the lack of a clear separation
of concerns. In fact, the code with the same responsibility is scattered across multiple
250 chapter 10 domain-specific languages using lambdas

statements. The code used to read the file line by line, for example, is located in
three places:
 where the filereader is created
 the second condition of the while loop, which checks whether the file has
terminated
 the end of the while loop itself that reads the next line in the file

similarly, the code that limits the number of lines collected in the list to the first 40
results is scattered across three statements:
 the one initializing the variable errorcount
 the first condition of the while loop
 the statement incrementing the counter when a line starting with "error" is
found in the log
achieving the same result in a more functional style through the stream interface is
much easier and results in far more compact code, as shown in listing 10.2.

Listing 10.2 reading the error lines in a log file in functional style

opens the file and creates a stream of strings
where each string corresponds to a line in the file.
List<string> errors = files. Lines(paths. Get(filename))
. Filter(line -> line. Startswith("error"))
. Limit(40)
limits the result to filters the line
the first 40 lines. . Collect(tolist());
starting with
"error".
Collects the resulting
strings in a list.
Files. Lines is a static utility method that returns a stream<string> where each
string represents a line in the file to be parsed. That part of the code is the only part
that has to read the file line by line. In the same way, the statement limit(40) is
enough to limit the number of collected error lines to the first 40. Can you imagine
something more obviously readable?
The fluent style of the stream api is another interesting aspect thats typical of a
well-designed dsl. All intermediate operations are lazy and return another stream
allowing a sequence of operations to be pipelined. The terminal operation is eager
and triggers the computation of the result of the whole pipeline.
Its time to investigate the apis of another small dsl designed to be used in con-
junction with the collect method of the stream interface: the collectors api.

10.2.2 collectors as a dsl to aggregate data
you saw that the stream interface can be viewed as a dsl that manipulates lists of
data. Similarly, the collector interface can be viewed as a dsl that performs aggrega-
tion on data. In chapter 6, we explored the collector interface and explained how to
use it to collect, to group, and to partition the items in a stream. We also investigated
small dsls in modern java apis 251

the static factory methods provided by the collectors class to conveniently create dif-
ferent flavors of collector objects and combine them. Its time to review how these
methods are designed from a dsl point of view. In particular, as the methods in the
comparator interface can be combined to support multifield sorting, collectors can
be combined to achieve multilevel grouping. You can group a list of cars, for example,
first by their brand and then by their color as follows:

map<string, map<color, list<car>>> carsbybrandandcolor =
cars. Stream(). Collect(groupingby(car: : getbrand,
groupingby(car: : getcolor)));

what do you notice here compared with what you did to concatenate two comparators?
You defined the multifield comparator by composing two comparators in a fluent way,

comparator<person> comparator =
comparing(person: : getage). Thencomparing(person: : getname);

whereas the collectors api allows you to create a multilevel collector by nesting
the collectors:

collector<? Super car, ? , map<brand, map<color, list<car>>>>
cargroupingcollector =
groupingby(car: : getbrand, groupingby(car: : getcolor));

normally, the fluent style is considered to be more readable than the nesting style,
especially when the composition involves three or more components. Is this differ-
ence in style a curiosity? In fact, it reflects a deliberate design choice caused by the fact
that the innermost collector has to be evaluated first, but logically, its the last group-
ing to be performed. In this case, nesting the collector creations with several static
methods instead of fluently concatenating them allows the innermost grouping to be
evaluated first but makes it appear to be the last one in the code.
It would be easier (except for the use of generics in the definitions) to implement
a groupingbuilder that delegates to the groupingby factory method but allows multi-
ple grouping operations to be composed fluently. This next listing shows how.

Listing 10.3 a fluent grouping collectors builder

import static java. Util. Stream. Collectors. Groupingby;

public class groupingbuilder<t, d, k> {
private final collector<? Super t, ? , map<k, d>> collector;

private groupingbuilder(collector<? Super t, ? , map<k, d>> collector) {
this. Collector = collector;
}

public collector<? Super t, ? , map<k, d>> get() {
return collector;
}
252 chapter 10 domain-specific languages using lambdas

public <j> groupingbuilder<t, map<k, d>, j>
after(function<? Super t, ? Extends j> classifier) {
return new groupingbuilder<>(groupingby(classifier, collector));
}

public static <t, d, k> groupingbuilder<t, list<t>, k>
groupon(function<? Super t, ? Extends k> classifier) {
return new groupingbuilder<>(groupingby(classifier));
}
}

whats the problem with this fluent builder? Trying to use it makes the problem evident:
collector<? Super car, ? , map<brand, map<color, list<car>>>>
cargroupingcollector =
groupon(car: : getcolor). After(car: : getbrand). Get()

as you can see, the use of this utility class is counterintuitive because the grouping
functions have to be written in reverse order relative to the corresponding nested
grouping level. If you try to refactor this fluent builder to fix the ordering issue, you
realize that unfortunately, the java type system wont allow you to do this.
By looking more closely at the native java api and the reasons behind its design
decisions, youve started to learn a few patterns and useful tricks for implementing
readable dsls. In the next section, you continue to investigate techniques for devel-
oping effective dsls.

10.3 patterns and techniques to create dsls in java
a dsl provides a friendly, readable api to work with a particular domain model. For
that reason, we start this section by defining a simple domain model; then we discuss
the patterns that can be used to create a dsl on top of it.
The sample domain model is made of three things. The first thing is plain java
beans modeling a stock quoted on a given market:
public class stock {

private string symbol;
private string market;

public string getsymbol() {
return symbol;
}
public void setsymbol(string symbol) {
this. Symbol = symbol;
}

public string getmarket() {
return market;
}
public void setmarket(string market) {
this. Market = market;
}
}
patterns and techniques to create dsls in java 253

the second thing is a trade to buy or sell a given quantity of a stock at a given price:
public class trade {

public enum type { buy, sell }
private type type;

private stock stock;
private int quantity;
private double price;

public type gettype() {
return type;
}
public void settype(type type) {
this. Type = type;
}

public int getquantity() {
return quantity;
}
public void setquantity(int quantity) {
this. Quantity = quantity;
}

public double getprice() {
return price;
}
public void setprice(double price) {
this. Price = price;
}

public stock getstock() {
return stock;
}
public void setstock(stock stock) {
this. Stock = stock;
}

public double getvalue() {
return quantity * price;
}
}

the final thing is an order placed by a customer to settle one or more trades:
public class order {

private string customer;
private list<trade> trades = new arraylist<>();

public void addtrade(trade trade) {
trades. Add(trade);
}
254 chapter 10 domain-specific languages using lambdas

public string getcustomer() {
return customer;
}
public void setcustomer(string customer) {
this. Customer = customer;
}

public double getvalue() {
return trades. Stream(). Maptodouble(trade: : getvalue). Sum();
}
}

this domain model is straightforward. Its cumbersome to create objects representing
orders, for example. Try to define a simple order that contains two trades for your cus-
tomer bigbank, as shown in listing 10.4.

Listing 10.4 creating a stock trading order by using the domain objects api directly

order order = new order();
order. Setcustomer("bigbank");

trade trade1 = new trade();
trade1. Settype(trade. Type. Buy);

stock stock1 = new stock();
stock1. Setsymbol("ibm");
stock1. Setmarket("nyse");

trade1. Setstock(stock1);
trade1. Setprice(125.00);
trade1. Setquantity(80);
order. Addtrade(trade1);

trade trade2 = new trade();
trade2. Settype(trade. Type. Buy);

stock stock2 = new stock();
stock2. Setsymbol("google");
stock2. Setmarket("nasdaq");

trade2. Setstock(stock2);
trade2. Setprice(375.00);
trade2. Setquantity(50);
order. Addtrade(trade2);

the verbosity of this code is hardly acceptable; you cant expect a non-developer
domain expert to understand and validate it at first glance. What you need is a dsl
that reflects the domain model and allows it to be manipulated in a more immediate,
intuitive way. You can employ various approaches to achieve this result. In the rest of
this section, you learn the pros and cons of these approaches.
Patterns and techniques to create dsls in java 255

10.3.1 method chaining
the first style of dsl to explore is one of the most common. It allows you to define a
trading order with a single chain of method invocations. The following listing shows
an example of this type of dsl.

Listing 10.5 creating a stock trading order with method chaining

order order = forcustomer( "bigbank" )
. Buy( 80 )
. Stock( "ibm" )
. On( "nyse" )
. At( 125.00 )
. Sell( 50 )
. Stock( "google" )
. On( "nasdaq" )
. At( 375.00 )
. End();

this code looks like a big improvement, doesnt it? Its very likely that your domain
expert will understand this code effortlessly. But how can you implement a dsl to
achieve this result? You need a few builders that create the objects of this domain
through a fluent api. The top-level builder creates and wraps an order, making it pos-
sible to add one or more trades to it, as shown in the next listing.

Listing 10.6 an order builder providing a method-chaining dsl

public class methodchainingorderbuilder { the order
wrapped by
public final order order = new order(); this builder
a static factory
private methodchainingorderbuilder(string customer) { method to create a
order. Setcustomer(customer); builder of an order
}
placed by a given
customer
public static methodchainingorderbuilder forcustomer(string customer) {
return new methodchainingorderbuilder(customer);
}
creates a
public tradebuilder buy(int quantity) { tradebuilder
creates a return new tradebuilder(this, trade. Type. Buy, quantity); to build a
tradebuilder } trade to buy
to build a a stock
trade to sell public tradebuilder sell(int quantity) {
a stock return new tradebuilder(this, trade. Type. Sell, quantity);
}

adds a trade public methodchainingorderbuilder addtrade(trade trade) {
to the order order. Addtrade(trade);
return this;
returns the order builder itself,
} allowing you to fluently create
and add further trades
256 chapter 10 domain-specific languages using lambdas

public order end() {
return order;
terminates the building of
} the order and returns it
}

the buy() and sell() methods of the order builder create and return another builder
that builds a trade and adds it to the order itself:
public class tradebuilder {
private final methodchainingorderbuilder builder;
public final trade trade = new trade();

private tradebuilder(methodchainingorderbuilder builder,
trade. Type type, int quantity) {
this. Builder = builder;
trade. Settype( type );
trade. Setquantity( quantity );
}

public stockbuilder stock(string symbol) {
return new stockbuilder(builder, trade, symbol);
}
}

the only public method of the tradebuilder is used to create a further builder,
which then builds an instance of the stock class:
public class stockbuilder {
private final methodchainingorderbuilder builder;
private final trade trade;
private final stock stock = new stock();

private stockbuilder(methodchainingorderbuilder builder,
trade trade, string symbol) {
this. Builder = builder;
this. Trade = trade;
stock. Setsymbol(symbol);
}

public tradebuilderwithstock on(string market) {
stock. Setmarket(market);
trade. Setstock(stock);
return new tradebuilderwithstock(builder, trade);
}
}

the stockbuilder has a single method, on(), that specifies the market for the stock,
adds the stock to the trade, and returns one last builder:
public class tradebuilderwithstock {
private final methodchainingorderbuilder builder;
private final trade trade;
patterns and techniques to create dsls in java 257

public tradebuilderwithstock(methodchainingorderbuilder builder,
trade trade) {
this. Builder = builder;
this. Trade = trade;
}

public methodchainingorderbuilder at(double price) {
trade. Setprice(price);
return builder. Addtrade(trade);
}
}

this one public method of tradebuilderwithstock sets the unit price of the traded
stock and returns the original order builder. As youve seen, this method allows you to
fluently add other trades to the order until the end method of the methodchaining-
orderbuilder is called. The choice of having multiple builder classesand in particu-
lar, two different trade buildersis made to force the user of this dsl to call the
methods of its fluent api in a predetermined sequence, ensuring that a trade has
been configured correctly before the user starts creating the next one. The other
advantage of this approach is that the parameters used to set an order up are scoped
inside the builder. This approach minimizes the use of static methods and allows the
names of the methods to act as named arguments, thus further improving the read-
ability of this style of dsl. Finally, the fluent dsl resulting from this technique has
the least syntactic noise possible.
Unfortunately, the main issue in method chaining is the verbosity required to
implement the builders. A lot of glue code is necessary to mix the top-level builders
with the lower-level ones. Another evident disadvantage is the fact that you have no
way to enforce the indentation convention that you used to underline the nesting
hierarchy of the objects in your domain.
In the next section, you investigate a second dsl pattern that has quite different
characteristics.

10.3.2 using nested functions
the nested function dsl pattern takes its name from the fact that it populates the
domain model by using functions that are nested within other functions. The follow-
ing listing illustrates the dsl style resulting from this approach.

Listing 10.7 creating a stock-trading order with nested functions

order order = order("bigbank",
buy(80,
stock("ibm", on("nyse")),
at(125.00)),
sell(50,
stock("google", on("nasdaq")),
at(375.00))
);
258 chapter 10 domain-specific languages using lambdas

the code required to implement this dsl style is far more compact than what you
learned in section 10.3.1.
The nestedfunctionorderbuilder in the following listing shows that its possible
to provide an api with this dsl style to your users. (in this listing, we implicitly
assume that all its static methods are imported. )

listing 10.8 an order builder providing a nested-function dsl

public class nestedfunctionorderbuilder {
creates an
public static order order(string customer, trade... Trades) { order for a
order order = new order(); given customer
order. Setcustomer(customer);
stream. Of(trades). Foreach(order: : addtrade);
adds all trades
return order; to the order
}
creates
a trade to public static trade buy(int quantity, stock stock, double price) {
buy a stock return buildtrade(quantity, stock, price, trade. Type. Buy);
}

public static trade sell(int quantity, stock stock, double price) {
return buildtrade(quantity, stock, price, trade. Type. Sell);
creates
a trade to }
sell a stock
private static trade buildtrade(int quantity, stock stock, double price,
trade. Type buy) {
trade trade = new trade();
trade. Setquantity(quantity);
trade. Settype(buy);
trade. Setstock(stock);
trade. Setprice(price);
return trade;
} a dummy method to
define the unit price
public static double at(double price) { of the traded stock
return price;
}

public static stock stock(string symbol, string market) {
stock stock = new stock();
creates
the traded stock. Setsymbol(symbol);
stock stock. Setmarket(market);
return stock; a dummy method to
} define the market
where the stock is
public static string on(string market) { traded
return market;
}
}

the other advantage of this technique compared with method chaining is that the
hierarchy structure of your domain objects (an order contains one or more trades,
patterns and techniques to create dsls in java 259

and each trade refers to a single stock in the example) is visible by the way in which
the different functions are nested.
Unfortunately, this pattern also has some issues. You may have noticed that the
resulting dsl requires a lot of parentheses. Moreover, the list of arguments that have
to be passed to the static methods is rigidly predetermined. If the objects of your
domain have some optional fields, you need to implement different overloaded ver-
sions of those methods, which allows you to omit the missing parameters. Finally, the
meanings of the different arguments are defined by their positions rather than their
names. You can mitigate this last problem by introducing a few dummy methods, as
you did with the at() and on() methods in your nestedfunctionorderbuilder, the
only purpose of which is to clarify the role of an argument.
The two dsl patterns weve shown you so far dont require the use of lambda
expressions. In the next section, we illustrate a third technique that leverages the
functional capabilities introduced by java 8.

10.3.3 function sequencing with lambda expressions
the next dsl pattern employs a sequence of functions defined with lambda expres-
sions. Implementing a dsl in this style on top of your usual stock-trading domain
model allows you to define an order, as shown in listing 10.9.

Listing 10.9 creating a stock-trading order with function sequencing

order order = order( o -> {
o. Forcustomer( "bigbank" );
o. Buy( t -> {
t. Quantity( 80 );
t. Price( 125.00 );
t. Stock( s -> {
s. Symbol( "ibm" );
s. Market( "nyse" );
} );
});
o. Sell( t -> {
t. Quantity( 50 );
t. Price( 375.00 );
t. Stock( s -> {
s. Symbol( "google" );
s. Market( "nasdaq" );
} );
});
} );

to implement this approach, you need to develop several builders that accept lambda
expressions and to populate the domain model by executing them. These builders
keep the intermediate state of the objects to be created the way you did in the dsl
implementation by using method chaining. As you did in the method-chaining pat-
tern, you have a top-level builder to create the order, but this time, the builder takes
260 chapter 10 domain-specific languages using lambdas

consumer objects as parameters so that the user of the dsl can use lambda expres-
sions to implement them. The next listing shows the code required to implement this
approach.

Listing 10.10 an order builder providing a function-sequencing dsl

public class lambdaorderbuilder {
the order wrapped
private order order = new order(); by this builder

public static order order(consumer<lambdaorderbuilder> consumer) {
lambdaorderbuilder builder = new lambdaorderbuilder();
consumer. Accept(builder);
return builder. Order; executes the lambda expression
}
passed to the order builder

returns the order populated
public void forcustomer(string customer) { by executing the consumer
order. Setcustomer(customer); of the orderbuilder
sets the
customer }
who placed
the order public void buy(consumer<tradebuilder> consumer) { consumes a tradebuilder
trade(consumer, trade. Type. Buy); to create a trade to
} buy a stock

public void sell(consumer<tradebuilder> consumer) { consumes a tradebuilder
trade(consumer, trade. Type. Sell); to create a trade to
} sell a stock

private void trade(consumer<tradebuilder> consumer, trade. Type type) {
tradebuilder builder = new tradebuilder();
builder. Trade. Settype(type);
consumer. Accept(builder);
executes the lambda
order. Addtrade(builder. Trade); expression passed to
} the tradebuilder
} adds to the order the trade populated by
executing the consumer of the tradebuilder

the buy() and sell() methods of the order builder accept two lambda expressions
that are consumer<tradebuilder>. When executed, these methods populate a buying
or selling trade, as follows:
public class tradebuilder {
private trade trade = new trade();

public void quantity(int quantity) {
trade. Setquantity( quantity );
}

public void price(double price) {
trade. Setprice( price );
}

public void stock(consumer<stockbuilder> consumer) {
stockbuilder builder = new stockbuilder();
patterns and techniques to create dsls in java 261

consumer. Accept(builder);
trade. Setstock(builder. Stock);
}
}

finally, the tradebuilder accepts the consumer of a third builder thats intended to
define the traded stock:
public class stockbuilder {
private stock stock = new stock();

public void symbol(string symbol) {
stock. Setsymbol( symbol );
}

public void market(string market) {
stock. Setmarket( market );
}
}

this pattern has the merit of combining two positive characteristics of the two previ-
ous dsl styles. Like the method-chaining pattern it allows to define the trading order
in a fluent way. In addition, similarly to the nested-function style, it preserves the hier-
archy structure of our domain objects in the nesting level of the different lambda
expressions.
Unfortunately, this approach requires a lot of setup code, and using the dsl itself
is affected by the noise of the java 8 lambda-expression syntax.
Choosing among these three dsl styles is mainly a matter of taste. It also requires
some experience to find the best fit for the domain model for which you want to cre-
ate a domain language. Moreover, its possible to combine two or more of these styles
in a single dsl, as you see in the next section.

10.3.4 putting it all together
as youve seen so far, all three dsl patterns have pros and cons, but nothing prevents
you from using them together within a single dsl. You could end up developing a
dsl through which you could define your stock-trading order as shown in the follow-
ing listing.

Listing 10.11 creating a stock-trading order by using multiple dsl patterns

order order =
forcustomer( "bigbank",
nested function to
buy( t -> t. Quantity( 80 ) specify attributes of
. Stock( "ibm" ) the top-level order
method chaining in
the body of the . On( "nyse" )
lambda expression . At( 125.00 )),
that populates the sell( t -> t. Quantity( 50 ) lambda expression to
trade object . Stock( "google" ) create a single trade
. On( "nasdaq" )
. At( 125.00 )) );
262 chapter 10 domain-specific languages using lambdas

in this example, the nested-function pattern is combined with the lambda approach.
Each trade is created by a consumer of a tradebuilder thats implemented by a
lambda expression, as shown in the next listing.

Listing 10.12 an order builder providing a dsl that mixes multiple styles

public class mixedbuilder {

public static order forcustomer(string customer,
tradebuilder... Builders) {
order order = new order();
order. Setcustomer(customer);
stream. Of(builders). Foreach(b -> order. Addtrade(b. Trade));
return order;
}

public static tradebuilder buy(consumer<tradebuilder> consumer) {
return buildtrade(consumer, trade. Type. Buy);
}

public static tradebuilder sell(consumer<tradebuilder> consumer) {
return buildtrade(consumer, trade. Type. Sell);
}

private static tradebuilder buildtrade(consumer<tradebuilder> consumer,
trade. Type buy) {
tradebuilder builder = new tradebuilder();
builder. Trade. Settype(buy);
consumer. Accept(builder);
return builder;
}
}

finally, the helper class tradebuilder and the stockbuilder that it uses internally
(implementation shown immediately after this paragraph) provide a fluent api imple-
menting the method-chaining pattern. After you make this choice, you can write the
body of the lambda expression through which the trade will be populated in the most
compact way possible:

public class tradebuilder {
private trade trade = new trade();

public tradebuilder quantity(int quantity) {
trade. Setquantity(quantity);
return this;
}

public tradebuilder at(double price) {
trade. Setprice(price);
return this;
}
patterns and techniques to create dsls in java 263

public stockbuilder stock(string symbol) {
return new stockbuilder(this, trade, symbol);
}
}

public class stockbuilder {
private final tradebuilder builder;
private final trade trade;
private final stock stock = new stock();

private stockbuilder(tradebuilder builder, trade trade, string symbol){
this. Builder = builder;
this. Trade = trade;
stock. Setsymbol(symbol);
}

public tradebuilder on(string market) {
stock. Setmarket(market);
trade. Setstock(stock);
return builder;
}
}

listing 10.12 is an example of how the three dsl patterns discussed in this chapter
can be combined to achieve a readable dsl. Doing so allows you to take advantage of
the pros of the various dsl styles, but this technique has a minor drawback: the result-
ing dsl appears to be less uniform than one that uses a single technique, so users of
this dsl probably will need more time to learn it.
So far, youve used lambda expressions, but, as the comparator and stream apis
show, using method references can further improve the readability of many dsls. We
demonstrate this fact in the next section through a practical example of using method
references in the stock-trading domain model.

10.3.5 using method references in a dsl
in this section, you try to add another simple feature to your stock-trading domain
model. This feature calculates the final value of an order after adding zero or more of
the following taxes to the orders net value, as shown in the next listing.

Listing 10.13 the taxes that can be applied to the order's net value

public class tax {
public static double regional(double value) {
return value * 1.1;
}

public static double general(double value) {
return value * 1.3;
}
264 chapter 10 domain-specific languages using lambdas

public static double surcharge(double value) {
return value * 1.05;
}
}

the simplest way to implement such a tax calculator is to use a static method that accepts
the order plus one boolean flag for each tax that could be applied (listing 10.14).

Listing 10.14 applying taxes to the order's net value with a set of boolean flags

public static double calculate(order order, boolean useregional,
boolean usegeneral, boolean usesurcharge) {
double value = order. Getvalue();
if (useregional) value = tax. Regional(value);
if (usegeneral) value = tax. General(value);
if (usesurcharge) value = tax. Surcharge(value);
return value;
}

this way, its possible to calculate the final value of an order after applying the
regional tax and the surcharge, but not the general tax, as follows:

double value = calculate(order, true, false, true);

the readability problem of this implementation is evident: its difficult to remember
the right sequence of boolean variables and to understand which taxes have been
applied and which havent. The canonical way to fix this issue is to implement a tax-
calculator that provides a minimal dsl to fluently set the boolean flags one by one,
as shown the next listing.

Listing 10.15 a tax calculator that fluently defines the taxes to be applied

public class taxcalculator {
private boolean useregional;
private boolean usegeneral;
private boolean usesurcharge;

public taxcalculator withtaxregional() {
useregional = true;
return this;
}

public taxcalculator withtaxgeneral() {
usegeneral= true;
return this;
}

public taxcalculator withtaxsurcharge() {
usesurcharge = true;
return this;
}
patterns and techniques to create dsls in java 265

public double calculate(order order) {
return calculate(order, useregional, usegeneral, usesurcharge);
}
}

using this taxcalculator makes clear that you want to apply the regional tax and the
surcharge to the net value of the order:
double value = new taxcalculator(). Withtaxregional()
. Withtaxsurcharge()
. Calculate(order);

the main issue with this solution is its verbosity. It doesnt scale because you need a
boolean field and a method for each tax in your domain. By using the functional
capabilities of java, you can achieve the same result in terms of readability in a far
more compact and flexible way. To see how, refactor your taxcalculator as shown in
this next listing.

Listing 10.16 a tax calculator that fluently combines the tax functions to be applied

the function calculating
public class taxcalculator { all the taxes to be applied
public doubleunaryoperator taxfunction = d -> d; to the orders value

public taxcalculator with(doubleunaryoperator f) { obtains the new tax-
taxfunction = taxfunction. Andthen(f);
calculating function,
composing the current
return this;
one with the one passed
}
as argument
public double calculate(order order) {
return taxfunction. Applyasdouble(order. Getvalue());
}
} calculates the final orders value by
applying the tax-calculating function
returns this, allowing further tax to the orders net value
functions to be concatenated fluently

with this solution, you need only one field: the function that, when applied to the
orders net value, adds in one shot all the taxes configured through the taxcalculator
class. The starting value of this function is the identity function. At this point, no tax
has been added yet, so the orders final value is the same as the net value. When a new
tax is added through the with() method, this tax is composed with the current tax-
calculating function, thus encompassing all the added taxes in a single function.
Finally, when an order is passed to the calculate() method, the tax-calculating func-
tion resulting from the composition of the various configured taxes is applied to the
orders net value. This refactored taxcalculator can be used as follows:
double value = new taxcalculator(). With(tax: : regional)
. With(tax: : surcharge)
. Calculate(order);
266 chapter 10 domain-specific languages using lambdas

this solution uses method references, is easy to read, and gives succinct code. Its also
flexible in that if and when a new tax function is added to the tax class, you can use it
immediately with your functional taxcalculator without modification.
Now that weve discussed the various techniques that can be used to implement a
dsl in java 8 and beyond, its interesting to investigate how these strategies have been
used in widely adopted java tools and frameworks.

10.4 real world java 8 dsl
in section 10.3, you learned three useful patterns for developing dsls in java,
together with their pros and cons. Table 10.1 summarizes what weve discussed so far.

Table 10.1 dsls patterns with their pros and cons

pattern name pros cons

method chaining  method names that act as keyword  verbose implementation
arguments  glue code to bind the builders
 works well with optional parameters  hierarchy of domain objects
 possible to enforce the dsl user to defined only by indentation
call methods in a pre-determined order convention
 minimal or no use of static methods
 lowest possible syntactic noise

nested functions  lower implementation verbosity  heavy use of static methods
 domain objects hierarchy echoed by  arguments defined by position
function nesting rather than name
 method overloading required for
optional parameters

function sequenc-  works well with optional parameters  verbose implementation
ing with lambdas  minimal or no use of static methods  more syntactic noise from lambda
 hierarchy of domain objects echoed by expressions in the dsl
lambdas nesting
 no glue code for builders

its time to consolidate what youve learned so far by analyzing how these patterns are
employed in three well-known java libraries: an sql mapping tool, a behavior-driven
development framework, and a tool that implements enterprise integration patterns.

10.4.1 jooq
sql is one of the most common and widely used dsls. For this reason, it shouldnt be
surprising that theres a java library providing a nice dsl to write and execute sql
queries. Jooq is an internal dsl that implements sql as a type-safe embedded lan-
guage directly in java. A source-code generator reverse-engineers the database schema,
which allows the java compiler to type-check complex sql statements. The product
of this reverse-engineering process generates information with which you can navigate
your database schema. As a simple example, the following sql query
real world java 8 dsl 267

select * from book
where book. Published_in = 2016
order by book. Title

can be written using the jooq dsl like this:
create. Selectfrom(book)
. Where(book. Published_in. Eq(2016))
. Orderby(book. Title)

another nice feature of the jooq dsl is the possibility of using it in combination
with the stream api. This feature allows you to manipulate in memory, with a single
fluent statement, the data resulting from the execution of the sql query, as shown in
the next listing.

Listing 10.17 selecting books from a database by using the jooq dsl

starts manipulating data fetched
from database with stream api creates the connection
class. Forname("org. H2. Driver"); to the sql database
try (connection c =
getconnection("jdbc: h2: ~/sql-goodies-with-mapping", "sa", "")) {
dsl. Using(c)
starts the jooq sql
. Select(book. Author, book. Title) statement, using the just-
. Where(book. Published_in. Eq(2016)) created database connection
. Orderby(book. Title)
. Fetch() defines the sql statement
. Stream() through the jooq dsl
. Collect(groupingby(
groups fetches the data from the database;
the books r -> r. Getvalue(book. Author),
jooq statement ends here
by author linkedhashmap: : new,
mapping(r -> r. Getvalue(book. Title), tolist())))
. Foreach((author, titles) ->
system. Out. Println(author + " is author of " + titles));
}
prints the authors names together
with the books they wrote

its evident that the main dsl pattern chosen to implement the jooq dsl is
method-chaining. In fact, various characteristics of this pattern (allowing optional
parameters and requiring certain methods to be called only in a predetermined
sequence) are essential to mimic the syntax of a well-formed sql query. These fea-
tures, together with its lower syntactic noise, make the method-chaining pattern a
good fit for jooqs needs.

10.4.2 cucumber
behavior-driven development (bdd) is an extension of test-driven development that
uses a simple domain-specific scripting language made of structured statements that
describe various business scenarios. Cucumber, like other bdd frameworks, translates
these statements into executable test cases. As a result, the scripts resulting from the
268 chapter 10 domain-specific languages using lambdas

application of this development technique can be used both as runnable tests and as
acceptance criteria for a given business feature. Bdd also focuses the development
effort on the delivery of prioritized, verifiable business value and bridges the gap
between domain experts and programmers by making them share a business vocabulary.
These abstract concepts can be clarified by a practical example that uses cucum-
ber, a bdd tool that enables developers to write business scenarios in plain english.
Use cucumbers scripting language as follows to define a simple business scenario:
feature: buy stock
scenario: buy 10 ibm stocks
given the price of a "ibm" stock is 125$
when i buy 10 "ibm"
then the order value should be 1250$

cucumber uses notation thats divided into three parts: the definition of prerequisites
(given), the actual calls to the domain objects under test, and (when) the assertions
checking the outcome of the test case (then).
The script that defines the test scenario is written with an external dsl that has a
limited number of keywords and lets you write sentences in a free format. These sen-
tences are matched through regular expressions that capture the variables of the test
case and pass them as arguments to the methods that implement the test itself. Using
the stock-trading domain model from the beginning of section 10.3, its possible to
develop a cucumber test case that checks whether the value of a stock-trading order is
calculated correctly, as shown in the next listing.

Listing 10.18 implementing a test scenario by using cucumber annotations

public class buystockssteps {
private map<string, integer> stockunitprices = new hashmap<>();
private order order = new order();

@given("^the price of a \"(. *? )\" stock is (\\d+)\\$$")
defines the unit
public void setunitprice(string stockname, int unitprice) { price of a stock as
stockunitvalues. Put(stockname, unitprice); a prerequisite of
stores
the stock } this scenario
unit price
@when("^i buy (\\d+) \"(. *? )\"$")
defines the
public void buystocks(int quantity, string stockname) { actions to be
trade trade = new trade(); taken on the
trade. Settype(trade. Type. Buy); populates the domain model
domain model under test
stock stock = new stock(); accordingly
stock. Setsymbol(stockname);

trade. Setstock(stock);
trade. Setprice(stockunitprices. Get(stockname));
trade. Setquantity(quantity);
order. Addtrade(trade);
}
real world java 8 dsl 269

@then("^the order value should be (\\d+)\\$$")
checks defines the expected
public void checkordervalue(int expectedvalue) { scenario outcome
the test assertequals(expectedvalue, order. Getvalue());
assertions }
}

the introduction of lambda expressions in java 8 allowed cucumber to develop an
alternative syntax that eliminated annotations by using two-argument methods: the
regular expression previously contained in the annotation value and the lambda
implementing the test method. When you use this second type of notation, you can
rewrite the test scenario like this:
public class buystockssteps implements cucumber. Api. Java8. En {
private map<string, integer> stockunitprices = new hashmap<>();
private order order = new order();
public buystockssteps() {
given("^the price of a \"(. *? )\" stock is (\\d+)\\$$",
(string stockname, int unitprice) -> {
stockunitvalues. Put(stockname, unitprice);
});
// ... When and then lambdas omitted for brevity
}
}

this alternative syntax has the obvious advantage of being compact. In particular,
replacing the test methods with anonymous lambdas eliminates the burden of find-
ing meaningful method names (which rarely adds anything to readability in a test
scenario).
Cucumbers dsl is extremely simple, but it demonstrates how to effectively com-
bine an external dsl with an internal one and (once again) shows that lambdas allow
you to write more compact, readable code.

10.4.3 spring integration
spring integration extends the dependency-injection-based spring programming model
to support the well-known enterprise integration patterns.1 spring integrations pri-
mary goals are to provide a simple model to implement complex enterprise integra-
tion solutions and to promote the adoption of an asynchronous, message-driven
architecture.
Spring integration enables lightweight remoting, messaging, and scheduling
within spring-based applications. These features are also available through a rich, flu-
ent dsl thats more than syntactic sugar built on top of traditional spring xml con-
figuration files.

1
for more details see the book: enterprise integration patterns: designing, building, and deploying messag-
ing solutions (addison-wesley) gregor hohpe and bobby woolf, 2004.
270 chapter 10 domain-specific languages using lambdas

spring integration implements all the most common patterns necessary for mes-
sage-based applications, such as channels, endpoints, pollers, and channel intercep-
tors. Endpoints are expressed as verbs in the dsl to improve readability, and
integration processes are constructed by composing these endpoints into one or more
message flows. The next listing shows how spring integration works with a simple but
complete example.

Listing 10.19 configuring a spring integration flow by using the spring integration dsl

@configuration
@enableintegration
public class myconfiguration {

@bean creates a new message-
public messagesource<? > integermessagesource() { source that at each
methodinvokingmessagesource source = invocation increments
new methodinvokingmessagesource(); an atomicinteger
source. Setobject(new atomicinteger());
the channel source. Setmethodname("getandincrement");
conveying return source;
the data }
arriving starts creating the
from the @bean integrationflow through
message- public directchannel inputchannel() { a builder following the
source return new directchannel(); method-chaining pattern
}
uses the formerly
@bean defined messagesource
public integrationflow myflow() { as the source for this
return integrationflows integrationflow
. From(this. Integermessagesource(),
c -> c. Poller(pollers. Fixedrate(10)))
polls the messagesource filters only the
to dequeue the data . Channel(this. Inputchannel())
. Filter((integer p) -> p % 2 == 0) even numbers
it conveys
. Transform(object: : tostring)
. Channel(messagechannels. Queue("queuechannel"))
. Get();
} sets channel queuechannel as
} terminates the building output for this integrationflow
of the integrationflow
and returns it converts the integers retrieved
from the messagesource
into strings

here, the method myflow() builds an integrationflow by using the spring integra-
tion dsl. It uses the fluent builder provided by the integrationflows class, which
implements the method-chaining pattern. In this case, the resulting flow polls a
messagesource at a fixed rate, providing a sequence of integers; filters the even ones
and converts them to strings, and finally sends the result to an output channel in a
style thats similar to the native java 8 stream api. This api allows a message to be sent
summary 271

to any component within the flow if you know its inputchannel name. If the flow starts
with a direct channel, not a messagesource, its possible to define the integrationflow
with a lambda expression as follows:
@bean
public integrationflow myflow() {
return flow -> flow. Filter((integer p) -> p % 2 == 0)
. Transform(object: : tostring)
. Handle(system. Out: : println);
}

as you see, the most widely used pattern in spring integration dsl is method chain-
ing. This pattern fits well with the main purpose of the integrationflow builder: cre-
ating a flow of message-passing and data transformations. As shown in this last
example, however, it also uses function sequencing with lambda expressions for the
top-level object to be built (and in some cases also for inner, more-complex method
arguments).

Summary
 the main purpose of a dsl is to fill the gap between developers and domain
experts. Its rare for the person who writes the code that implements the busi-
ness logic of an application to also have deep knowledge in the business field in
which the program will be used. Writing this business logic in a language that
non-developers can understand doesnt turn domain experts into program-
mers, but it does allow them to read and validate the logic.
 The two main categories of dsls are internal (implemented in the same lan-
guage used to develop the application in which the dsl will be used) and
external (using a different language designed ad hoc). Internal dsls require
less development effort but have a syntax constrained by the hosting lan-
guage. External dsls offer a higher degree of flexibility but are harder to
implement.
 Its possible to develop a polyglot dsl by using another programming language
already available on the jvm, such as scala or groovy. These languages are
often more flexible and concise than java. Integrating them with java requires a
more-complex building process, however, and their interoperability with java
can be far from seamless.
 Due to its verbosity and rigid syntax, java isnt the ideal programming language
to use to develop internal dsls, but the introduction of lambda expressions
and method references in java 8 hugely improved this situation.
 Modern java already provides small dsls in its native api. These dsls, like the
ones in the stream and collectors classes, are useful and convenient, particu-
larly for sorting, filtering, transforming, and grouping collections of data.
 The three main patterns used to implement dsls in java are method chaining,
nested functions, and function sequencing. Each pattern has pros and cons, but
272 chapter 10 domain-specific languages using lambdas

you can combine all three patterns in a single dsl to take advantage of all three
techniques.
 Many java frameworks and libraries allow their features to be used through a
dsl. This chapter looked at three of them: jooq, an sql mapping tool; cucum-
ber, a bdd framework; and spring integration, a spring extension that imple-
ments enterprise integration patterns.
Part 4

everyday java

t he fourth part of this book explores various new features in java 8 and java 9
centered around making it easier and more reliable to code your projects. We
start with two apis introduced in java 8.
Chapter 11 covers the java. Util. Optional class, which allows you to both
design better apis and reduce null pointer exceptions.
Chapter 12 explores the date and time api, which greatly improves the pre-
vious error-prone apis for working with dates and time.
Then we explain java 8 and java 9 enhancements for writing big systems and
enabling them to evolve.
In chapter 13, youll learn what default methods are, how you can use them
to evolve apis in a compatible way, some practical usage patterns, and rules for
using default methods effectively.
Chapter 14 is new for this second edition and explores the java module sys-
tema major enhancement in java 9 that enables huge systems to be modular-
ized in a documented and enforceable way, rather than being only a haphazard
collection of packages.
Using optional as
a better alternative to null

this chapter covers
 whats wrong with null references and why you
should avoid them
 from null to optional: rewriting your domain
model in a null-safe way
 putting optionals to work: removing null checks
from your code
 different ways to read the value possibly
contained in an optional
 rethinking programming given potentially missing
values

raise your hand if you ever got a nullpointerexception during your life as a java
developer. Keep it up if this exception is the one you encounter most frequently.
Unfortunately, we cant see you at this moment, but we believe that theres a high
probability that your hand is raised now. We also guess that you may be thinking
something like yes, i agree. Nullpointerexceptions are a pain for any java devel-
oper, novice, or expert. But theres not much we can do about them, because this is
the price we pay to use such a convenient, and maybe unavoidable, construct as

275
276 chapter 11 using optional as a better alternative to null

null references. This feeling is common in the (imperative) programming world;
nevertheless, it may not be the whole truth and is more likely a bias with solid histori-
cal roots.
British computer scientist tony hoare introduced null references back in 1965
while designing algol w, one of the first typed programming languages with heap-
allocated records, later saying that he did so simply because it was so easy to imple-
ment. Despite his goal to ensure that all use of references could be absolutely safe,
with checking performed automatically by the compiler, he decided to make an
exception for null references because he thought that they were the most convenient
way to model the absence of a value. After many years, he regretted this decision, calling
it my billion-dollar mistake. Weve all seen the effect. We examine a field of an
object, perhaps to determine whether its value is one of two expected forms, only to
find that were examining not an object but a null pointer that promptly raises that
annoying nullpointerexception.
In reality, hoares statement could underestimate the costs incurred by millions of
developers fixing bugs caused by null references in the past 50 years. Indeed, the vast
majority of the languages1 created in recent decades, including java, have been built
with the same design decision, maybe for reasons of compatibility with older lan-
guages or (more probably), as hoare states, simply because it was so easy to imple-
ment. We start by showing you a simple example of the problems with null.

11.1 how do you model the absence of a value?
Imagine that you have the following nested object structure for a person who owns a
car and has car insurance in the following listing.

Listing 11.1 the person/car/insurance data model

public class person {
private car car;
public car getcar() { return car; }
}
public class car {
private insurance insurance;
public insurance getinsurance() { return insurance; }
}
public class insurance {
private string name;
public string getname() { return name; }
}

1
notable exceptions include most typed functional languages, such as haskell and ml. These languages
include algebraic data types that allow data types to be expressed succinctly, including explicit specification of
whether special values such as null are to be included on a type-by-type basis.
How do you model the absence of a value? 277

whats problematic with the following code?
Public string getcarinsurancename(person person) {
return person. Getcar(). Getinsurance(). Getname();
}

this code looks pretty reasonable, but many people dont own a car, so whats the
result of calling the method getcar? A common unfortunate practice is to return the
null reference to indicate the absence of a value (here, to indicate the absence of a
car). As a consequence, the call to getinsurance returns the insurance of a null ref-
erence, which results in a nullpointerexception at runtime and stops your program
from running further. But thats not all. What if person was null? What if the method
getinsurance returned null too?

11.1.1 reducing nullpointerexceptions with defensive checking
what can you do to avoid running into an unexpected nullpointerexception? Typ-
ically, you can add null checks where necessary (and sometimes, in an excess of
defensive programming, even where not necessary) and often with different styles.
A first attempt to write a method preventing a nullpointerexception is shown in
the following listing.

Listing 11.2 null-safe attempt 1: deep doubts

public string getcarinsurancename(person person) {
if (person ! = null) { each null check
car car = person. Getcar(); increases the
if (car ! = null) { nesting level of the
insurance insurance = car. Getinsurance(); remaining part of
if (insurance ! = null) { the invocation chain.
Return insurance. Getname();
}
}
}
return "unknown";
}

this method performs a null check every time it dereferences a variable, returning
the string "unknown" if any of the variables traversed in this dereferencing chain is a
null value. The only exception to this rule is that youre not checking to see whether
the name of the insurance company is null because (like any other company) you
know it must have a name. Note that you can avoid this last check only because of your
knowledge of the business domain, but that fact isnt reflected in the java classes mod-
eling your data.
We labeled the method in listing 11.2 deep doubts because it shows a recurring
pattern: every time you doubt that a variable could be null, youre obliged to add a
further nested if block, increasing the indentation level of the code. This tech-
nique clearly scales poorly and compromises readability, so maybe youd like to
278 chapter 11 using optional as a better alternative to null

attempt another solution. Try to avoid this problem by doing something different as
shown in the next listing.

Listing 11.3 null-safe attempt 2: too many exits

public string getcarinsurancename(person person) {
if (person == null) {
return "unknown";
}
car car = person. Getcar(); each null check
if (car == null) { adds a further
return "unknown"; exit point.
}
insurance insurance = car. Getinsurance();
if (insurance == null) {
return "unknown";
}
return insurance. Getname();
}

in this second attempt, you try to avoid the deeply nested if blocks, adopting a differ-
ent strategy: every time you meet a null variable, you return the string "unknown".
Nevertheless, this solution is also far from ideal; now the method has four distinct exit
points, making it hard to maintain. Even worse, the default value to be returned in
case of a null, the string "unknown", is repeated in three placesand (we hope) not
misspelled! (you may want to extract the repeated string into a constant to prevent
this problem, of course. )
furthermore, the process is error-prone. What if you forget to check whether one
property could be null? We argue in this chapter that using null to represent the
absence of a value is the wrong approach. What you need is a better way to model
the absence and presence of a value.

11.1.2 problems with null
to recap our discussion so far, the use of null references in java causes both theoreti-
cal and practical problems:
 its a source of error. Nullpointerexception is by far the most common excep-
tion in java.
 It bloats your code. It worsens readability by making it necessary to fill your code
with null checks that are often deeply nested.
 Its meaningless. It doesnt have any semantic meaning, and in particular, it rep-
resents the wrong way to model the absence of a value in a statically typed lan-
guage.
 It breaks java philosophy. Java always hides pointers from developers except in
one case: the null pointer.
 It creates a hole in the type system. Null carries no type or other information, so it
can be assigned to any reference type. This situation is a problem because when
how do you model the absence of a value? 279

null is propagated to another part of the system, you have no idea what that
null was initially supposed to be.
To provide some context for other solutions, in the next section we briefly look at
what other programming languages have to offer.

11.1.3 what are the alternatives to null in other languages?
In recent years, languages such as groovy worked around this problem by introducing
a safe navigation operator, represented by ?. , to safely navigate potentially null values.
To understand how this process works, consider the following groovy code, which
retrieves the name of the insurance company used by a given person to insure a car:

def carinsurancename = person?. Car?. Insurance?. Name

what this statement does should be clear. A person may not have a car, and you tend
to model this possibility by assigning a null to the car reference of the person object.
Similarly, a car may not be insured. The groovy safe navigation operator allows you to
safely navigate these potentially null references without throwing a nullpointer-
exception by propagating the null reference through the invocations chain, return-
ing a null in the event that any value in the chain is a null.
A similar feature was proposed and then discarded for java 7. Somehow, though,
we dont seem to miss a safe navigation operator in java. The first temptation of all
java developers when confronted with a nullpointerexception is to fix it quickly by
adding an if statement, checking that a value isnt null before invoking a method on
it. If you solve this problem in this way, without wondering whether its correct for
your algorithm or your data model to present a null value in that specific situation,
youre not fixing a bug but hiding it, making its discovery and remedy far more diffi-
cult for whoever will be called to work on it next time (likely you in the next week or
month). Youre sweeping the dirt under the carpet. Groovys null-safe dereferencing
operator is only a bigger and more powerful broom for making this mistake without
worrying too much about its consequences.
Other functional languages, such as haskell and scala, take a different view. Has-
kell includes a maybe type, which essentially encapsulates an optional value. A value of
type maybe can contain a value of a given type or nothing. Haskell no concept of a
null reference. Scala has a similar construct called option[t] to encapsulate the pres-
ence or absence of a value of type t, which we discuss in chapter 20. Then you have to
explicitly check whether a value is present or not using operations available on the
option type, which enforces the idea of null checking. You can no longer forget to
check for nullbecause checking is enforced by the type system.
Okay, weve diverged a bit, and all this sounds fairly abstract. You may wonder
about java 8. Java 8 takes inspiration from this idea of an optional value by introduc-
ing a new class called java. Util. Optional<t>! In this chapter, we show the advan-
tages of using this class to model potentially absent values instead of assigning a null
reference to them. We also clarify how this migration from nulls to optionals
280 chapter 11 using optional as a better alternative to null

requires you to rethink the way you deal with optional values in your domain model.
Finally, we explore the features of this new optional class and provide a few practical
examples showing how to use it effectively. Ultimately, you learn how to design better
apis in which users can tell whether to expect an optional value by reading the signa-
ture of a method.

11.2 introducing the optional class
java 8 introduces a new class called java. Util. Optional<t> thats inspired by haskell
and scala. The class encapsulates an optional value. If you know that a person may not
have a car, for example, the car variable inside the person class shouldnt be declared
type car and assigned to a null reference when the person doesnt own a car; instead,
it should be type optional<car>, as illustrated in figure 11.1.

Optional<car> optional<car>

car

figure 11.1 an optional car
contains an object an empty optional
of type car

when a value is present, the optional class wraps it. Conversely, the absence of a value
is modeled with an empty optional returned by the method optional. Empty. This
static factory method returns a special singleton instance of the optional class. You
may wonder about the difference between a null reference and optional. Empty().
Semantically, they could be seen as the same thing, but in practice, the difference is
huge. Trying to dereference a null invariably causes a nullpointerexception,
whereas optional. Empty() is a valid, workable object of type optional that can be
invoked in useful ways. Youll soon see how.
An important, practical semantic difference in using optionals instead of nulls is
that in the first case, declaring a variable of type optional<car> instead of car clearly
signals that a missing value is permitted there. Conversely, always using the type car
and possibly assigning a null reference to a variable of that type implies that you dont
have any help, other than your knowledge of the business model, in understanding
whether the null belongs to the valid domain of that given variable.
With this act in mind, you can rework the original model from listing 11.1, using
the optional class as shown in the following listing.

Listing 11.4 redefining the person/car/insurance data model by using optional

public class person { a person may not own a car, so
private optional<car> car; you declare this field optional.
Patterns for adopting optionals 281

public optional<car> getcar() { return car; }
} a car may not be insured,
public class car { so you declare this field
private optional<insurance> insurance;
optional.
Public optional<insurance> getinsurance() { return insurance; }
}
public class insurance { an insurance company
private string name; must have a name.
Public string getname() { return name; }
}

note how the use of the optional class enriches the semantics of your model. The
fact that a person references an optional<car>, and a car references an optional
<insurance>, makes it explicit in the domain that a person may or may not own a car,
and that car may or may not be insured.
At the same time, the fact that the name of the insurance company is declared of
type string instead of optional<string> makes it evident that an insurance company
must have a name. This way, you know for certain whether youll get a nullpointer-
exception when dereferencing the name of an insurance company; you dont have to
add a null check, because doing so will hide the problem instead of fixing it. An
insurance company must have a name, so if you find one without a name, youll have
to work out whats wrong in your data instead of adding a piece of code to cover up
this circumstance. Consistently using optional values creates a clear distinction
between a missing value thats planned for and a value thats absent only because of a
bug in your algorithm or a problem in your data. Its important to note that the inten-
tion of the optional class isnt to replace every single null reference. Instead, its pur-
pose is to help you design more-comprehensible apis so that by reading the signature
of a method, you can tell whether to expect an optional value. Youre forced to
actively unwrap an optional to deal with the absence of a value.

11.3 patterns for adopting optionals
so far, so good; youve learned how to employ optionals in types to clarify your
domain model, and youve seen the advantages of this process over representing miss-
ing values with null references. How can you use optionals now? More specifically,
how can you use a value wrapped in an optional?

11.3.1 creating optional objects
the first step before working with optional is learning how to create optional objects!
You can create them in several ways.
Empty optional
as mentioned earlier, you can get hold of an empty optional object by using the static
factory method optional. Empty:

optional<car> optcar = optional. Empty();
282 chapter 11 using optional as a better alternative to null

optional from a non-null value
you can also create an optional from a non-null value with the static factory method
optional. Of:

optional<car> optcar = optional. Of(car);

if car were null, a nullpointerexception would be thrown immediately (rather than
getting a latent error when you try to access properties of the car).
Optional from null
finally, by using the static factory method optional. Ofnullable, you can create an
optional object that may hold a null value:

optional<car> optcar = optional. Ofnullable(car);

if car were null, the resulting optional object would be empty.
You might imagine that well continue by investigating how to get a value out of an
optional. A get method does precisely this, and we talk more about it later. But get
raises an exception when the optional is empty, so using it in an ill-disciplined manner
effectively re-creates all the maintenance problems caused by using null. Instead, we
start by looking at ways of using optional values that avoid explicit tests, inspired by
similar operations on streams.

11.3.2 extracting and transforming values from optionals with map
a common pattern is to extract information from an object. You may want to extract
the name from an insurance company, for example. You need to check whether
insurance is null before extracting the name as follows:
string name = null;
if(insurance ! = null){
name = insurance. Getname();
}

optional supports a map method for this pattern, which works as follows (from here
on, we use the model presented in listing 11.4):
optional<insurance> optinsurance = optional. Ofnullable(insurance);
optional<string> name = optinsurance. Map(insurance: : getname);

this method is conceptually similar to the map method of stream you saw in chapters
4 and 5. The map operation applies the provided function to each element of a stream.
You could also think of an optional object as being a particular collection of data,
containing at most a single element. If the optional contains a value, the function
passed as argument to map transforms that value. If the optional is empty, nothing
happens. Figure 11.2 illustrates this similarity, showing what happens when you pass a
function that transforms a square into a triangle to the map methods of both a stream
of square and an optional of square.
Patterns for adopting optionals 283

map( -> )
stream stream

optional optional
map( -> )

figure 11.2 comparing the map methods of streams and optionals

this idea looks useful, but how can you use it to rewrite the code in listing 11.1,
public string getcarinsurancename(person person) {
return person. Getcar(). Getinsurance(). Getname();
}

which chains several method calls, in a safe way?
The answer is to use another method supported by optional called flatmap.

11.3.3 chaining optional objects with flatmap
because youve learned how to use map, your first reaction may be to use map to rewrite
the code as follows:
optional<person> optperson = optional. Of(person);
optional<string> name =
optperson. Map(person: : getcar)
. Map(car: : getinsurance)
. Map(insurance: : getname);

unfortunately, this code doesnt compile. Why? The
variable optperson is of type optional<person>, so its optional
perfectly fine to call the map method. But getcar
returns an object of type optional<car> (as presented optional
in listing 11.4), which means that the result of the map
car
operation is an object of type optional<optional<car>>.
As a result, the call to getinsurance is invalid because
the outermost optional contains as its value another
optional, which of course doesnt support the get- figure 11.3 a two-level
insurance method. Figure 11.3 illustrates the nested optional
optional structure youd get.
How can you solve this problem? Again, you can look at a pattern youve used pre-
viously with streams: the flatmap method. With streams, the flatmap method takes a
function as an argument and returns another stream. This function is applied to each
284 chapter 11 using optional as a better alternative to null

element of a stream, resulting in a stream of streams. But flatmap has the effect of
replacing each generated stream with the contents of that stream. In other words, all
the separate streams that are generated by the function get amalgamated or flattened
into a single stream. What you want here is something similar, but you want to flatten
a two-level optional into one.
As figure 11.2 does for the map method, figure 11.4 illustrates the similarities
between the flatmap methods of the stream and optional classes.

Flatmap( -> )
stream stream

optional optional
flatmap( -> )

figure 11.4 comparing the flatmap methods of stream and optional

here, the function passed to the streams flatmap method transforms each square
into another stream containing two triangles. Then the result of a simple map is a
stream containing three other streams, each with two triangles, but the flatmap
method flattens this two-level stream into a single stream containing six triangles in
total. In the same way, the function passed to the optionals flatmap method trans-
forms the square contained in the original optional into an optional containing a tri-
angle. If this function were passed to the map method, the result would be an optional
containing another optional that in turn contains a triangle, but the flatmap method
flattens this two-level optional into a single optional containing a triangle.
Finding a cars insurance company name with optionals
now that you know the theory of the map and flatmap methods of optional, youre
ready to put them into practice. The ugly attempts made in listings 11.2 and 11.3
can be rewritten by using the optional-based data model of listing 11.4 as follows.

Listing 11.5 finding a cars insurance company name with optionals

public string getcarinsurancename(optional<person> person) {
return person. Flatmap(person: : getcar)
. Flatmap(car: : getinsurance) a default value if the
. Map(insurance: : getname) resulting optional is
. Orelse("unknown"); empty
}
patterns for adopting optionals 285

comparing listing 11.5 with the two former attempts shows the advantages of using
optionals when dealing with potentially missing values. This time, you can obtain what
you want with an easily comprehensible statement instead of increasing the code com-
plexity with conditional branches.
In implementation terms, first note that you modify the signature of the getcar-
insurancename method from listings 11.2 and 11.3. We explicitly said that there could
be a case in which a nonexistent person is passed to this method, such as when that
person is retrieved from a database via an identifier, and you want to model the possi-
bility that no person exists in your data for the given identifier. You model this addi-
tional requirement by changing the type of the methods argument from person to
optional<person>.
Once again, this approach allows you to make explicit through the type system
something that otherwise would remain implicit in your knowledge of the domain
model: the first purpose of a language, even a programming language, is communica-
tion. Declaring a method to take an optional as an argument or to return an optional
as a result documents to your colleaguesand all future users of your methodthat it
can take an empty value or give an empty value as a result.
Person/car/insurance dereferencing chain using optionals
starting with this optional<person>, the car from the person, the insurance from
the car, and the string containing the insurance company name from the insurance
are dereferenced with a combination of the map and flatmap methods introduced
earlier in this chapter. Figure 11.5 illustrates this pipeline of operations.

Optional optional
flatmap(person: : getcar) flatmap(car: : getinsurance)
person car

step 1 step 2

optional optional
insurance orelse("unknown") map(insurance: : getname)
company string insurance
name

step 4 step 3

figure 11.5 the person/car/insurance dereferencing chain using optionals

here, you begin with the optional that wraps the person and invokes flat-
map(person: : getcar) on it. As we said, you can logically think of this invocation as
something that happens in two steps. In step 1, a function is applied to the person
286 chapter 11 using optional as a better alternative to null

inside the optional to transform it. In this case, the function is expressed with a
method reference invoking the method getcar on that person. Because that method
returns an optional<car>, the person inside the optional is transformed into an
instance of that type, resulting in a two-level optional thats flattened as part of the
flatmap operation. From a theoretical point of view, you can think of this flattening
operation as the operation that combines two nested optionals, resulting in an empty
optional if at least one of them is empty. What happens in reality is that if you invoke
flatmap on an empty optional, nothing is changed, and the empty optional is returned
as is. Conversely, if the optional wraps a person, the function passed to the flatmap
method is applied to that person. Because the value produced by that function appli-
cation is already an optional, the flatmap method can return it as is.
The second step is similar to the first one, transforming the optional<car> into
an optional<insurance>. Step 3 turns the optional<insurance> into an optional
<string>: because the insurance. Getname() method returns a string. In this case, a
flatmap isnt necessary.
At this point the resulting optional will be empty if any of the methods in this invo-
cation chain returns an empty optional or otherwise contains the desired insurance
company name. How do you read that value? After all, youll end up getting an
optional<string> that may or may not contain the name of the insurance company.
In listing 11.5, we used another method called orelse, which provides a default value
in case the optional is empty. Many methods provide default actions or unwrap an
optional. In the next section, we look at those methods in detail.

Using optionals in a domain model and why theyre not serializable
in listing 11.4, we showed how to use optionals in your domain model to mark with
a specific type the values that are allowed to be missing or remain undefined. The
designers of the optional class, however, developed it based on different assump-
tions and with a different use case in mind. In particular, java language architect
brian goetz clearly stated that the purpose of optional is to support the optional-
return idiom only.
Because the optional class wasnt intended for use as a field type, it doesnt imple-
ment the serializable interface. For this reason, using optionals in your domain
model could break applications with tools or frameworks that require a serializable
model to work. Nevertheless, we believe that weve showed you why using optionals
as a proper type in your domain is a good idea, especially when you have to traverse a
graph of objects that potentially arent present. Alternatively, if you need to have a seri-
alizable domain model, we suggest that you at least provide a method allowing access
to any possibly missing value as an optional, as in the following example:
public class person {
private car car;
public optional<car> getcarasoptional() {
return optional. Ofnullable(car);
}
}
patterns for adopting optionals 287

11.3.4 manipulating a stream of optionals
the optionals stream() method, introduced in java 9, allows you to convert an
optional with a value to a stream containing only that value or an empty optional
to an equally empty stream. This technique can be particularly convenient in a com-
mon case: when you have a stream of optional and need to transform it into
another stream containing only the values present in the nonempty optional of the
original stream. In this section, we demonstrate with another practical example why
you could find yourself having to deal with a stream of optional and how to per-
form this operation.
The example in listing 11.6 uses the person/car/insurance domain model
defined in listing 11.4, suppose that youre required to implement a method thats
passed with a list<person> and that should return a set<string> containing all
the distinct names of the insurance companies used by the people in that list who
own a car.

Listing 11.6 finding distinct insurance company names used by a list of persons

convert the list of persons into a flatmap each optional<car>
stream of optional<car> with the into the corresponding
cars eventually owned by them. Optional<insurance>.
Public set<string> getcarinsurancenames(list<person> persons) {
return persons. Stream()
. Map(person: : getcar)
. Map(optcar -> optcar. Flatmap(car: : getinsurance))
. Map(optins -> optins. Map(insurance: : getname))
. Flatmap(optional: : stream)
. Collect(toset()); map each
} optional<insurance> into
transform the the optional<string>
collect the result strings stream<optional<string>> containing the
into a set to obtain only into a stream<string> containing corresponding name.
The distinct values. Only the present names.

Often, manipulating the elements of a stream results in a long chain of transforma-
tions, filters, and other operations, but this case has an additional complication
because each element is also wrapped into an optional. Remember that you modeled
the fact that a person may not have a car by making its getcar() method return an
optional<car> instead of a simple car. So, after the first map transformation, you obtain
a stream<optional<car>>. At this point, the two subsequent maps allow you to trans-
form each optional<car> into an optional<insurance> and then each of them into an
optional<string> as you did in listing 11.5 for a single element instead of a stream.
At the end of these three transformations, you obtain a stream<optional<string>>
in which some of these optionals may be empty because a person doesnt own a car
or because the car isnt insured. The use of optionals allows you to perform these
operations in a completely null-safe way even in case of missing values, but now you
288 chapter 11 using optional as a better alternative to null

have the problem of getting rid of the empty optionals and unwrapping the values
contained in the remaining ones before collecting the results into a set. You could
have obtained this result with a filter followed by a map, of course, as follows:

stream<optional<string>> stream = ...
Set<string> result = stream. Filter(optional: : ispresent)
. Map(optional: : get)
. Collect(toset());

as anticipated in listing 11.6, however, its possible to achieve the same result in a sin-
gle operation instead of two by using the stream() method of the optional class.
Indeed, this method transforms each optional into a stream with zero or one ele-
ments, depending on whether the transformed optional is empty. For this reason, a
reference to that method can be seen as a function from a single element of the
stream to another stream and then passed to the flatmap method invoked on the
original stream. As youve already learned, in this way each element is converted to a
stream and then the two-level stream of streams is flattened into a single-level one.
This trick allows you to unwrap the optionals containing a value and skip the empty
ones in only one step.

11.3.5 default actions and unwrapping an optional
in section 11.3.3, you decided to read an optional value using the orelse method,
which allows you to also provide a default value that will be returned in the case of an
empty optional. The optional class provides several instance methods to read the
value contained by an optional instance:
 get() is the simplest but also the least safe of these methods. It returns the
wrapped value if one is present and throws a nosuchelementexception other-
wise. For this reason, using this method is almost always a bad idea unless youre
sure that the optional contains a value. In addition, this method isnt much of
an improvement on nested null checks.
 Orelse(t other) is the method used in listing 11.5, and as we noted there, it
allows you to provide a default value when the optional doesnt contain a value.
 Orelseget(supplier<? Extends t> other) is the lazy counterpart of the orelse
method, because the supplier is invoked only if the optional contains no value.
You should use this method when the default value is time-consuming to create
(to gain efficiency) or you want the supplier to be invoked only if the optional is
empty (when using orelseget is vital).
 Or(supplier<? Extends optional<? Extends t>> supplier) is similar to the
former orelseget method, but it doesnt unwrap the value inside the optional,
if present. In practice, this method (introduced with java 9) doesnt perform
any action and returns the optional as it is when it contains a value, but lazily
provides a different optional when the original one is empty.
Patterns for adopting optionals 289

 orelsethrow(supplier<? Extends x> exceptionsupplier) is similar to the get
method in that it throws an exception when the optional is empty, but it allows
you to choose the type of exception that you want to throw.
 Ifpresent(consumer<? Super t> consumer) lets you execute the action given as
argument if a value is present; otherwise, no action is taken.
Java 9 introduced an additional instance method:
 ifpresentorelse(consumer<? Super t> action, runnable emptyaction). This
differs from ifpresent by taking a runnable that gives an empty-based action to
be executed when the optional is empty.

11.3.6 combining two optionals
now suppose that you have a method that, given a person and a car, queries some
external services and implements some complex business logic to find the insurance
company that offers the cheapest policy for that combination:
public insurance findcheapestinsurance(person person, car car) {
// queries services provided by the different insurance companies
// compare all those data
return cheapestcompany;
}

also suppose that you want to develop a null-safe version of this method, taking two
optionals as arguments and returning an optional<insurance> that will be empty if
at least one of the values passed in to it is also empty. The optional class also provides
an ispresent method that returns true if the optional contains a value, so your first
attempt could be to implement this method as follows:
public optional<insurance> nullsafefindcheapestinsurance(
optional<person> person, optional<car> car) {
if (person. Ispresent() && car. Ispresent()) {
return optional. Of(findcheapestinsurance(person. Get(), car. Get()));
} else {
return optional. Empty();
}
}

this method has the advantage of making clear in its signature that both the person
and the car values passed to it could be missing and that for this reason, it couldnt
return any value. Unfortunately, its implementation resembles too closely the null
checks that youd write if the method took as arguments a person and a car, both of
which could be null. Is there a better, more idiomatic way to implement this method
by using the features of the optional class? Take a few minutes to go through quiz 11.1,
and try to find an elegant solution.
The analogies between the optional class and the stream interface arent limited
to the map and flatmap methods. A third method, filter, behaves in a similar fashion
on both classes, and we explore it next.
290 chapter 11 using optional as a better alternative to null

quiz 11.1: combining two optionals without unwrapping them
using a combination of the map and flatmap methods you learned in this section,
rewrite the implementation of the former nullsafefindcheapestinsurance()
method in a single statement.
Answer:
you can implement that method in a single statement and without using any condi-
tional constructs like the ternary operator as follows:

public optional<insurance> nullsafefindcheapestinsurance(
optional<person> person, optional<car> car) {
return person. Flatmap(p -> car. Map(c -> findcheapestinsurance(p, c)));
}

here, you invoke a flatmap on the first optional, so if this optional is empty, the
lambda expression passed to it wont be executed, and this invocation will return an
empty optional. Conversely, if the person is present, flatmap uses it as the input to
a function returning an optional<insurance> as required by the flatmap method.
The body of this function invokes a map on the second optional, so if it doesnt con-
tain any car, the function returns an empty optional, and so does the whole null-
safefindcheapestinsurance method. Finally, if both the person and the car are
present, the lambda expression passed as an argument to the map method can safely
invoke the original findcheapestinsurance method with them.

11.3.7 rejecting certain values with filter
often, you need to call a method on an object to check some property. You may need
to check whether the insurances name is equal to cambridgeinsurance, for example.
To do so in a safe way, first check whether the reference that points to an insurance
object is null and then call the getname method, as follows:
insurance insurance = ... ;
if(insurance ! = null && "cambridgeinsurance". Equals(insurance. Getname())){
system. Out. Println("ok");
}

you can rewrite this pattern by using the filter method on an optional object, as
follows:
optional<insurance> optinsurance = ... ;
optinsurance. Filter(insurance ->
"cambridgeinsurance". Equals(insurance. Getname()))
. Ifpresent(x -> system. Out. Println("ok"));

the filter method takes a predicate as an argument. If a value is present in the
optional object, and that value matches the predicate, the filter method returns
that value; otherwise, it returns an empty optional object. If you remember that you
patterns for adopting optionals 291

can think of an optional as being a stream containing at most a single element, the
behavior of this method should be clear. If the optional is already empty, it doesnt
have any effect; otherwise, it applies the predicate to the value contained in the
optional. If this application returns true, the optional returns unchanged; otherwise,
the value is filtered away, leaving the optional empty. You can test your understanding
of how the filter method works by working through quiz 11.2.

Quiz 11.2: filtering an optional
supposing that the person class of your person/car/insurance model also has a
method getage to access the age of the person, modify the getcarinsurancename
method in listing 11.5 by using the signature
public string getcarinsurancename(optional<person> person, int minage)

so that the insurance company name is returned only if the person has an age greater
than or equal to the minage argument.
Answer:
you can filter the optional<person>, to remove any contained person whose age
fails to be at least the minage argument, by encoding this condition in a predicate
passed to the filter method as follows:
public string getcarinsurancename(optional<person> person, int minage) {
return person. Filter(p -> p. Getage() >= minage)
. Flatmap(person: : getcar)
. Flatmap(car: : getinsurance)
. Map(insurance: : getname)
. Orelse("unknown");
}

in the next section, we investigate the remaining features of the optional class and
provide more practical examples of various techniques you could use to reimplement
the code you write to manage missing values.
Table 11.1 summarizes the methods of the optional class.

Table 11.1 the methods of the optional class

method description

empty returns an empty optional instance

filter if the value is present and matches the given predicate, returns this optional;
otherwise, returns the empty one

flatmap if a value is present, returns the optional resulting from the application of the
provided mapping function to it; otherwise, returns the empty optional

get returns the value wrapped by this optional if present; otherwise, throws a
nosuchelementexception
292 chapter 11 using optional as a better alternative to null

table 11.1 the methods of the optional class (continued)

method description

ifpresent if a value is present, invokes the specified consumer with the value; otherwise,
does nothing

ifpresentorelse if a value is present, performs an action with the value as input; otherwise, per-
forms a different action with no input

ispresent returns true if a value is present; otherwise, returns false

map if a value is present, applies the provided mapping function to it

of returns an optional wrapping the given value or throws a nullpointer-
exception if this value is null

ofnullable returns an optional wrapping the given value or the empty optional if this
value is null

or if the value is present, returns the same optional; otherwise, returns
another optional produced by the supplying function

orelse returns the value if present; otherwise, returns the given default value

orelseget returns the value if present; otherwise, returns the one provided by the given
supplier

orelsethrow returns the value if present; otherwise, throws the exception created by the
given supplier

stream if a value is present, returns a stream containing only it; otherwise, returns an
empty stream

11.4 practical examples of using optional
as youve learned, effective use of the new optional class implies a complete rethink
of how you deal with potentially missing values. This rethink involves not only the
code you write, but also (and possibly even more important) how you interact with
native java apis.
Indeed, we believe that many of those apis would have been written differently if
the optional class had been available when they were developed. For backward-com-
patibility reasons, old java apis cant be changed to make proper use of optionals, but
all is not lost. You can fix, or at least work around, this issue by adding to your code
small utility methods that allow you to benefit from the power of optionals. You see
how to do this with a couple of practical examples.

11.4.1 wrapping a potentially null value in an optional
an existing java api almost always returns a null to signal that the required value is
absent or that the computation to obtain it failed for some reason. The get method of
a map returns null as its value if it contains no mapping for the requested key, for
example. But for the reasons we listed earlier, in most cases like this one, you prefer
practical examples of using optional 293

for these methods to return an optional. You cant modify the signature of these meth-
ods, but you can easily wrap the value they return with an optional. Continuing with
the map example, and supposing that you have a map<string, object>, accessing the
value indexed by key with

object value = map. Get("key");

returns null if theres no value in the map associated with the string "key". You can
improve such code by wrapping in an optional the value returned by the map. You
can either add an ugly if-then-else that adds to code complexity, or you can use the
method optional. Ofnullable that we discussed earlier:

optional<object> value = optional. Ofnullable(map. Get("key"));

you can use this method every time you want to safely transform a value that could be
null into an optional.

11.4.2 exceptions vs. Optional
throwing an exception is another common alternative in the java api to returning a
null when a value cant be provided. A typical example is the conversion of string
into an int provided by the integer. Parseint(string) static method. In this case, if
the string doesnt contain a parseable integer, this method throws a numberformat-
exception. Once again, the net effect is that the code signals an invalid argument if a
string doesnt represent an integer, the only difference being that this time, you have
to check it with a try/catch block instead of using an if condition to control whether
a value isnt null.
You could also model the invalid value caused by nonconvertible strings with an
empty optional, so you prefer that parseint returns an optional. You cant change the
original java method, but nothing prevents you from implementing a tiny utility
method, wrapping it, and returning an optional as desired, as shown in the follow-
ing listing.

Listing 11.7 converting a string to an integer returning an optional

public static optional<integer> stringtoint(string s) {
try { if the string can
return optional. Of(integer. Parseint(s));
be converted to an
integer, return an
} catch (numberformatexception e) {
optional containing it.
Return optional. Empty();
otherwise,
} return an
} empty optional.

Our suggestion is to collect several similar methods in a utility class, which you can call
optionalutility. From then on, youll always be allowed to convert a string to an
optional<integer> by using this optionalutility. Stringtoint method. You can
forget that you encapsulated the ugly try/catch logic in it.
294 chapter 11 using optional as a better alternative to null

11.4.3 primitive optionals and why you shouldnt use them
note that like streams, optionals also have primitive counterpartsoptionalint,
optionallong, and optionaldoubleso the method in listing 11.7 could have returned
optionalint instead of optional<integer>. In chapter 5, we encouraged the use of
primitive streams (especially when they could contain a huge number of elements)
for performance reasons, but because an optional can have at most a single value,
that justification doesnt apply here.
We discourage using primitive optionals because they lack the map, flatmap, and
filter methods, which (as you saw in section 11.2) are the most useful methods of
the optional class. Moreover, as happens for streams, an optional cant be composed
with its primitive counterpart, so if the method of listing 11.7 returned optionalint,
you couldnt pass it as a method reference to the flatmap method of another optional.

11.4.4 putting it all together
in this section, we demonstrate how the methods of the optional class that weve pre-
sented so far can be used together in a more compelling use case. Suppose that you
have some properties that are passed as configuration arguments to your program.
For the purpose of this example and to test the code youll develop, create some sam-
ple properties as follows:
properties props = new properties();
props. Setproperty("a", "5");
props. Setproperty("b", "true");
props. Setproperty("c", "-3");

also suppose that your program needs to read a value from these properties and
interpret it as a duration in seconds. Because a duration has to be a positive (>0) num-
ber, youll want a method with the signature

public int readduration(properties props, string name)

so that when the value of a given property is a string representing a positive integer,
the method returns that integer, but it returns zero in all other cases. To clarify this
requirement, formalize it with a few junit assertions:
assertequals(5, readduration(param, "a"));
assertequals(0, readduration(param, "b"));
assertequals(0, readduration(param, "c"));
assertequals(0, readduration(param, "d"));

these assertions reflect the original requirement: the readduration method returns
5 for the property "a" because the value of this property is a string thats convertible
in a positive number, and the method returns 0 for "b" because it isnt a number,
returns 0 for "c" because its a number but is negative, and returns 0 for "d" because
practical examples of using optional 295

a property with that name doesnt exist. Try to implement the method that satisfies
this requirement in imperative style, as shown in the next listing.

Listing 11.8 reading duration from a property imperatively

public int readduration(properties props, string name) { make sure that a
string value = props. Getproperty(name); property exists with
if (value ! = null) { the required name.
Try {
try to convert the string
int i = integer. Parseint(value);
property to a number.
If (i > 0) {
return i; check whether the resulting
} number is positive.
} catch (numberformatexception nfe) { }
}
return 0;
return 0 if any of
} the conditions fails.

As you might expect, the resulting implementation is convoluted and not readable,
presenting multiple nested conditions coded as both if statements and a try/catch
block. Take a few minutes to figure out in quiz 11.3 how you can achieve the same
result by using what youve learned in this chapter.

Quiz 11.3: reading duration from a property by using an optional
using the features of the optional class and the utility method of listing 11.7, try to
reimplement the imperative method of listing 11.8 with a single fluent statement.
Answer:
because the value returned by the properties. Getproperty(string) method is a
null when the required property doesnt exist, its convenient to turn this value
into an optional with the ofnullable factory method. Then you can convert the
optional<string> to an optional<integer>, passing to its flatmap method a ref-
erence to the optionalutility. Stringtoint method developed in listing 11.7.
Finally, you can easily filter away the negative number. In this way, if any of these
operations returns an empty optional, the method returns the 0 thats passed as the
default value to the orelse method; otherwise, it returns the positive integer con-
tained in the optional. This description is implemented as follows:
public int readduration(properties props, string name) {
return optional. Ofnullable(props. Getproperty(name))
. Flatmap(optionalutility: : stringtoint)
. Filter(i -> i > 0)
. Orelse(0);
}

note the common style in using optionals and streams; both are reminiscent of a data-
base query in which several operations are chained together.
296 chapter 11 using optional as a better alternative to null

summary
 null references were historically introduced in programming languages to sig-
nal the absence of a value.
 Java 8 introduced the class java. Util. Optional<t> to model the presence or
absence of a value.
 You can create optional objects with the static factory methods optional. Empty,
optional. Of, and optional. Ofnullable.
 The optional class supports many methodssuch as map, flatmap, and filter
that are conceptually similar to the methods of a stream.
 Using optional forces you to actively unwrap an optional to deal with the
absence of a value; as a result, you protect your code against unintended null
pointer exceptions.
 Using optional can help you design better apis in which, by reading the signa-
ture of a method, users can tell whether to expect an optional value.
New date and time api

this chapter covers
 why we needed a new date and time library,
introduced in java 8
 representing date and time for both humans
and machines
 defining an amount of time
 manipulating, formatting, and parsing dates
 dealing with different time zones and calendars

the java api includes many useful components to help you build complex applica-
tions. Unfortunately, the java api isnt always perfect. We believe that the majority
of experienced java developers will agree that date and time support before java 8
was far from ideal. Dont worry, though; java 8 introduces a brand-new date and
time api to tackle this issue.
In java 1.0, the only support for date and time was the java. Util. Date class.
Despite its name, this class doesnt represent a date, but a point in time with milli-
second precision. Even worse, the usability of this class is harmed by some nebulous
design decisions such as the choice of its offsets: the years start from 1900, whereas

297
298 chapter 12 new date and time api

the months start at index 0. If you wanted to represent the release date of java 9,
which is 21 september 2017, youd have to create an instance of date as follows:
date date = new date(117, 8, 21);

printing this date produces, for the authors:
thu sep 21 00:00:00 cet 2017

not very intuitive, is it? Moreover, even the string returned by the tostring method
of the date class could be quite misleading. It also includes the jvms default time
zone, cet, which is central europe time in our case. Indeed, the date class itself
merely inserts the jvm default time zone!
The problems and limitations of the date class were immediately clear when java
1.0 came out, but it was also clear that the problems couldnt be fixed without break-
ing its backward compatibility. As a consequence, in java 1.1 many methods of the
date class were deprecated, and the class was replaced by the alternative java. Util
. Calendar class. Unfortunately, calendar has similar problems and design flaws that
lead to error-prone code. Months also start at index 0. (at least calendar got rid of
the 1900 offset for the year. ) worse, the presence of both the date and calendar
classes increases confusion among developers. (which one should you use? ) in addi-
tion, features such as dateformat, used to format and parse dates or time in a lan-
guage-independent manner, work only with the date class.
The dateformat comes with its own set of problems. It isnt thread-safe, for exam-
ple, which means that if two threads try to parse a date by using the same formatter at
the same time, you may receive unpredictable results.
Finally, both date and calendar are mutable classes. What does it mean to mutate
the 21st of september 2017 to the 25th of october? This design choice can lead you
into a maintenance nightmare, as youll learn in more detail in chapter 18, which is
about functional programming.
The consequence is that all these flaws and inconsistencies have encouraged the
use of third-party date and time libraries, such as joda-time. For these reasons, oracle
decided to provide high-quality date and time support in the native java api. As a
result, java 8 integrates many of the joda-time features in the java. Time package.
In this chapter, we explore the features introduced by the new date and time api.
We start with basic use cases such as creating dates and times that are suitable to be
used by both humans and machines. Then we gradually explore more-advanced appli-
cations of the new date and time api, such as manipulating, parsing, and printing
date-time objects, and working with different time zones and alternative calendars.

12.1 localdate, localtime, localdatetime, instant,
duration, and period
we start by exploring how to create simple dates and intervals. The java. Time pack-
age includes many new classes to help you: localdate, localtime, localdatetime,
instant, duration, and period.
Localdate, localtime, localdatetime, instant, duration, and period 299

12.1.1 working with localdate and localtime
the class localdate probably is the first one youll come across when you start using
the new date and time api. An instance of this class is an immutable object repre-
senting a plain date without the time of day. In particular, it doesnt carry any informa-
tion about the time zone.
You can create a localdate instance by using the of static factory method. A
localdate instance provides many methods to read its most commonly used values
(year, month, day of the week, and so on), as shown in the following listing.

Listing 12.1 creating a localdate and reading its values

localdate date = localdate. Of(2017, 9, 21); 2017-09-21
int year = date. Getyear(); 2017
month month = date. Getmonth(); september
int day = date. Getdayofmonth(); 21
dayofweek dow = date. Getdayofweek(); thursday
int len = date. Lengthofmonth(); 30 (days in september)
boolean leap = date. Isleapyear(); false (not a leap year)

its also possible to obtain the current date from the system clock by using the now fac-
tory method:

localdate today = localdate. Now();

all the other date-time classes that we investigate in the remaining part of this chapter
provide a similar factory method. You can also access the same information by passing
a temporalfield to the get method. The temporalfield is an interface defining how
to access the value of a specific field of a temporal object. The chronofield enumera-
tion implements this interface, so you can conveniently use an element of that enu-
meration with the get method, as shown in the next listing.

Listing 12.2 reading localdate values by using a temporalfield

int year = date. Get(chronofield. Year);
int month = date. Get(chronofield. Month_of_year);
int day = date. Get(chronofield. Day_of_month);

you could use the built-in getyear(), getmonthvalue(), and getdayofmonth() meth-
ods in a more-readable form to access the information as follows:
int year = date. Getyear();
int month = date. Getmonthvalue();
int day = date. Getdayofmonth();

similarly, the time of day, such as 13:45:20, is represented by the localtime class. You
can create instances of localtime by using two overloaded static factory methods
300 chapter 12 new date and time api

named of. The first one accepts an hour and a minute, and the second one also
accepts a second. Like the localdate class, the localtime class provides some getter
methods to access its values, as shown in the following listing.

Listing 12.3 creating a localtime and reading its values

localtime time = localtime. Of(13, 45, 20); 13:45:20
int hour = time. Gethour(); 13
int minute = time. Getminute(); 45
int second = time. Getsecond(); 20

you can create both localdate and localtime by parsing a string representing
them. To achieve this task, use their parse static methods:
localdate date = localdate. Parse("2017-09-21");
localtime time = localtime. Parse("13:45:20");

its possible to pass a datetimeformatter to the parse method. An instance of this
class specifies how to format a date and/or a time object. Its intended to be a replace-
ment for the old java. Util. Dateformat that we mentioned earlier. We show in more
detail how you can use a datetimeformatter in section 12.2.2. Also note that both
these parse methods throw a datetimeparseexception, which extends runtime-
exception in case the string argument cant be parsed as a valid localdate or
localtime.

12.1.2 combining a date and a time
the composite class called localdatetime pairs a localdate and a localtime. It rep-
resents both a date and a time without a time zone and can be created directly or by
combining a date and time, as shown in the following listing.

Listing 12.4 creating a localdatetime directly or by combining a date and a time

// 2017-09-21t13:45:20
localdatetime dt1 = localdatetime. Of(2017, month. September, 21, 13, 45, 20);
localdatetime dt2 = localdatetime. Of(date, time);
localdatetime dt3 = date. Attime(13, 45, 20);
localdatetime dt4 = date. Attime(time);
localdatetime dt5 = time. Atdate(date);

note that its possible to create a localdatetime by passing a time to a localdate or a
date to a localtime, using their attime or atdate methods, respectively. You can also
extract the localdate or localtime component from a localdatetime by using the
tolocaldate and tolocaltime methods:

localdate date1 = dt1. Tolocaldate(); 2017-09-21
localtime time1 = dt1. Tolocaltime(); 13:45:20
localdate, localtime, localdatetime, instant, duration, and period 301

12.1.3 instant: a date and time for machines
as humans, were used to thinking of dates and time in terms of weeks, days, hours,
and minutes. Nonetheless, this representation isnt easy for a computer to deal with.
From a machine point of view, the most natural format to model time is a single large
number representing a point on a continuous timeline. This approach is used by the
new java. Time. Instant class, which represents the number of seconds passed since
the unix epoch time, set by convention to midnight of january 1, 1970 utc.
You can create an instance of this class by passing the number of seconds to its
ofepochsecond static factory method. In addition, the instant class supports nano-
second precision. A supplementary overloaded version of the ofepochsecond static
factory method accepts a second argument thats a nanosecond adjustment to the
passed number of seconds. This overloaded version adjusts the nanosecond argu-
ment, ensuring that the stored nanosecond fraction is between 0 and 999,999,999. As
a result, the following invocations of the ofepochsecond factory method return
exactly the same instant:
one billion nanoseconds
instant. Ofepochsecond(3); (1 second) after 2 seconds
instant. Ofepochsecond(3, 0);
one billion nanoseconds
instant. Ofepochsecond(2, 1_000_000_000);
(1 second) before 4 seconds
instant. Ofepochsecond(4, -1_000_000_000);

as youve already seen for localdate and the other human-readable date-time classes,
the instant class supports another static factory method named now, which allows you
to capture a timestamp of the current moment. Its important to reinforce that an
instant is intended for use only by a machine. It consists of a number of seconds and
nanoseconds. As a consequence, it doesnt provide any ability to handle units of time
that are meaningful to humans. A statement like

int day = instant. Now(). Get(chronofield. Day_of_month);

throws an exception like this:

java. Time. Temporal. Unsupportedtemporaltypeexception: unsupported field:
dayofmonth

but you can work with instants by using the duration and period classes, which we
look at next.

12.1.4 defining a duration or a period
all the classes youve seen so far implement the temporal interface, which defines
how to read and manipulate the values of an object modeling a generic point in time.
Weve shown you a few ways to create different temporal instances. The next natural
step is creating a duration between two temporal objects. The between static factory
302 chapter 12 new date and time api

method of the duration class serves exactly this purpose. You can create a duration
between two localtimes, two localdatetimes, or two instants as follows:
duration d1 = duration. Between(time1, time2);
duration d1 = duration. Between(datetime1, datetime2);
duration d2 = duration. Between(instant1, instant2);

because localdatetime and instant are made for different purposes, one to be used
by humans and the other by machines, youre not allowed to mix them. If you try to
create a duration between them, youll only obtain a datetimeexception. Moreover,
because the duration class is used to represent an amount of time measured in sec-
onds and eventually nanoseconds, you cant pass a localdate to the between method.
When you need to model an amount of time in terms of years, months, and days,
you can use the period class. You can find out the difference between two localdates
with the between factory method of that class:
period tendays = period. Between(localdate. Of(2017, 9, 11),
localdate. Of(2017, 9, 21));

finally, the duration and period classes have other convenient factory methods to
create instances of them directly, without defining them as the difference between two
temporal objects, as shown in the following listing.

Listing 12.5 creating durations and periods

duration threeminutes = duration. Ofminutes(3);
duration threeminutes = duration. Of(3, chronounit. Minutes);
period tendays = period. Ofdays(10);
period threeweeks = period. Ofweeks(3);
period twoyearssixmonthsoneday = period. Of(2, 6, 1);

the duration and period classes share many similar methods, which table 12.1 lists.

Table 12.1 the common methods of date-time classes representing an interval

method static description

between yes creates an interval between two points in time

from yes creates an interval from a temporal unit

of yes creates an instance of this interval from its constituent parts

parse yes creates an instance of this interval from a string

addto no creates a copy of this interval, adding to it the specified temporal object

get no reads part of the state of this interval

isnegative no checks whether this interval is negative, excluding zero

iszero no checks whether this interval is zero-length
manipulating, parsing, and formatting dates 303

table 12.1 the common methods of date-time classes representing an interval (continued)

method static description

minus no creates a copy of this interval with an amount of time subtracted

multipliedby no creates a copy of this interval multiplied by the given scalar

negated no creates a copy of this interval with the length negated

plus no creates a copy of this interval with an amount of time added

subtractfrom no subtracts this interval from the specified temporal object

all the classes weve investigated so far are immutable, which is a great design choice
to allow a more functional programming style, ensure thread safety, and preserve the
consistency of the domain model. Nevertheless, the new date and time api offers
some handy methods for creating modified versions of those objects. You may want to
add three days to an existing localdate instance, for example, and we explore how to
do this in the next section. In addition, we explore how to create a date-time format-
ter from a given pattern, such as dd/mm/yyyy, or even programmatically, as well as
how to use this formatter for both parsing and printing a date.

12.2 manipulating, parsing, and formatting dates
the most immediate and easiest way to create a modified version of an existing
localdate is to change one of its attributes, using one of its withattribute methods.
Note that all the methods return a new object with the modified attribute, as shown in
listing 12.6; they dont mutate the existing object!

Listing 12.6 manipulating the attributes of a localdate in an absolute way

localdate date1 = localdate. Of(2017, 9, 21); 2017-09-21
localdate date2 = date1. Withyear(2011); 2011-09-21
localdate date3 = date2. Withdayofmonth(25); 2011-09-25
localdate date4 = date3. With(chronofield. Month_of_year, 2); 2011-02-25

you can do the same thing with the more generic with method, taking a temporal-
field as the first argument, as in the last statement of listing 12.6. This last with
method is the dual of the get method used in listing 12.2. Both of these methods are
declared in the temporal interface implemented by all the classes, such as localdate,
localtime, localdatetime, and instant, of the date and time api. More precisely,
the get and with methods let you respectively read and modify1 fields of a temporal
object. They throw an unsupportedtemporaltypeexception if the requested field

1
remember that such with methods dont modify the existing temporal object but create a copy with the
specific field updated. This process is called a functional update (see chapter 19).
304 chapter 12 new date and time api

isnt supported by the specific temporal, such as a chronofield. Month_of_year on an
instant or a chronofield. Nano_of_second on a localdate.
Its even possible to manipulate a localdate in a declarative manner. You can add
or subtract a given amount of time, for example, as shown in listing 12.7.

Listing 12.7 manipulating the attributes of a localdate in a relative way

localdate date1 = localdate. Of(2017, 9, 21); 2017-09-21
localdate date2 = date1. Plusweeks(1); 2017-09-28
localdate date3 = date2. Minusyears(6); 2011-09-28
localdate date4 = date3. Plus(6, chronounit. Months); 2012-03-28

similarly to what weve explained about the with and get methods, the generic
plus method used in the last statement of listing 12.7, together with the analogous
minus method, is declared in the temporal interface. These methods allow you to
move a temporal back or forward a given amount of time, defined by a number plus
a temporalunit, where the chronounit enumeration offers a convenient implemen-
tation of the temporalunit interface.
As you may have anticipated, all the date-time classes representing a point in time
such as localdate, localtime, localdatetime, and instant have many methods in
common. Table 12.2 summarizes these methods.

Table 12.2 the common methods of date-time classes representing a point in time

method static description

from yes creates an instance of this class from the passed temporal object

now yes creates a temporal object from the system clock

of yes creates an instance of this temporal object from its constituent parts

parse yes creates an instance of this temporal object from a string

atoffset no combines this temporal object with a zone offset

atzone no combines this temporal object with a time zone

format no converts this temporal object to a string by using the specified formatter
(not available for instant)

get no reads part of the state of this temporal object

minus no creates a copy of this temporal object with an amount of time subtracted

plus no creates a copy of this temporal object with an amount of time added

with no creates a copy of this temporal object with part of the state changed

check what youve learned up to now about manipulating dates with quiz 12.1.
Manipulating, parsing, and formatting dates 305

quiz 12.1: manipulating a localdate
what will the value of the date variable be after the following manipulations?
Localdate date = localdate. Of(2014, 3, 18);
date = date. With(chronofield. Month_of_year, 9);
date = date. Plusyears(2). Minusdays(10);
date. Withyear(2011);

answer:
2016-09-08

as youve seen, you can manipulate the date both in an absolute way and in a relative
way. You can also concatenate more manipulations in a single statement, because
every change creates a new localdate object, and the subsequent invocation
manipulates the object created by the former one. Finally, the last statement in this
code snippet has no observable effect because as usual, it creates a new localdate
instance, but were not assigning this new value to any variable.

12.2.1 working with temporaladjusters
all the date manipulations youve seen so far are relatively straightforward. Some-
times, though, you need to perform advanced operations, such as adjusting a date to
the next sunday, the next working day, or the last day of the month. In such cases,
you can pass to an overloaded version of the with method a temporaladjuster that
provides a more customizable way to define the manipulation needed to operate on
a specific date. The date and time api already provides many predefined temporal-
adjusters for the most common use cases. You can access them by using the static fac-
tory methods contained in the temporaladjusters class, as shown in listing 12.8.

Listing 12.8 using the predefined temporaladjusters

import static java. Time. Temporal. Temporaladjusters. *;
localdate date1 = localdate. Of(2014, 3, 18); 2014-03-18
localdate date2 = date1. With(nextorsame(dayofweek. Sunday)); 2014-03-23
localdate date3 = date2. With(lastdayofmonth()); 2014-03-31

table 12.3 lists the temporaladjusters that you can create with these factory methods.

Table 12.3 the factory methods of the temporaladjusters class

method description

dayofweekinmonth creates a new date in the same month with the ordinal day of week. (neg-
ative numbers count backward from the end of the month. )

firstdayofmonth creates a new date set to the first day of the current month.
306 chapter 12 new date and time api

table 12.3 the factory methods of the temporaladjusters class (continued)

method description

firstdayofnextmonth creates a new date set to the first day of the next month.

Firstdayofnextyear creates a new date set to the first day of the next year.

Firstdayofyear creates a new date set to the first day of the current year.

Firstinmonth creates a new date in the same month with the first matching day of
the week.

Lastdayofmonth creates a new date set to the last day of the current month.

Lastdayofnextmonth creates a new date set to the last day of the next month.

Lastdayofnextyear creates a new date set to the last day of the next year.

Lastdayofyear creates a new date set to the last day of the current year.

Lastinmonth creates a new date in the same month with the last matching day of
the week.

Next creates a new date set to the first occurrence of the specified day of
previous week after/before the date being adjusted.

Nextorsame creates a new date set to the first occurrence of the specified day of
previousorsame week after/before the date being adjusted unless its already on that day,
in which case the same object is returned.

As you can see, temporaladjusters allow you to perform more-complex date manip-
ulations that still read like the problem statement. Moreover, its relatively simple to
create your own custom temporaladjuster implementation if you cant find a pre-
defined temporaladjuster that fits your needs. In fact, the temporaladjuster inter-
face declares only a single method (which makes it a functional interface), defined as
shown in the following listing.

Listing 12.9 the temporaladjuster interface

@functionalinterface
public interface temporaladjuster {
temporal adjustinto(temporal temporal);
}

this example means that an implementation of the temporaladjuster interface
defines how to convert a temporal object to another temporal. You can think of a
temporaladjuster as being like a unaryoperator<temporal>. Take a few minutes to
practice what youve learned so far and implement your own temporaladjuster in
quiz 12.2.
Manipulating, parsing, and formatting dates 307

quiz 12.2: implementing a custom temporaladjuster
develop a class named nextworkingday, implementing the temporaladjuster
interface that moves a date forward by one day but skips saturdays and sundays.
Using

date = date. With(new nextworkingday());

should move the date to the next day, if this day is between monday and friday, but
to the next monday if its a saturday or a sunday.
Answer:
you can implement the nextworkingday adjuster as follows:

public class nextworkingday implements temporaladjuster {
@override read the
public temporal adjustinto(temporal temporal) { current day.
Dayofweek dow =
dayofweek. Of(temporal. Get(chronofield. Day_of_week));
int daytoadd = 1;
normally but add
add one if (dow == dayofweek. Friday) daytoadd = 3;
else if (dow == dayofweek. Saturday) daytoadd = 2;
three days
day. If today is
return temporal. Plus(daytoadd, chronounit. Days);
a friday.
}
} return the modified
add two days
date adding the right
number of days. If today is a
saturday.

This temporaladjuster normally moves a date forward one day, except when today
is a friday or saturday, in which case it advances the dates by three or two days,
respectively. Note that because a temporaladjuster is a functional interface, you
could pass the behavior of this adjuster in a lambda expression:
date = date. With(temporal -> {
dayofweek dow =
dayofweek. Of(temporal. Get(chronofield. Day_of_week));
int daytoadd = 1;
if (dow == dayofweek. Friday) daytoadd = 3;
else if (dow == dayofweek. Saturday) daytoadd = 2;
return temporal. Plus(daytoadd, chronounit. Days);
});

youre likely to want to apply this manipulation to a date in several points of your code,
and for this reason, we suggest encapsulating its logic in a proper class, as we did
here. Do the same for all the manipulations you use frequently. Youll end up with a
small library of adjusters that you and your team can easily reuse in your codebase.
If you want to define the temporaladjuster with a lambda expression, its preferable
to do so by using the ofdateadjuster static factory of the temporaladjusters
class, which accepts a unaryoperator<localdate> as follows:
308 chapter 12 new date and time api

(continued)
temporaladjuster nextworkingday = temporaladjusters. Ofdateadjuster(
temporal -> {
dayofweek dow =
dayofweek. Of(temporal. Get(chronofield. Day_of_week));
int daytoadd = 1;
if (dow == dayofweek. Friday) daytoadd = 3;
else if (dow == dayofweek. Saturday) daytoadd = 2;
return temporal. Plus(daytoadd, chronounit. Days);
});
date = date. With(nextworkingday);

another common operation that you may want to perform on your date and time
objects is printing them in different formats specific to your business domain. Simi-
larly, you may want to convert strings representing dates in those formats to actual
date objects. In the next section, we demonstrate the mechanisms provided by the
new date and time api to accomplish these tasks.

12.2.2 printing and parsing date-time objects
formatting and parsing are other relevant features for working with dates and times.
The new java. Time. Format package is devoted to these purposes. The most important
class of this package is datetimeformatter. The easiest way to create a formatter is
through its static factory methods and constants. The constants such as basic_iso_date
and iso_local_date are predefined instances of the datetimeformatter class. You
can use all datetimeformatters to create a string representing a given date or time
in a specific format. Here, for example, we produce a string by using two different
formatters:
localdate date = localdate. Of(2014, 3, 18);
string s1 = date. Format(datetimeformatter. Basic_iso_date); 20140318
string s2 = date. Format(datetimeformatter. Iso_local_date); 2014-03-18

you can also parse a string representing a date or a time in that format to re-create
the date object itself. You can achieve this task by using the parse factory method pro-
vided by all the classes of the date and time api representing a point in time or an
interval:
localdate date1 = localdate. Parse("20140318",
datetimeformatter. Basic_iso_date);
localdate date2 = localdate. Parse("2014-03-18",
datetimeformatter. Iso_local_date);

in comparison with the old java. Util. Dateformat class, all the datetimeformatter
instances are thread-safe. Therefore, you can create singleton formatters like the ones
defined by the datetimeformatter constants and share them among multiple threads.
Manipulating, parsing, and formatting dates 309

the next listing shows how the datetimeformatter class also supports a static factory
method that lets you create a formatter from a specific pattern.

Listing 12.10 creating a datetimeformatter from a pattern

datetimeformatter formatter = datetimeformatter. Ofpattern("dd/mm/yyyy");
localdate date1 = localdate. Of(2014, 3, 18);
string formatteddate = date1. Format(formatter);
localdate date2 = localdate. Parse(formatteddate, formatter);

here, the localdates format method produces a string representing the date with
the requested pattern. Next, the static parse method re-creates the same date by pars-
ing the generated string, using the same formatter. The ofpattern method also has
an overloaded version that allows you to create a formatter for a given locale, as
shown in the following listing.

Listing 12.11 creating a localized datetimeformatter

datetimeformatter italianformatter =
datetimeformatter. Ofpattern("d. Mmmm yyyy", locale. Italian);
localdate date1 = localdate. Of(2014, 3, 18);
string formatteddate = date. Format(italianformatter); // 18. Marzo 2014
localdate date2 = localdate. Parse(formatteddate, italianformatter);

finally, in case you need even more control, the datetimeformatterbuilder class lets
you define complex formatters step by step by using meaningful methods. In addition,
it provides you the ability to have case-insensitive parsing, lenient parsing (allowing
the parser to use heuristics to interpret inputs that dont precisely match the specified
format), padding, and optional sections of the formatter. You can programmatically
build the same italianformatter we used in listing 12.11 through the datetime-
formatterbuilder, for example, as shown in the following listing.

Listing 12.12 building a datetimeformatter

datetimeformatter italianformatter = new datetimeformatterbuilder()
. Appendtext(chronofield. Day_of_month)
. Appendliteral(". ")
. Appendtext(chronofield. Month_of_year)
. Appendliteral(" ")
. Appendtext(chronofield. Year)
. Parsecaseinsensitive()
. Toformatter(locale. Italian);

so far, youve learned how to create, manipulate, format, and parse both points in
time and intervals, but you havent seen how to deal with subtleties involving dates
and time. You may need to deal with different time zones or alternative calendar
systems. In the next sections, we explore these topics by using the new date and
time api.
310 chapter 12 new date and time api

12.3 working with different time zones and calendars
none of the classes youve seen so far contain any information about time zones. Deal-
ing with time zones is another important issue thats been vastly simplified by the new
date and time api. The new java. Time. Zoneid class is the replacement for the old
java. Util. Timezone class. It aims to better shield you from the complexities related
to time zones, such as dealing with daylight saving time (dst). Like the other classes
of the date and time api, its immutable.

12.3.1 using time zones
a time zone is a set of rules corresponding to a region in which the standard time is the
same. About 40 time zones are held in instances of the zonerules class. You can call
getrules() on a zoneid to obtain the rules for that time zone. A specific zoneid is
identified by a region id, as in this example:

zoneid romezone = zoneid. Of("europe/rome");

all the region ids are in the format "{area}/{city}", and the set of available loca-
tions is the one supplied by the internet assigned numbers authority (iana) time
zone database (see https: //www. Iana. Org/time-zones). You can also convert an old
timezone object to a zoneid by using the new method tozoneid:

zoneid zoneid = timezone. Getdefault(). Tozoneid();

when you have a zoneid object, you can combine it with a localdate, a localdate-
time, or an instant to transform it into zoneddatetime instances, which represent
points in time relative to the specified time zone, as shown in the following listing.

Listing 12.13 applying a time zone to a point in time

localdate date = localdate. Of(2014, month. March, 18);
zoneddatetime zdt1 = date. Atstartofday(romezone);
localdatetime datetime = localdatetime. Of(2014, month. March, 18, 13, 45);
zoneddatetime zdt2 = datetime. Atzone(romezone);
instant instant = instant. Now();
zoneddatetime zdt3 = instant. Atzone(romezone);

figure 12.1 illustrates the components of a zoneddatetime to help you understand
the differences among localdate, localtime, localdatetime, and zoneid.

2014-05-14t15:33:05.941+01:00[europe/london]
localdate localtime zoneid

localdatetime

zoneddatetime

figure 12.1 making sense of a zoneddatetime
working with different time zones and calendars 311

you can also convert a localdatetime to an instant by using a zoneid:
localdatetime datetime = localdatetime. Of(2014, month. March, 18, 13, 45);
instant instantfromdatetime = datetime. Toinstant(romezone);

or you can do it the other way around:
instant instant = instant. Now();
localdatetime timefrominstant = localdatetime. Ofinstant(instant, romezone);

note that working with instant is quite useful because you often have to work with leg-
acy code that deals with the date class. There, two methods were added to help inter-
operate between the deprecated api and the new date and time api: toinstant() and
the static method frominstant().

12.3.2 fixed offset from utc/greenwich
another common way to express a time zone is to use a fixed offset from utc/green-
wich. You can use this notation to say, new york is five hours behind london, for
example. In cases like this one, you can use the zoneoffset class, a subclass of zoneid
that represents the difference between a time and the zero meridian of greenwich,
london, as follows:
zoneoffset newyorkoffset = zoneoffset. Of("-05:00");

the -05:00 offset indeed corresponds to u. S. Eastern standard time. Be aware, how-
ever, that a zoneoffset defined this way doesnt have any daylight saving time man-
agement, and for this reason, it isnt suggested in the majority of cases. Because a
zoneoffset is also a zoneid, you can use it as shown in listing 12.13 earlier in this
chapter. You can also create an offsetdatetime, which represents a date-time with an
offset from utc/greenwich in the iso-8601 calendar system:
localdatetime datetime = localdatetime. Of(2014, month. March, 18, 13, 45);
offsetdatetime datetimeinnewyork = offsetdatetime. Of(date, newyorkoffset);

another advanced feature supported by the new date and time api is support for
non-iso calendaring systems.

12.3.3 using alternative calendar systems
the iso-8601 calendar system is the de facto world civil calendar system. But four
additional calendar systems are provided in java 8. Each of these calendar systems has
a dedicated date class: thaibuddhistdate, minguodate, japanesedate, and hijrah-
date. All these classes, together with localdate, implement the chronolocaldate
interface, which is intended to model a date in an arbitrary chronology. You can cre-
ate an instance of one of these classes out of a localdate. More generally, you can create
any other temporal instance by using their from static factory methods as follows:
localdate date = localdate. Of(2014, month. March, 18);
japanesedate japanesedate = japanesedate. From(date);
312 chapter 12 new date and time api

alternatively, you can explicitly create a calendar system for a specific locale and create
an instance of a date for that locale. In the new date and time api, the chronology
interface models a calendar system, and you can obtain an instance of it by using its
oflocale static factory method:

chronology japanesechronology = chronology. Oflocale(locale. Japan);
chronolocaldate now = japanesechronology. Datenow();

the designers of the date and time api advise using localdate instead of chrono-
localdate for most cases, because a developer could make assumptions in his code
that unfortunately arent true in a multicalendar system. Such assumptions might
include believing that the value of a day or month will never be higher than 31, that a
year contains 12 months, or even that a year has a fixed number of months. For these
reasons, we recommend using localdate throughout your application, including all
storage, manipulation, and interpretation of business rules, whereas you should
employ chronolocaldate only when you need to localize the input or output of your
program.
Islamic calendar
of the new calendars added to java 8, the hijrahdate (islamic calendar) seems to be
the most complex because it can have variants. The hijrah calendar system is based on
lunar months. There are a variety of methods to determine a new month, such as a
new moon that could be visible anywhere in the world or that must be visible first in
saudi arabia. The withvariant method is used to choose the desired variant. Java 8
includes the umm al-qura variant for hijrahdate as standard.
The following code illustrates an example of displaying the start and end dates of
ramadan for the current islamic year in iso date:
get the current hijrah date; then change
it to have the first day of ramadan,
which is the ninth month.
Hijrahdate ramadandate =
hijrahdate. Now(). With(chronofield. Day_of_month, 1)
. With(chronofield. Month_of_year, 9);
system. Out. Println("ramadan starts on " +
isochronology. Instance. Date(ramadandate) +
ramadan 1438 " and ends on " +
started on 2017-05- isochronology. Instance. Date(
26 and ended on ramadandate. With(
2017-06-24. Temporaladjusters. Lastdayofmonth())));

isochronology. Instance is a static
instance of the isochronology class.
Summary 313

summary
 the old java. Util. Date class and all other classes used to model dates and
times in java before java 8 have many inconsistencies and design flaws, includ-
ing mutability and some poorly chosen offsets, defaults, and naming.
 All the date-time objects of the new date and time api are immutable.
 This new api provides two different time representations to manage the differ-
ent needs of humans and machines when operating on it.
 You can manipulate date and time objects in both an absolute and relative man-
ner, and the result of these manipulations is always a new instance, leaving the
original one unchanged.
 Temporaladjusters allow you to manipulate a date in a more complex way than
changing one of its values, and you can define and use your own custom date
transformations.
 You can define a formatter to print and parse date-time objects in a specific for-
mat. These formatters can be created from a pattern or programmatically, and
theyre all thread-safe.
 You can represent a time zone, relative to a specific region/location and as a
fixed offset from utc/greenwich, and apply it to a date-time object to localize it.
 You can use calendar systems different from the iso-8601 standard system.
Default methods

this chapter covers
 what default methods are
 evolving apis in a compatible way
 usage patterns for default methods
 resolution rules

traditionally, a java interface groups related methods together into a contract. Any
(nonabstract) class that implements an interface must provide an implementation
for each method defined by the interface or inherit the implementation from a
superclass. But this requirement causes a problem when library designers need to
update an interface to add a new method. Indeed, existing concrete classes (which
may not be under the interface designers control) need to be modified to reflect
the new interface contract. This situation is particularly problematic because the
java 8 api introduces many new methods on existing interfaces, such as the sort
method on the list interface that you used in previous chapters. Imagine all the
angry maintainers of alternative collection frameworks such as guava and apache
commons who now need to modify all the classes implementing the list interface
to provide an implementation for the sort method too!

314
315

but dont worry. Java 8 introduced a new mechanism to tackle this problem. It may
sound surprising, but since java 8 interfaces can declare methods with implementa-
tion code in two ways. First, java 8 allowed static methods inside interfaces. Second, java
8 introduced a new feature called default methods that allows you to provide a default
implementation for methods in an interface. In other words, interfaces can now pro-
vide concrete implementation for methods. As a result, existing classes implementing
an interface automatically inherit the default implementations if they dont provide
one explicitly, which allows you to evolve interfaces nonintrusively. Youve been using
several default methods all along. Two examples youve seen are sort in the list
interface and stream in the collection interface.
The sort method in the list interface, which you saw in chapter 1, is new to java
8 and is defined as follows:
default void sort(comparator<? Super e> c){
collections. Sort(this, c);
}

note the new default modifier before the return type. This modifier is how you can
tell that a method is a default method. Here, the sort method calls the collections
. Sort method to perform the sorting. Thanks to this new method, you can sort a list
by calling the method directly:

list<integer> numbers = arrays. Aslist(3, 5, 1, 2, 6); sort is a default method
numbers. Sort(comparator. Naturalorder()); in the list interface.

Something else is new in this code. Notice that you call the comparator. Naturalorder
method. This new static method in the comparator interface returns a comparator
object to sort the elements in natural order (the standard alphanumerical sort). The
stream method in collection you saw in chapter 4 looks like this:
default stream<e> stream() {
return streamsupport. Stream(spliterator(), false);
}

here, the stream method, which you used extensively in previous chapters to process
collections, calls the streamsupport. Stream method to return a stream. Notice how
the body of the stream method is calling the method spliterator, which is also a
default method of the collection interface.
Wow! Are interfaces like abstract classes now? Yes and no; there are fundamental
differences, which we explain in this chapter. More important, why should you care
about default methods? The main users of default methods are library designers. As
we explain later, default methods were introduced to evolve libraries such as the java
api in a compatible way, as illustrated in figure 13.1.
In a nutshell, adding a method to an interface is the source of many problems; exist-
ing classes implementing the interface need to be changed to provide an implementa-
tion for the method. If youre in control of the interface and all its implementations, the
316 chapter 13 default methods

interface api version 1 implementation from user
filled boxes represent
implemented methods
methods and dashed boxes
to support represent abstract
implements methods.

Interface api version 2 implementation from user

methods
to support

implements ? ? ? ? ? ? ?

The implementation from the user needs
to be modified to support the new method
specified by the interface contract.

Interface api version 2
with default method implementation from user

methods
to support

implements

the implementation from the user doesnt
need to be modified as it inherits the
default method from the interface!

Figure 13.1 adding a method to an interface

situation isnt too bad. But this is often not the caseand it provides the motivation
for default methods, which let classes inherit a default implementation from an inter-
face automatically.
If youre a library designer, this chapter is important because default methods pro-
vide a means of evolving interfaces without modifying existing implementations. Also,
as we explain later in the chapter, default methods can help structure your programs
by providing a flexible mechanism for multiple inheritance of behavior; a class can
inherit default methods from several interfaces. Therefore, you may still be interested
in finding out about default methods even if youre not a library designer.

Static methods and interfaces
a common pattern in java is to define both an interface and a utility companion class
defining many static methods for working with instances of the interface. Collec-
tions is a companion class to deal with collection objects, for example. Now that
static methods can exist inside interfaces, such utility classes in your code can go
away, and their static methods can be moved inside an interface. These companion
classes remain in the java api to preserve backward compatibility.
Evolving apis 317

the chapter is structured as follows. First, we walk you through a use case of evolving
an api and the problems that can arise. Then we explain what default methods are
and discuss how you can use them to tackle the problems in the use case. Next, we
show how you can create your own default methods to achieve a form of multiple
inheritance in java. We conclude with some more technical information about how
the java compiler resolves possible ambiguities when a class inherits several default
methods with the same signature.

13.1 evolving apis
to understand why its difficult to evolve an api when its been published, suppose for
the purpose of this section that youre the designer of a popular java drawing library.
Your library contains a resizable interface that defines many methods that a simple
resizable shape must support: setheight, setwidth, getheight, getwidth, and set-
absolutesize. In addition, you provide several out-of-the-box implementations for it,
such as square and rectangle. Because your library is so popular, you have some
users who have created their own interesting implementations, such as ellipse, using
your resizable interface.
A few months after releasing your api, you realize that resizable is missing some
features. It would be nice, for example, if the interface had a setrelativesize
method that takes as argument a growth factor to resize a shape. You might add the
setrelativesize method to resizable and update your implementations of square
and rectangle. But not so fast! What about all your users who created their own
implementations of the resizable interface? Unfortunately, you dont have access to
and cant change their classes that implement resizable. This problem is the same
one that java library designers face when they need to evolve the java api. In the next
section, we look in detail at an example that shows the consequences of modifying an
interface thats already been published.

13.1.1 api version 1
the first version of your resizable interface has the following methods:
public interface resizable extends drawable{
int getwidth();
int getheight();
void setwidth(int width);
void setheight(int height);
void setabsolutesize(int width, int height);
}

user implementation
one of your most loyal users decides to create his own implementation of resizable
called ellipse:
public class ellipse implements resizable {
...
}
318 chapter 13 default methods

hes created a game that processes different types of resizable shapes (including his
own ellipse):
public class game{ a list of shapes
public static void main(string... Args){ that are resizable
list<resizable> resizableshapes =
arrays. Aslist(new square(), new rectangle(), new ellipse());
utils. Paint(resizableshapes);
}
}
public class utils{
public static void paint(list<resizable> l){
l. Foreach(r -> {
r. Setabsolutesize(42, 42);
calling the setabsolutesize
r. Draw(); method on each shape
});
}
}

13.1.2 api version 2
after your library has been in use for a few months, you receive many requests to
update your implementations of resizable: square, rectangle, and so on to support
the setrelativesize method. You come up with version 2 of your api, as shown here
and illustrated in figure 13.2:

api version 1 api version 2

resizable
resizable
+ void setrelativesize(int, int)

ellipse utils
ellipse utils

game
game

figure 13.2 evolving an api by adding a method to resizable. Recompiling the application
produces errors because it depends on the resizable interface.

Public interface resizable {
int getwidth();
int getheight();
void setwidth(int width);
evolving apis 319

void setheight(int height);
void setabsolutesize(int width, int height); adding a new method
void setrelativesize(int wfactor, int hfactor); for api version 2
}

problems for your users
this update of resizable creates problems. First, the interface now demands an
implementation of setrelativesize, but the ellipse implementation that your user
created doesnt implement the method setrelativesize. Adding a new method to
an interface is binary compatible, which means that existing class file implementations
still run without the implementation of the new method if no attempt is made to
recompile them. In this case, the game will still run (unless its recompiled) despite
the addition of the setrelativesize method to the resizable interface. Nonethe-
less, the user could modify the method utils. Paint in his game to use the set-
relativesize method because the paint method expects a list of resizable objects
as an argument. If an ellipse object is passed, an error is thrown at runtime because
the setrelativesize method isnt implemented:
exception in thread "main" java. Lang. Abstractmethoderror:
lambdasinaction. Chap9. Ellipse. Setrelativesize(ii)v

second, if the user tries to rebuild his entire application (including ellipse), hell
get the following compile error:
lambdasinaction/chap9/ellipse. Java:6: error: ellipse is not abstract and does
not override abstract method setrelativesize(int, int) in resizable

consequently, updating a published api creates backward incompatibilities, which is
why evolving existing apis, such as the official java collections api, causes problems
for users of the apis. You have alternatives to evolving an api, but theyre poor
choices. You could create a separate version of your api and maintain both the old
and the new versions, for example, but this option is inconvenient for several reasons.
First, its more complex for you to maintain as a library designer. Second, your users
may have to use both versions of your api in the same code base, which affects mem-
ory space and loading time because more class files are required for their projects.
In this case, default methods come to the rescue. They let library designers evolve
apis without breaking existing code because classes that implement an updated inter-
face automatically inherit a default implementation.

Different types of compatibilities: binary, source, and behavioral
there are three main kinds of compatibility when introducing a change to a java pro-
gram: binary, source, and behavioral compatibilities (see https: //blogs. Oracle. Com/
darcy/entry/kinds_of_compatibility). You saw that adding a method to an interface is
binary compatible but results in a compiler error if the class implementing the interface
is recompiled. Its good to know the different kinds of compatibilities, so in this side-
bar, we examine them in detail.
320 chapter 13 default methods

(continued)
binary compatibility means that existing binaries running without errors continue to
link (which involves verification, preparation, and resolution) without error after intro-
ducing a change. Adding a method to an interface is binary compatible, for example,
because if its not called, existing methods of the interface can still run without
problems.
In its simplest form, source compatibility means that an existing program will still
compile after introducing a change. Adding a method to an interface isnt source com-
patible; existing implementations wont recompile because they need to implement
the new method.
Finally, behavioral compatibility means running a program after a change with the
same input results in the same behavior. Adding a method to an interface is behav-
ioral compatible because the method is never called in the program (or gets over-
ridden by an implementation).

13.2 default methods in a nutshell
youve seen how adding methods to a published api disrupts existing implementa-
tions. Default methods are new in java 8 to evolve apis in a compatible way. Now an
interface can contain method signatures for which an implementing class doesnt pro-
vide an implementation. Who implements them? The missing method bodies are
given as part of the interface (hence, default implementations) rather than in the
implementing class.
How do you recognize a default method? Simple: it starts with a default modifier
and contains a body like a method declared in a class. In the context of a collection
library, you could define an interface sized with one abstract method size and a
default method isempty, as follows:

public interface sized {
int size();
default boolean isempty() { a default method
return size() == 0;
}
}

now any class that implements the sized interface automatically inherits the imple-
mentation of isempty. Consequently, adding a method to an interface with a default
implementation isnt a source incompatibility.
Now go back to the initial example of the java drawing library and your game.
Concretely, to evolve your library in a compatible way (which means that the users
of your library dont have to modify all their classes that implement resizable),
use a default method and provide a default implementation for setrelativesize,
as follows:
default methods in a nutshell 321

default void setrelativesize(int wfactor, int hfactor){
setabsolutesize(getwidth() / wfactor, getheight() / hfactor);
}

because interfaces can now have methods with implementation, does that mean mul-
tiple inheritance has arrived in java? What happens if an implementing class also
defines the same method signature or default methods can be overridden? Dont
worry about these issues for now; a few rules and mechanisms are available to help you
deal with these issues. We explore them in detail in section 13.4.
You may have guessed that default methods are used extensively in the java 8 api.
You saw in the introduction of this chapter that the stream method in the collection
interface that we used extensively in previous chapters is a default method. The sort
method in the list interface is also a default method. Many of the functional inter-
faces we presented in chapter 3such as predicate, function, and comparatoralso
introduced new default methods, such as predicate. And and function. Andthen.
(remember that a functional interface contains only one abstract method; default
methods are nonabstract methods. )

abstract classes vs. Interfaces in java 8
whats the difference between an abstract class and an interface? Both can contain
abstract methods and methods with a body.
First, a class can extend only from one abstract class, but a class can implement mul-
tiple interfaces.
Second, an abstract class can enforce a common state through instance variables
(fields). An interface cant have instance variables.

To put your knowledge of default methods to use, have a go at quiz 13.1.

Quiz 13.1: removeif
for this quiz, pretend that youre one of the masters of the java language and api.
Youve received many requests for a removeif method to use on arraylist, tree-
set, linkedlist, and all other collections. The removeif method should remove all
elements from a collection that match a given predicate. Your task in this quiz is to
figure out the best way to enhance the collections api with this new method.
Answer:
whats the most disruptive way to enhance the collections api? You could copy
and paste the implementation of removeif in each concrete class of the collections
api, but that solution would be a crime to the java community. What else can you
do? Well, all the collection classes implement an interface called java. Util.
Collection. Great; can you add a method there? Yes. Youve learned that default
322 chapter 13 default methods

(continued)
methods allow you to add implementations inside an interface in a source-compatible
way. All classes that implement collection (including classes from your users
that arent part of the collections api) can use the implementation of removeif.
The code solution for removeif is as follows (which is roughly the implementation
in the official java 8 collections api). This solution is a default method inside the
collection interface:
default boolean removeif(predicate<? Super e> filter) {
boolean removed = false;
iterator<e> each = iterator();
while(each. Hasnext()) {
if(filter. Test(each. Next())) {
each. Remove();
removed = true;
}
}
return removed;
}

13.3 usage patterns for default methods
youve seen that default methods can be useful for evolving a library in a compatible
way. Can you do anything else with them? You can create your own interfaces that
have default methods too. You may want to do this for two use cases that we explore in
the following sections: optional methods and multiple inheritance of behavior.

13.3.1 optional methods
youre likely to have come across classes that implement an interface but leave empty
some method implementations. Take the iterator interface, for example, which
defines hasnext and next but also the remove method. Before java 8, remove was
often ignored because users decided not to use that capability. As a result, many
classes that implement iterator have an empty implementation for remove, which
results in unnecessary boilerplate code.
With default methods, you can provide a default implementation for such meth-
ods, so concrete classes dont need to explicitly provide an empty implementation.
The iterator interface in java 8 provides a default implementation for remove as
follows:

interface iterator<t> {
boolean hasnext();
t next();
default void remove() {
throw new unsupportedoperationexception();
}
}
usage patterns for default methods 323

consequently, you can reduce boilerplate code. Any class that implements the iterator
interface no longer needs to declare an empty remove method to ignore it because
now it has a default implementation.

13.3.2 multiple inheritance of behavior
default methods enable something elegant that wasnt possible before: multiple inheri-
tance of behavior, which is the ability of a class to reuse code from multiple places (fig-
ure 13.3).

Single inheritance multiple inheritance

functionality #1 functionality #1

functionality #2

functionality #3

functionality #1

functionality #2

functionality #1 functionality #3

a class inheriting functionality a class inheriting functionality
from only one place from multiple places

figure 13.3 single inheritance versus multiple inheritance

remember that classes in java can inherit from only one other class, but classes have
always been allowed to implement multiple interfaces. To confirm, heres how the
class arraylist is defined in the java api:

public class arraylist<e> extends abstractlist<e> inherits from
implements list<e>, randomaccess, cloneable, one class
serializable {
implements
}
four interfaces

multiple inheritance of types
here, arraylist is extending one class and directly implementing four interfaces. As
a result, an arraylist is a direct subtype of seven types: abstractlist, list, random-
access, cloneable, serializable, iterable, and collection. In a sense, you already
have multiple inheritance of types.
324 chapter 13 default methods

because interface methods can have implementations in java 8, classes can inherit
behavior (implementation code) from multiple interfaces. In the next section, we
explore an example to show how you can use this capability to your benefit. Keeping
interfaces minimal and orthogonal lets you achieve great reuse and composition of
behavior inside your code base.
Minimal interfaces with orthogonal functionalities
suppose that you need to define several shapes with different characteristics for the
game youre creating. Some shapes should be resizable but not rotatable; some should
be rotatable and movable but not resizable. How can you achieve great code reuse?
You can start by defining a stand-alone rotatable interface with two abstract meth-
ods: setrotationangle and getrotationangle. The interface also declares a default
rotateby method that you can implement by using the setrotationangle and get-
rotationangle methods as follows:
public interface rotatable {
void setrotationangle(int angleindegrees); a default implementation
int getrotationangle(); for the method rotateby
default void rotateby(int angleindegrees){
setrotationangle((getrotationangle () + angleindegrees) % 360);
}
}

this technique is somewhat related to the template design pattern, in which a skele-
ton algorithm is defined in terms of other methods that need to be implemented.
Now any class that implements rotatable will need to provide an implementation
for setrotationangle and getrotationangle but will inherit the default implemen-
tation of rotateby for free.
Similarly, you can define two interfaces that you saw earlier: moveable and resizable.
Both interfaces contain default implementations. Heres the code for moveable:
public interface moveable {
int getx();
int gety();
void setx(int x);
void sety(int y);
default void movehorizontally(int distance){
setx(getx() + distance);
}
default void movevertically(int distance){
sety(gety() + distance);
}
}

and heres the code for resizable:
public interface resizable {
int getwidth();
int getheight();
void setwidth(int width);
usage patterns for default methods 325

void setheight(int height);
void setabsolutesize(int width, int height);
default void setrelativesize(int wfactor, int hfactor){
setabsolutesize(getwidth() / wfactor, getheight() / hfactor);
}
}

composing interfaces
you can create different concrete classes for your game by composing these interfaces.
Monsters, for example, can be moveable, rotatable, and resizable:
public class monster implements rotatable, moveable, resizable {
...
} needs to provide implementations for all
abstract methods but not the default methods

the monster class automatically inherits the default methods from the rotatable,
moveable, and resizable interfaces. In this case, monster inherits the implementa-
tions of rotateby, movehorizontally, movevertically, and setrelativesize.
Now you can call the different methods directly:
constructor internally sets the
coordinates, height, width,
monster m = new monster(); and default angle.
M. Rotateby(180);
m. Movevertically(10); calling rotateby
calling movevertically from rotatable
from moveable

suppose that now you need to declare another class thats moveable and rotatable but
not resizable, such as the sun. You dont need to copy and paste code; you can reuse the
default implementations from the moveable and rotatable interfaces, as shown here.
Public class sun implements moveable, rotatable {
...
} needs to provide implementations for all
abstract methods but not the default methods

figure 13.4 illustrates the uml diagram of this scenario.

Rotatable moveable resizable

sun monster
figure 13.4 multiple behavior composition

heres another advantage of defining simple interfaces with default implementations
like the ones for your game. Suppose that you need to modify the implementation of
326 chapter 13 default methods

movevertically to make it more efficient. You can change its implementation directly
in the moveable interface, and all classes implementing it automatically inherit the
code (provided that they didnt implement the method themselves)!

Inheritance considered harmful
inheritance shouldnt be your answer to everything when it comes to reusing code.
For inheriting from a class that has 100 methods and fields to reuse one method is
a bad idea, for example, because it adds unnecessary complexity. Youd be better
off using delegation: create a method that calls directly the method of the class you
need via a member variable. For this reason, youll sometimes find classes that are
declared final intentionally: they cant be inherited from to prevent this kind of anti-
pattern or have their core behavior messed with. Note that sometimes, final classes
have a place. String is final, for example, because you dont want anybody to be
able to interfere with such core functionality.
The same idea applies to interfaces with default methods. By keeping your interface
minimal, you can achieve greater composition because you can select only the imple-
mentations you need.

Youve seen that default methods are useful for many usage patterns. But heres some
food for thought: what if a class implements two interfaces that have the same default
method signature? Which method is the class allowed to use? We explore this problem
in the next section.

13.4 resolution rules
as you know, in java a class can extend only one parent class but implement multiple
interfaces. With the introduction of default methods in java 8, theres the possibility
of a class inheriting more than one method with the same signature. Which version of
the method should be used? Such conflicts probably are quite rare in practice, but
when they do occur, there must be rules that specify how to deal with the conflict. This
section explains how the java compiler resolves such potential conflicts. We aim to
answer questions such as in the code that follows, which hello method is c calling?
Note that the examples that follow are intended to explore problematic scenarios;
such scenarios wont necessarily happen frequently in practice:

public interface a {
default void hello() {
system. Out. Println("hello from a");
}
}
public interface b extends a {
default void hello() {
system. Out. Println("hello from b");
}
}
resolution rules 327

public class c implements b, a {
public static void main(string... Args) {
new c(). Hello();
} what gets
} printed?

In addition, you may have heard of the diamond problem in c++ in which a class can
inherit two methods with the same signature. Which one gets chosen? Java 8 provides
resolution rules to solve this issue too. Read on!

13.4.1 three resolution rules to know
you have three rules to follow when a class inherits a method with the same signature
from multiple places (such as another class or interface):
1 classes always win. A method declaration in the class or a superclass takes prior-
ity over any default method declaration.
2 otherwise, subinterfaces win: the method with the same signature in the most
specific default-providing interface is selected. (if b extends a, b is more specific
than a. )
3 finally, if the choice is still ambiguous, the class inheriting from multiple inter-
faces has to explicitly select which default method implementation to use by
overriding it and calling the desired method explicitly.
We promise that these are the only rules you need to know! In the next section, we
look at some examples.

13.4.2 most specific default-providing interface wins
here, you revisit the example from the beginning of this section in which c imple-
ments both b and a, which define a default method called hello. In addition, b
extends a. Figure 13.5 provides a uml diagram for the scenario.

A
c
+ void hello()
b

+ void hello()

figure 13.5 the most specific default-providing interface wins.

Which declaration of the hello method will the compiler use? Rule 2 says that the
method with the most specific default-providing interface is selected. Because b is
more specific than a, the hello from b is selected. Consequently, the program prints
"hello from b".
328 chapter 13 default methods

now consider what would happen if c were inheriting from d as follows (illustrated
in figure 13.6):

d

a
c
+ void hello()
b

+ void hello()

figure 13.6 inheriting from a class and implementing two interfaces

public class d implements a{ }
public class c extends d implements b, a {
public static void main(string... Args) { what gets
new c(). Hello(); printed?
}
}

rule 1 says that a method declaration in the class takes priority. But d doesnt override
hello; it implements interface a. Consequently, it has a default method from inter-
face a. Rule 2 says that if there are no methods in the class or superclass, the method
with the most specific default-providing interface is selected. The compiler, therefore,
has a choice between the hello method from interface a and the hello method from
interface b. Because b is more specific, the program prints "hello from b" again.
To check your understanding of the resolution rules, try quiz 13.2.

Quiz 13.2: remember the resolution rules
for this quiz, reuse the preceding example, except that d explicitly overrides the
hello method from a. What do you think will get printed?
Public class d implements a{
void hello(){
system. Out. Println("hello from d");
}
}
public class c extends d implements b, a {
public static void main(string... Args) {
new c(). Hello();
}
}

answer:
the program prints "hello from d" because a method declaration from a superclass
has priority, as stated by rule 1.
Resolution rules 329

note that if d were declared as follows,
public abstract class d implements a {
public abstract void hello();
}

c would be forced to implement the method hello itself, even though default imple-
mentations exist elsewhere in the hierarchy.

13.4.3 conflicts and explicit disambiguation
the examples youve seen so far could be resolved by the first two resolution rules.
Now suppose that b doesnt extend a anymore (illustrated in figure 13.7):

a

+ void hello()
c

b

+ void hello()
figure 13.7 implementing two interfaces

public interface a {
default void hello() {
system. Out. Println("hello from a");
}
}
public interface b {
default void hello() {
system. Out. Println("hello from b");
}
}
public class c implements b, a { }

rule 2 doesnt help you now because theres no more-specific interface to select. Both
hello methods from a and b could be valid options. Thus, the java compiler produces
a compile error because it doesnt know which method is more suitable: "error:
class c inherits unrelated defaults for hello() from types b and a. "
resolving the conflict
there arent many solutions to resolve the conflict between the two possible valid
methods; you have to explicitly decide which method declaration you want c to use.
To do so, you can override the hello method in class c and then, in its body, explicitly
call the method you want to use. Java 8 introduces the new syntax x. Super. M(... )
where x is the superinterface whose method m you want to call. If you want c to use the
default method from b, for example, the code looks like this:
330 chapter 13 default methods

public class c implements b, a {
void hello(){ explicitly choosing to call
b. Super. Hello(); the method from interface b
}
}

have a go at quiz 13.3 to investigate a related tricky case.

Quiz 13.3: almost the same signature
for this quiz, assume that interfaces a and b are declared as follows:
public interface a{
default number getnumber(){
return 10;
}
}
public interface b{
default integer getnumber(){
return 42;
}
}

also assume that class c is declared as follows:
public class c implements b, a {
public static void main(string... Args) {
system. Out. Println(new c(). Getnumber());
}
}

what will the program print?
Answer:
c cant distinguish which method of a or b is more specific. For this reason, class c
wont compile.

13.4.4 diamond problem
finally, consider a scenario that sends shivers through the c++ community:
public interface a{
default void hello(){
system. Out. Println("hello from a");
}
}
public interface b extends a { }
public interface c extends a { }
public class d implements b, c {
public static void main(string... Args) { what gets
new d(). Hello(); printed?
}
}
resolution rules 331

figure 13.8 illustrates the uml diagram for this scenario. The problem is called a dia-
mond problem because the diagram resembles a diamond. What default method decla-
ration does d inherit: the one from b or the one from c? You have only one method
declaration to choose. Only a declares a default method. Because the interface is a
superinterface of d, the code prints "hello from a".

C

a
d
+ void hello()
b

figure 13.8 the diamond problem

now what happens if b also has a default hello method with the same signature?
Rule 2 says that you select the most specific default-providing interface. Because b is
more specific than a, the default method declaration from b is selected. If both b and
c declare a hello method with the same signature, you have a conflict and need to
solve it explicitly, as we showed earlier.
As a side note, you may wonder what happens if you add an abstract hello method
(one thats not default) in interface c as follows (still no methods in a and b):

public interface c extends a {
void hello();
}

the new abstract hello method in c takes priority over the default hello method
from interface a because c is more specific. Therefore, class d needs to provide an
explicit implementation for hello; otherwise, the program wont compile.

C++ diamond problem
the diamond problem is more complicated in c++. First, c++ allows multiple inheri-
tance of classes. By default, if a class d inherits from classes b and c, and classes
b and c both inherit from a, class d has access to a copy of a b object and a copy of
a c object. As a result, uses of methods from a have to be explicitly qualified: are
they coming from b or c? In addition, classes have state, so modifying member vari-
ables from b isnt reflected in the copy of the c object.
332 chapter 13 default methods

youve seen that the default methods resolution mechanism is simple if a class inher-
its from several methods with the same signature. Follow three rules systematically to
solve all possible conflicts:
1 first, an explicit method declaration in the class or a superclass takes priority
over any default method declaration.
2 otherwise, the method with the same signature in the most specific default-
providing interface is selected.
3 finally, if theres still a conflict, you have to explicitly override the default meth-
ods and choose which one your class should use.

Summary
 interfaces in java 8 can have implementation code through default methods
and static methods.
 Default methods start with a default keyword and contain a body, as class
methods do.
 Adding an abstract method to a published interface is a source incompatibility.
 Default methods help library designers evolve apis in a backward-compatible way.
 Default methods can be used for creating optional methods and multiple inher-
itance of behavior.
 Resolution rules exist to resolve conflicts when a class inherits from several
default methods with the same signature.
 A method declaration in the class or a superclass takes priority over any default
method declaration. Otherwise, the method with the same signature in the most
specific default-providing interface is selected.
 When two methods are equally specific, a class must explicitly override this
method, such as to select which one to call.
The java module system

this chapter covers
 the evolutionary forces causing java to adopt a
module system
 the main structure: module declarations and
requires and exports directives
 automatic modules for legacy java archives
(jars)
 modularization and the jdk library
 modules and maven builds
 a brief summary of module directives beyond
simple requires and exports

the main and most-discussed new feature introduced with java 9 is its module sys-
tem. This feature was developed within project jigsaw, and its development took
almost a decade. This timeline is a good measure of both the importance of this
addition and the difficulties that the java development team met while implement-
ing it. This chapter provides background on why you should care as a developer
what a module system is, as well as an overview of what the new java module system
is intended for and how you can benefit from it.

333
334 chapter 14 the java module system

note that the java module system is a complex topic that merits a whole book.
We recommend the java module system by nicolai parlog (manning publications,
https: //www. Manning. Com/books/the-java-module-system) for a comprehensive
resource. In this chapter, we deliberately keep to the broad-brush picture so that you
understand the main motivation and get a rapid overview of how to work with java
modules.

14.1 the driving force: reasoning about software
before you delve into the details of the java module system, its useful to understand
some motivation and background to appreciate the goals set out by the java language
designers. What does modularity mean? What problem is the module system looking
to address? This book has spent quite a lot of time discussing new language features
that help us write code that reads closer to the problem statement and, as a result, is
easier to understand and maintain. This concern is a low-level one, however. Ulti-
mately, at a high level (software-architectural level), you want to work with a software
project thats easy to reason about because this makes you more productive when you
introduce changes in your code base. In the following sections, we highlight two
design principles that help produce software thats easier to reason about: separation of
concerns and information hiding.

14.1.1 separation of concerns
separation of concerns (soc) is a principle that promotes decomposing a computer
program into distinct features. Suppose that you need to develop an accounting appli-
cation that parses expenses in different formats, analyzes them, and provides sum-
mary reports to your customer. By applying soc, you split parsing, analysis, and
reporting into separate parts called modulescohesive groups of code that have little
overlap. In other words, a module groups classes, allowing you to express visibility
relationships between classes in your application.
You might say, ah, but java packages already group classes. Youre right, but java 9
modules give you finer-grained control of which classes can see which other classes
and allow this control to be checked at compile time. In essence, java packages dont
support modularity.
The soc principle is useful at an architectural point of view (such as model versus
view versus controller) and in a low-level approach (such as separating the business
logic from the recovery mechanism). The benefits are
 allowing work on individual parts in isolation, which helps team collaboration
 facilitating reuse of separate parts
 easier maintenance of the overall system

14.1.2 information hiding
information hiding is a principle that encourages hiding implementation details. Why
is this principle important? In the context of building software, requirements can
the driving force: reasoning about software 335

change frequently. By hiding implementation details, you can reduce the chances that
a local change will require cascading changes in other parts of your program. In other
words, its a useful principle for managing and protecting your code. You often hear
the term encapsulation used to indicate that a specific piece of code is so well isolated
from the other parts of the application that changing its internal implementation
wont negatively affect them. In java, you can get a compiler to check that components
within a class are well encapsulated by using the private keyword appropriately. But
until java 9, there was no language structure to allow the compiler to check that classes
and packages were available only for the intended purposes.

14.1.3 java software
these two principles are fundamental in any well-designed software. How do they fit
with java language features? Java is an object-oriented language, and you work with
classes and interfaces. You make your code modular by grouping packages, classes, and
interfaces that address a specific concern. In practice, reasoning about raw code is a
bit abstract. As a result, tools such as uml diagrams (or, more simply, boxes and
arrows) help you reason about your software by visually representing dependencies
among parts of your code. Figure 14.1 shows a uml diagram for an application man-
aging a user profile that has been decomposed into three specific concerns.

View model

uses
userprofileview user

uses uses

userprofilecontroller

figure 14.1 three separate
concerns with dependencies
controller

what about information hiding? In java, youre familiar with using visibility modifiers
to control access to methods, fields, and classes: public, protected, package-level, and
private. As we clarify in the next section, however, their granularity isnt fine enough
in many cases, and you could be obliged to declare a method public even if you didnt
intend to make it accessible for end users. This concern wasnt a huge one in the early
days of java, when applications and dependency chains were relatively small. Now that
many java applications are large, the problem is more important. Indeed, if you see a
public field or method in a class, you probably feel entitled to use it (dont you? ),
336 chapter 14 the java module system

even though the designer may regard it as being only for private use among a few of
his own classes!
Now that you understand the benefits of modularization, you may wonder how
supporting it causes changes in java. We explain in the next section.

14.2 why the java module system was designed
in this section, you learn why a new module system was designed for the java language
and compiler. First, we cover the limitations of modularity before java 9. Next, we pro-
vide background about the jdk library and explain why modularizing it was important.

14.2.1 modularity limitations
unfortunately, the built-in support in java to help produce modular software projects
was somewhat limited before java 9. Java has had three levels at which code was
grouped: classes, packages, and jars. For classes, java has always had support for
access modifiers and encapsulation. There was little encapsulation at the package and
jar levels, however.
Limited visibility control
as discussed in the previous section, java provides access modifiers to support infor-
mation hiding. These modifiers are public, protected, package-level, and private visi-
bility. But what about controlling visibility between packages? Most applications have
several packages defined to group various classes, but packages have limited support
for visibility control. If you want classes and interfaces from one package to be visible
to another package, you have to declare them as public. As a consequence, these
classes and interfaces are accessible to everyone else as well. A typical occurrence of
this problem is when you see companion packages with names that include the string
"impl" to provide default implementations. In this case, because the code inside that
package was defined as public, you have no way to prevent users from using these
internal implementations. As a result, it becomes difficult to evolve your code without
making breaking changes, because what you thought was for internal use only was
used by a programmer temporarily to get something working and then frozen into the
system. Worse, this situation is bad from a security point of view because you poten-
tially increase the attack surface as more code is exposed to the risk of tampering.
Class path
earlier in this chapter, we discussed the benefits of software written in a way that
makes it simple to maintain and understandin other words, easier to reason about.
We also talked about separation of concerns and modeling dependencies between
modules. Unfortunately, java historically falls short in supporting these ideas when it
comes to bundling and running an application. In fact, you have to ship all your com-
piled classes into one single flat jar, which is made accessible from the class path.1
then the jvm can dynamically locate and load classes from the class path as needed.

1
this spelling is used in java documentation, but classpath is often used for arguments to programs.
Why the java module system was designed 337

unfortunately, the combination of the class path and jars has several downsides.
First, the class path has no notion of versioning for the same class. You cant, for
example, specify that the class jsonparser from a parsing library should belong to
version 1.0 or version 2.0, so you cant predict what will happen if the same library
with two different versions is available on the class path. This situation is common in
large applications, as you may have different versions of the same libraries used by dif-
ferent components of your application.
Second, the class path doesnt support explicit dependencies; all the classes inside
different jars are merged into one bag of classes on the class path. In other words,
the class path doesnt let you declare explicitly that one jar depends on a set of
classes contained inside another jar. This situation makes it difficult to reason about
the class path and to ask questions such as:
 is anything missing?
 Are there any conflicts?

Build tools such as maven and gradle can help you solve this problem. Before java 9,
however, neither java nor the jvm had any support for explicit dependencies. The
issues combined are often referred to as jar hell or class path hell. The direct conse-
quence of these problems is that its common to have to keep adding and removing
class files on the class path in a trial-and-error cycle, in the hope that the jvm will exe-
cute your application without throwing runtime exceptions such as classnotfound-
exception. Ideally, youd like such problems to be discovered early in the development
process. Using the java 9 module system consistently enables all such errors to be
detected at compile time.
Encapsulation and class path hell arent problems only for your software architec-
ture, however. What about the jdk itself?

14.2.2 monolithic jdk
the java development kit (jdk) is a collection of tools that lets you work with and run
java programs. Perhaps the most important tools youre familiar with are javac to com-
pile java programs and java to load and run a java application, along with the jdk
library, which provides runtime support including input/output, collections, and
streams. The first version was released in 1996. Its important to understand that like
any software, the jdk has grown and increased considerably in size. Many technologies
were added and later deprecated. Corba is a good example. It doesnt matter
whether youre using corba in your application or not; its classes are shipped with
the jdk. This situation becomes problematic especially in applications that run on
mobile or in the cloud and typically dont need all the parts available in the jdk library.
How can you get away from this problem as a whole ecosystem? Java 8 introduced
the notion of compact profiles as a step forward. Three profiles were introduced to
have different memory footprints, depending on which parts of the jdk library
youre interested in. Compact profiles, however, provided only a short-term fix. Many
338 chapter 14 the java module system

internal apis in the jdk arent meant for public use. Unfortunately, due to the poor
encapsulation provided by the java language, those apis are commonly used. The
class sun. Misc. Unsafe, for example, is used by several libraries (including spring,
netty, and mockito) but was never intended to be made available outside the jdk
internals. As a result, its extremely difficult to evolve these apis without introducing
incompatible changes.
All these problems provided the motivation for designing a java module system that
also can be used to modularize the jdk itself. In a nutshell, new structuring constructs
were required to allow you to choose what parts of the jdk you need and how to reason
about the class path, and to provide stronger encapsulation to evolve the platform.

14.2.3 comparison with osgi
this section compares java 9 modules with osgi. If you havent heard of osgi, we
suggest that you skip this section.
Before the introduction of modules based on project jigsaw into java 9, java
already had a powerful module system, named osgi, even if it wasnt formally part of
the java platform. The open service gateway initiative (osgi) started in 2000 and,
until the arrival of java 9, represented the de-facto standard for implementing a mod-
ular application on the jvm.
In reality, osgi and the new java 9 module system arent mutually exclusive; they
can coexist in the same application. In fact, their features overlap only partially. Osgi
has a much wider scope and provides many capabilities that arent available in jigsaw.
Osgi modules are called bundles and run inside a specific osgi framework. Sev-
eral certified osgi framework implementations exist, but the two with the widest
adoption are apache felix and equinox (which is also used to run the eclipse ide).
When running inside an osgi framework, a single bundle can be remotely installed,
started, stopped, updated, and uninstalled without a reboot. In other words, osgi
defines a clear life cycle for bundles made by the states listed in table 14.1.

Table 14.1 bundle states in osgi

bundle state description

installed the bundle has been successfully installed.

Resolved all java classes that the bundle needs are available.

Starting the bundle is being started, and the bundleactivator. Start
method has been called, but the start method hasnt yet returned.

Active the bundle has been successfully activated and is running.

Stopping the bundle is being stopped. The bundleactivator. Stop
method has been called, but the stop method hasnt yet returned.

Uninstalled the bundle has been uninstalled. It cant move into another state.
Java modules: the big picture 339

the possibility of hot-swapping different subparts of your application without the
need to restart it probably is the main advantage of osgi over jigsaw. Each bundle is
defined through a text file describing which external packages are required by the
bundle to work and which internal packages are publicly exported by the bundle and
then made available to other bundles.
Another interesting characteristic of osgi is that it allows different versions of the
same bundle to be installed in the framework at the same time. The java 9 module sys-
tem doesnt support version control because jigsaw still uses one single class loader
per application, whereas osgi loads each bundle in its own class loader.

14.3 java modules: the big picture
java 9 provides a new unit of java program structure: the module. A module is intro-
duced with a new keyword2 module, followed by its name and its body. Such a module
descriptor3 lives in a special file: module-info. Java, which is compiled to module-
info. Class. The body of a module descriptor consists of clauses, of which the two
most important are requires and exports. The former clause specifies what other
modules your modules need to run, and exports specifies everything that your mod-
ule wants to be visible for other modules to use. You learn about these clauses in more
detail in later sections.
A module descriptor describes and encapsulates one or more packages (and typi-
cally lives in the same folder as these packages), but in simple use cases, it exports
(makes visible) only one of these packages.
The core structure of a java module descriptor is shown in figure 14.2.

Module module-name

a single exported package
exports package-names for simple use

any number of modules,
requires module-names including zero
figure 14.2 core structure
of a java module descriptor
(module-info. Java)

it is helpful to think of the exports and requires parts of a module as being respectively
like the lugs (or tabs) and holes of a jigsaw puzzle (which is perhaps where the working
name project jigsaw originated). Figure 14.3 shows an example with several modules.

2
technically, java 9 module-forming identifierssuch as module, requires, and exportare restricted key-
words. You can still use them as identifiers elsewhere in your program (for backward compatibility), but
theyre interpreted as keywords in a context where modules are allowed.
3
legally, the textual form is called a module declaration, and the binary form in module-info. Class is referred
to as a module descriptor.
340 chapter 14 the java module system

requires b requires c

module a

exports pkgc

exports pkgb

requires d

module b module c
figure 14.3 jigsaw-puzzle-style
example of a java system built
from four modules (a, b, c, d).
Exports pkgd module a requires modules b and
c to be present, and thereby gets
access to the packages pkgb and
pkgc (exported by modules b and
c, respectively). Module c may
similarly use package pkgd,
which it has required from module
module d c, but module b cant use pkgd.

When you use tools such as maven, much of the detail of module descriptions is han-
dled by an ide and is hidden from the user.
Having said that, in the next section we explore these concepts in more detail
based on examples.

14.4 developing an application with the
java module system
in this section, you get an overview of the java 9 module system by building a simple
modular application from the ground up. You learn how to structure, package, and
launch a small modular application. This section doesnt explain each topic in detail but
shows you the big picture, so you can delve independently in more depth if needed.

14.4.1 setting up an application
to get started with the java module system, you need an example project to write
code for. Perhaps youre traveling a lot, grocery shopping, or going out for coffee with
developing an application with the java module system 341

your friends, and you have to deal with a lot of receipts. Nobody ever enjoyed manag-
ing expenses. To help yourself out, you write an application that can manage your
expenses. The application needs to conduct several tasks:
 read a list of expenses from a file or a url;
 parse the string representations of these expenses;
 calculate statistics;
 display a useful summary;
 provide a main start-up and close-down coordinator for these tasks.

You need to define different classes and interfaces to model the concepts in this appli-
cation. First, a reader interface lets you read serialized expenses from a source. Youll
have different implementations, such as httpreader or filereader, depending on
the source. You also need a parser interface to deserialize the json objects into a
domain object expense that you can manipulate in your java application. Finally, you
need a class summarycalculator to be responsible for calculating statistics, given a list
of expense objects, and to return summarystatistics objects.
Now that you have a project, how do you modularize it by using the java module
system? Its clear that the project involves several concerns, which you want to separate:
 reading data from different sources (reader, httpreader, filereader)
 parsing the data from different formats (parser, jsonparser, expensejson-
parser)
 representing domain objects (expense)
 calculating and returning statistics (summarycalculator, summarystatistics)
 coordinating the different concerns (expensesapplication)

here, well take a fine-grained approach for pedagogic reasons. You can group each
concern into a separate module, as follows (and we discuss the module-naming
scheme in more detail later):
 expenses. Readers
 expenses. Readers. Http
 expenses. Readers. File
 expenses. Parsers
 expenses. Parsers. Json
 expenses. Model
 expenses. Statistics
 expenses. Application

for this simple application, you adopt a fine-grained decomposition to exemplify the
different parts of the module system. In practice, taking such a fine-grained approach
for a simple project would result in a high up-front cost for the arguably limited bene-
fit of properly encapsulating small parts of the project. As the project grows and more
internal implementations are added, however, the encapsulation and reasoning
342 chapter 14 the java module system

benefits become more apparent. You could imagine the preceding list as being a list
of packages, depending on your application boundaries. A module groups a series of
packages. Perhaps each module has implementation-specific packages that you dont
want to expose to other modules. The expenses. Statistics module, for example,
may contain several packages for different implementations of experimental statistical
methods. Later, you can decide which of these packages to release to users.

14.4.2 fine-grained and coarse-grained modularization
when youre modularizing a system, you can choose the granularity. In the most fine-
grained scheme, every package has its own module (as in the previous section); in the
most coarse-grained scheme, a single module contains all the packages in your system.
As noted in the previous section, the first schema increases the design cost for limited
gains, and the second one loses all benefits of modularization. The best choice is a
pragmatic decomposition of the system into modules along with a regular review pro-
cess to ensure that an evolving software project remains sufficiently modularized that
you can continue to reason about it and modify it.
In short, modularization is the enemy of software rust.

14.4.3 java module system basics
lets begin with a basic modular application, which has only one module to support
the main application. The project directory structure is as follows, with each level
nested in a directory:
| expenses. Application
| module-info. Java
| com
| example
| expenses
| application
| expensesapplication. Java

youve noticed this mysterious module-info. Java that was part of the project structure.
This file is a module descriptor, as we explained earlier in the chapter, and it must be
located at the root of the modules source-code file hierarchy to let you specify the
dependencies of your module and what you want to expose. For your expenses applica-
tion, the top-level module-info. Java file contains a module description that has a
name but is otherwise empty because it neither depends on any other module nor
exposes its functionality to other modules. Youll learn about more-sophisticated fea-
tures later, starting with section 14.5. The content of module-info. Java is as follows:
module expenses. Application {

}

how do you run a modular application? Take a look at some commands to understand
the low-level parts. This code is automated by your ide and build system but seeing
working with several modules 343

whats happening is useful. When youre in the module source directory of your proj-
ect, run the following commands:
javac module-info. Java
com/example/expenses/application/expensesapplication. Java -d target

jar cvfe expenses-application. Jar
com. Example. Expenses. Application. Expensesapplication -c target

these commands produce output similar to the following, which shows which folders
and class files are incorporated into the generated jar (expenses-application. Jar):
added manifest
added module-info: module-info. Class
adding: com/(in = 0) (out= 0)(stored 0%)
adding: com/example/(in = 0) (out= 0)(stored 0%)
adding: com/example/expenses/(in = 0) (out= 0)(stored 0%)
adding: com/example/expenses/application/(in = 0) (out= 0)(stored 0%)
adding: com/example/expenses/application/expensesapplication. Class(in = 456)
(out= 306)(deflated 32%)

finally, you run the generated jar as a modular application:
java --module-path expenses-application. Jar \
--module expenses/com. Example. Expenses. Application. Expensesapplication

you should be familiar with the first two steps, which represent a standard way to pack-
age a java application into a jar. The only new part is that the file module-info. Java
becomes part of the compilation step.
The java program, which runs java . Class files, has two new options:
 --module-paththis option specifies what modules are available to load. This
option differs from the --classpath argument, which makes class files available.
 --modulethis option specifies the main module and class to run.

A modules declaration doesnt include a version string. Addressing the version-
selection problem wasnt a specific design point for the java 9 module system, so ver-
sioning isnt supported. The justification was that this problem is one for build tools
and container applications to address.

14.5 working with several modules
now that you know how to set up a basic application with one module, youre ready to
do something a bit more realistic with multiple modules. You want your expense appli-
cation to read expenses from a source. To this end, introduce a new module
expenses. Readers that encapsulates these responsibilities. The interaction between
the two modules expenses. Application and expenses. Readers is specified by the
java 9 exports and requires clauses.
344 chapter 14 the java module system

14.5.1 the exports clause
heres how we might declare the module expenses. Readers. (dont worry about the
syntax and concepts yet; we cover these topics later. )
module expenses. Readers {

exports com. Example. Expenses. Readers; these are package
exports com. Example. Expenses. Readers. File; names, not module
exports com. Example. Expenses. Readers. Http; names.
}

theres one new thing: the exports clause, which makes the public types in specific
packages available for use by other modules. By default, everything is encapsulated
within a module. The module system takes a whitelist approach that helps you get
strong encapsulation, as you need to explicitly decide what to make available for
another module to use. (this approach prevents you from accidentally exporting
some internal features that a hacker can exploit to compromise your systems several
years later. )
the directory structure of the two-module version of your project now looks like this:
| expenses. Application
| module-info. Java
| com
| example
| expenses
| application
| expensesapplication. Java

| expenses. Readers
| module-info. Java
| com
| example
| expenses
| readers
| reader. Java
| file
| filereader. Java
| http
| httpreader. Java

14.5.2 the requires clause
alternatively, you could have written module-info. Java as follows:

module expenses. Readers { this is a module name,
requires java. Base; not a package name.

Exports com. Example. Expenses. Readers; this is a package
exports com. Example. Expenses. Readers. File; name, not a
exports com. Example. Expenses. Readers. Http; module name.
}
compiling and packaging 345

the new element is the requires clause, which lets you specify what the module
depends on. By default, all modules depend on a platform module called java. Base
that includes the java main packages such as net, io, and util. This module is always
required by default, so you dont need to say so explicitly. (this is similar to how saying
"class foo { ... }" in java is equivalent to saying "class foo extends object { ... }").
It becomes useful when you need to import modules other than java. Base.
The combination of the requires and exports clauses makes access control of
classes more sophisticated in java 9. Table 14.2 summarizes the differences in visibility
with the different access modifiers before and after java 9.
Table 14.2 java 9 provides finer control over class visibility

class visibility before java 9 after java 9

all classes public to everyone   (combination of exports and
requires clauses)

limited number of classes public   (combination of exports and
requires clauses)

public inside one module only   (no exports clause)
protected  
package  
private  

14.5.3 naming
at this stage, its useful to comment on the naming convention for modules. We went
for a short approach (for example, expenses. Application) so as not to confuse the
ideas of modules and packages. (a module can export multiple packages. ) the rec-
ommended convention is different, however.
Oracle recommends you name modules following the same reverse internet domain-
name convention (for example, com. Iteratrlearning. Training) used for packages. Fur-
ther, a modules name should correspond to its principal exported api package, which
should also follow that convention. If a module doesnt have that package, or if for other
reasons it requires a name that doesnt correspond to one of its exported packages, it
should start with the reversed form of an internet domain associated with its author.
Now that youve learned how to set up a project with multiple modules, how do
you package and run it? We cover this topic in the next section.

14.6 compiling and packaging
now that youre comfortable with setting a project and declaring a module, youre
ready to see how you can use build tools like maven to compile your project. This sec-
tion assumes that youre familiar with maven, which is one of the most common build
346 chapter 14 the java module system

tools in the java ecosystem. Another popular building tool is gradle, which we encour-
age you to explore if you havent heard of it.
First, you need to introduce a pom. Xml file for each module. In fact, each module
can be compiled independently so that it behaves as a project on its own. You also
need to add a pom. Xml for the parent of all the modules to coordinate the build for
the whole project. The overall structure now looks as follows:

| pom. Xml
| expenses. Application
| pom. Xml
| src
| main
| java
| module-info. Java
| com
| example
| expenses
| application
| expensesapplication. Java
| expenses. Readers
| pom. Xml
| src
| main
| java
| module-info. Java
| com
| example
| expenses
| readers
| reader. Java
| file
| filereader. Java
| http
| httpreader. Java

notice the three new pom. Xml files and the maven directory project structure. The
module descriptor (module-info. Java) needs to be located in the src/main/java
directory. Maven will set up javac to use the appropriate module source path.
The pom. Xml for the expenses. Readers project looks like this:

<? Xml version="1.0" encoding="utf-8"? >
<project xmlns="http: //maven. Apache. Org/pom/4.0.0"
xmlns: xsi="http: //www. W3. Org/2001/xmlschema-instance"
xsi: schemalocation="http: //maven. Apache. Org/pom/4.0.0
http: //maven. Apache. Org/xsd/maven-4.0.0. Xsd">
<modelversion>4.0.0</modelversion>

<groupid>com. Example</groupid>
<artifactid>expenses. Readers</artifactid>
<version>1.0</version>
<packaging>jar</packaging>
compiling and packaging 347

<parent>
<groupid>com. Example</groupid>
<artifactid>expenses</artifactid>
<version>1.0</version>
</parent>
</project>

the important thing to note is that this code explicitly mentions the parent module to
help in the build process. The parent is the artifact with the id expenses. You need to
define the parent in pom. Xml, as you see shortly.
Next, you need to specify the pom. Xml for the expenses. Application module.
This file is similar to the preceding one, but you have to add a dependency to the
expenses. Readers project, because expensesapplication requires the classes and
interfaces that it contains to compile:
<? Xml version="1.0" encoding="utf-8"? >
<project xmlns="http: //maven. Apache. Org/pom/4.0.0"
xmlns: xsi="http: //www. W3. Org/2001/xmlschema-instance"
xsi: schemalocation="http: //maven. Apache. Org/pom/4.0.0
http: //maven. Apache. Org/xsd/maven-4.0.0. Xsd">
<modelversion>4.0.0</modelversion>

<groupid>com. Example</groupid>
<artifactid>expenses. Application</artifactid>
<version>1.0</version>
<packaging>jar</packaging>

<parent>
<groupid>com. Example</groupid>
<artifactid>expenses</artifactid>
<version>1.0</version>
</parent>

<dependencies>
<dependency>
<groupid>com. Example</groupid>
<artifactid>expenses. Readers</artifactid>
<version>1.0</version>
</dependency>
</dependencies>

</project>

now that two modules, expenses. Application and expenses. Readers, have their own
pom. Xml, you can set up the global pom. Xml to guide the build process. Maven supports
projects that have multiple maven modules with the special xml element <module>,
which refers to the childrens artifact ids. Heres the complete definition, which
refers to the two child modules expenses. Application and expenses. Readers:
<? Xml version="1.0" encoding="utf-8"? >
<project xmlns="http: //maven. Apache. Org/pom/4.0.0"
xmlns: xsi="http: //www. W3. Org/2001/xmlschema-instance"
348 chapter 14 the java module system

xsi: schemalocation="http: //maven. Apache. Org/pom/4.0.0
http: //maven. Apache. Org/xsd/maven-4.0.0. Xsd">
<modelversion>4.0.0</modelversion>

<groupid>com. Example</groupid>
<artifactid>expenses</artifactid>
<packaging>pom</packaging>
<version>1.0</version>

<modules>
<module>expenses. Application</module>
<module>expenses. Readers</module>
</modules>

<build>
<pluginmanagement>
<plugins>
<plugin>
<groupid>org. Apache. Maven. Plugins</groupid>
<artifactid>maven-compiler-plugin</artifactid>
<version>3.7.0</version>
<configuration>
<source>9</source>
<target>9</target>
</configuration>
</plugin>
</plugins>
</pluginmanagement>
</build>
</project>

congratulations! Now you can run the command mvn clean package to generate the
jars for the modules in your project. This command generates
. /expenses. Application/target/expenses. Application-1.0. Jar
. /expenses. Readers/target/expenses. Readers-1.0. Jar

you can run your module application by including these two jars on the module path
as follows:
java --module-path \
. /expenses. Application/target/expenses. Application-1.0. Jar: \
. /expenses. Readers/target/expenses. Readers-1.0. Jar \
--module \
expenses. Application/com. Example. Expenses. Application. Expensesapplication

so far, youve learned about modules you created, and youve seen how to use
requires to reference java. Base. Real-world software, however, depends on external
modules and libraries. How does that process work, and what if legacy libraries
havent been updated with an explicit module-info. Java? In the next section, we
answer these questions by introducing automatic modules.
Automatic modules 349

14.7 automatic modules
you may decide that the implementation of your httpreader is low-level; instead,
youd like to use a specialized library such as the httpclient from the apache project.
How do you incorporate that library into your project? Youve learned about the
requires clause, so try to add it in the module-info. Java for the expenses. Readers
project. Run mvn clean package again to see what happens. Unfortunately, the result
is bad news:

[error] module not found: httpclient

you get this error because you also need to update your pom. Xml to state the depen-
dency. The maven compiler plugin puts all dependencies on the module path when
youre building a project that has a module-info. Java so that the appropriate jars
are downloaded and recognized in your project, as follows:
<dependencies>
<dependency>
<groupid>org. Apache. Httpcomponents</groupid>
<artifactid>httpclient</artifactid>
<version>4.5.3</version>
</dependency>
</dependencies>

now running mvn clean package builds the project correctly. Notice something inter-
esting, though: the library httpclient isnt a java module. Its an external library that
you want to use as a module, but it hasnt yet been modularized. Java turns the appro-
priate jar into a so-called automatic module. Any jar on the module path without a
module-info file becomes an automatic module. Automatic modules implicitly export
all their packages. A name for this automatic module is invented automatically,
derived from the jar name. You have a few ways to derive the name, but the easiest
way is to use the jar tool with the --describe-module argument:
jar --file=. /expenses. Readers/target/dependency/httpclient-4.5.3. Jar \
--describe-module
httpclient@4.5.3 automatic

in this case, the name is httpclient.
The final step is running the application and adding the httpclient jar to the
module path:
java --module-path \
. /expenses. Application/target/expenses. Application-1.0. Jar: \
. /expenses. Readers/target/expenses. Readers-1.0. Jar \
. /expenses. Readers/target/dependency/httpclient-4.5.3. Jar \
--module \
expenses. Application/com. Example. Expenses. Application. Expensesapplication
350 chapter 14 the java module system

note theres a project (https: //github. Com/moditect/moditect) to provide
better support for the java 9 module system within maven, such as to gener-
ate module-info files automatically.

14.8 module declaration and clauses
the java module system is a large beast. As we mentioned earlier, we recommend that
you read a dedicated book on the topic if youd like to go further. Nonetheless, this
section gives you a brief overview of other keywords available in the module declara-
tion language to give you an idea of whats possible.
As you learned in the earlier sections, you declare a module by using the module
directive. Here, it has the name com. Iteratrlearning. Application:
module com. Iteratrlearning. Application {

}

what can go inside the module declaration? Youve learned about the requires
and exports clauses, but there are other clauses, including requires-transitive,
exports-to, open, opens, uses, and provides. We look at these clauses in turn in
the following sections.

14.8.1 requires
the requires clause lets you specify that your module depends on another module at
both compile time and runtime. The module com. Iteratrlearning. Application,
for example, depends on the module com. Iteratrlearning. Ui:
module com. Iteratrlearning. Application {
requires com. Iteratrlearning. Ui;
}

the result is that only public types that were exported by com. Iteratrlearning. Ui
are available for com. Iteratrlearning. Application to use.

14.8.2 exports
the exports clause makes the public types in specific packages available for use by
other modules. By default, no package is exported. You gain strong encapsulation
by making explicit what packages should be exported. In the following example, the
packages com. Iteratrlearning. Ui. Panels and com. Iteratrlearning. Ui. Widgets
are exported. (note that exports takes a package name as an argument and that
requires takes a module name, despite the similar naming schemes. )
module com. Iteratrlearning. Ui {
requires com. Iteratrlearning. Core;
exports com. Iteratrlearning. Ui. Panels;
exports com. Iteratrlearning. Ui. Widgets;
}
module declaration and clauses 351

14.8.3 requires transitive
you can specify that a module can use the public types required by another module.
You can modify the requires clause, for example, to requires-transitive inside the
declaration of the module com. Iteratrlearning. Ui:

module com. Iteratrlearning. Ui {
requires transitive com. Iteratrlearning. Core;

exports com. Iteratrlearning. Ui. Panels;
exports com. Iteratrlearning. Ui. Widgets;
}

module com. Iteratrlearning. Application {
requires com. Iteratrlearning. Ui;
}

the result is that the module com. Iteratrlearning. Application has access to the
public types exported by com. Iteratrlearning. Core. Transitivity is useful when
the module required (here, com. Iteratrlearning. Ui) returns types from another
module required by this module (com. Iteratrlearning. Core). It would be annoy-
ing to re-declare requires com. Iteratrlearning. Core inside the module com
. Iteratrlearning. Application. This problem is solved by transitive. Now any
module that depends on com. Iteratrlearning. Ui automatically reads the com
. Iteratrlearning. Core module.

14.8.4 exports to
you have a further level of visibility control, in that you can restrict the allowed users
of a particular export by using the exports to construct. As you saw in section 14.8.2,
you can restrict the allowed users of com. Iteratrlearning. Ui. Widgets to com
. Iteratrlearning. Ui. Widgetuser by adjusting the module declaration like so:

module com. Iteratrlearning. Ui {
requires com. Iteratrlearning. Core;

exports com. Iteratrlearning. Ui. Panels;
exports com. Iteratrlearning. Ui. Widgets to
com. Iteratrlearning. Ui. Widgetuser;
}

14.8.5 open and opens
using the open qualifier on module declaration gives other modules reflective access
to all its packages. The open qualifier has no effect on module visibility except for
allowing reflective access, as in this example:
open module com. Iteratrlearning. Ui {

}
352 chapter 14 the java module system

before java 9, you could inspect the private state of objects by using reflection. In
other words, nothing was truly encapsulated. Object-relational mapping (orm) tools
such as hibernate often use this capability to access and modify state directly. In java
9, reflection is no longer allowed by default. The open clause in the preceding code
serves to allow that behavior when its needed.
Instead of opening a whole module to reflection, you can use an opens clause
within a module declaration to open its packages individually, as required. You can
also use the to qualifier in the opens-to variant to limit the modules allowed to per-
form reflective access, analogous to how exports-to limits the modules allowed to
require an exported package.

14.8.6 uses and provides
if youre familiar with services and serviceloader, the java module system allows you
to specify a module as a service provider using the provides clause and a service con-
sumer using the uses clause. This topic is advanced, however, and beyond the scope
of this chapter. If youre interested in combining modules and service loaders, we rec-
ommend that you read a comprehensive resource such as the java module system, by
nicolai parlog (manning publications), mentioned earlier in this chapter.

14.9 a bigger example and where to learn more
you can get a flavor of the module system from the following example, taken from
oracles java documentation. This example shows a module declaration using most of
the features discussed in this chapter. The example isnt meant to frighten you (the
vast majority of module statements are simple exports and requires), but it gives you a
look at some richer features:
module com. Example. Foo {
requires com. Example. Foo. Http;
requires java. Logging;

requires transitive com. Example. Foo. Network;

exports com. Example. Foo. Bar;
exports com. Example. Foo. Internal to com. Example. Foo. Probe;

opens com. Example. Foo. Quux;
opens com. Example. Foo. Internal to com. Example. Foo. Network,
com. Example. Foo. Probe;

uses com. Example. Foo. Spi. Intf;
provides com. Example. Foo. Spi. Intf with com. Example. Foo. Impl;
}

this chapter discussed the need for the new java module system and provided a gentle
introduction to its main features. We didnt cover many features, including service load-
ers, additional module descriptor clauses, and tools for working with modules such as
jdeps and jlink. If youre a java ee developer, its important to keep in mind when
summary 353

migrating your applications to java 9 that several packages relevant to ee arent loaded
by default in the modularized java 9 virtual machine. The jaxb api classes, for example,
are now considered to be java ee apis and are no longer available in the default class
path in java se 9. You need to explicitly add modules of interest by using the --add-
modules command-line switch to keep compatibility. To add java. Xml. Bind, for exam-
ple, you need to specify --add-modules java. Xml. Bind.
As we noted earlier, doing the java module system justice would require a whole
book, not a single chapter. To explore the details in greater depth, we suggest a book
such as the java module system, by nicolai parlog (manning publications), mentioned
earlier in this chapter.

Summary
 separation of concerns and information hiding are two important principles to
help construct software that you can reason about.
 Before java 9, you made code modular by introducing packages, classes, and
interfaces that have a specific concern, but these elements werent rich enough
for effective encapsulation.
 The class path hell problem makes it hard to reason about the dependencies
of an application.
 Before java 9, the jdk was monolithic, resulting in high maintenance costs and
restricted evolution.
 Java 9 introduced a new module system in which a module-info. Java file names
a module and specifies its dependencies (requires) and public api (exports).
 The requires clause lets you specify dependencies on other modules.
 The exports clause makes the public types of specific packages in a module
available for use by other modules.
 The prefered naming convention for a module follows the reverse internet
domain-name convention.
 Any jar on the module path without a module-info file becomes an automatic
module.
 Automatic modules implicitly export all their packages.
 Maven supports applications structured with the java 9 module system.
Part 5

enhanced java concurrency

t he fifth part of this book explores the more advanced ways of structuring
concurrent programs in javabeyond the ideas of easy-to-use parallel process-
ing for streams introduced in chapters 6 and 7. Again, nothing in the rest of the
book depends on this part, so do feel free to skip this part if you dont (yet) need
to explore these ideas.
Chapter 15 is new to this second edition and covers the big-picture idea of
asynchronous apis, including the ideas of futures and the publish-subscribe
protocol behind reactive programming and encapsulated in the java 9 flow api.
Chapter 16 explores completablefuture, which lets you express complex
asynchronous computations in a declarative wayparalleling the design of the
streams api.
Chapter 17 is also new to this second edition and explores the java 9 flow
api in detail, focusing on practical reactive programming code.
Concepts behind
completablefuture and
reactive programming

this chapter covers
 threads, futures, and the evolutionary forces
causing java to support richer concurrency apis
 asynchronous apis
 the boxes-and-channels view of concurrent
computing
 completablefuture combinators to connect boxes
dynamically
 the publish-subscribe protocol that forms the
basis of the java 9 flow api for reactive
programming
 reactive programming and reactive systems

in recent years, two trends are obliging developers to rethink the way software is
written. The first trend is related to the hardware on which applications run, and
the second trend concerns how applications are structured (particularly how they
interact). We discussed the effect of the hardware trend in chapter 7. We noted that
since the advent of multicore processors, the most effective way to speed your appli-
cations is to write software that can fully exploit multicore processors. You saw that

357
358 chapter 15 concepts behind completablefuture and reactive programming

you can split large tasks and make each subtask run in parallel with the others. You
also learned how the fork/join framework (available since java 7) and parallel streams
(new in java 8) allow you to accomplish this task in a simpler, more effective way than
working directly with threads.
The second trend reflects the increasing availability and use by applications of
internet services. The adoption of microservices architecture, for example, has
grown over the past few years. Instead of being one monolithic application, your
application is subdivided into smaller services. The coordination of these smaller
services requires increased network communication. Similarly, many more internet
services are accessible through public apis, made available by known providers such
as google (localization information), facebook (social information), and twitter
(news). Nowadays, its relatively rare to develop a website or a network application
that works in total isolation. Its far more likely that your next web application will
be a mashup, using content from multiple sources and aggregating it to ease your
users lives.
You may want to build a website that collects and summarizes social-media senti-
ment on a given topic to your french users. To do so, you could use the facebook or
twitter api to find trending comments about that topic in many languages and rank
the most relevant ones with your internal algorithms. Then you might use google
translate to translate the comments into french or use google maps to geolocate
their authors, aggregate all this information, and display it on your website.
If any of these external network services are slow to respond, of course, youll want
to provide partial results to your users, perhaps showing your text results alongside a
generic map with a question mark in it instead of showing a blank screen until the
map server responds or times out. Figure 15.1 illustrates how this style of mashup appli-
cation interacts with remote services.

Your
program
comments ranking french
topic sentiment
tweets sentiment

facebook

remote
googletranslate
twitter services

figure 15.1 a typical mashup application

to implement applications like this, you have to contact multiple web services across
the internet. But you dont want to block your computations and waste billions of
precious clock cycles of your cpu waiting for an answer from these services. You
359

shouldnt have to wait for data from facebook before processing the data coming
from twitter, for example.
This situation represents the other side of the multitask-programming coin. The
fork/join framework and parallel streams, discussed in chapter 7, are valuable tools
for parallelism; they divide a task into multiple subtasks and perform those subtasks in
parallel on different cores, cpus, or even machines.
Conversely, when youre dealing with concurrency instead of parallelism, or when your
main goal is to perform several loosely related tasks on the same cpus, keeping their
cores as busy as possible to maximize the throughput of your application, you want to
avoid blocking a thread and wasting its computational resources while waiting (potentially
for quite a while) for a result from a remote service or from interrogating a database.
Java offers two main tool sets for such circumstances. First, as youll see in chap-
ters 16 and 17, the future interface, and particularly its java 8 completablefuture
implementation, often provide simple and effective solutions (chapter 16). More
recently, java 9 added the idea of reactive programming, built around the idea of the
so-called publish-subscribe protocol via the flow api, which offers more sophisticated
programming approaches (chapter 17).
Figure 15.2 illustrates the difference between concurrency and parallelism. Con-
currency is a programming property (overlapped execution) that can occur even for a
single-core machine, whereas parallelism is a property of execution hardware (simul-
taneous execution).

Concurrency parallelism

core 1 core 2 core 1 core 2

task1

task2 task1 task2

task1

figure 15.2 concurrency versus parallelism

the rest of this chapter explains the fundamental ideas underpinning javas new
completablefuture and flow apis.
We start by explaining the java evolution of concurrency, including threads, and
higher-level abstractions, including thread pools and futures (section 15.1). We note
that chapter 7 dealt with mainly using parallelism in looplike programs. Section 15.2
explores how you can better exploit concurrency for method calls. Section 15.3 gives
you a diagrammatic way to see parts of programs as boxes that communicate over
360 chapter 15 concepts behind completablefuture and reactive programming

guidance for the reader
this chapter contains little real-life java code. We suggest that readers who want to
see only code skip to chapters 16 and 17. On the other hand, as weve all discovered,
code that implements unfamiliar ideas can be hard to understand. Therefore, we use
simple functions and include diagrams to explain the big-picture ideas, such as the pub-
lish-subscribe protocol behind the flow api capturing reactive programming.

Channels. Section 15.4 and section 15.5 look at the completablefuture and reactive-
programming principles in java 8 and 9. Finally, section 15.6 explains the difference
between a reactive system and reactive programming.
We exemplify most of the concepts with a running example showing how to calculate
expressions such as f(x)+g(x) and then return, or print, the result by using various java
concurrency featuresassuming that f(x) and g(x) are long-running computations.

15.1 evolving java support for expressing concurrency
java has evolved considerably in its support for concurrent programming, largely
reflecting the changes in hardware, software systems, and programming concepts over
the past 20 years. Summarizing this evolution can help you understand the reason for
the new additions and their roles in programming and system design.
Initially, java had locks (via synchronized classes and methods), runnables and
threads. In 2004, java 5 introduced the java. Util. Concurrent package, which sup-
ported more expressive concurrency, particularly the executorservice1 interface
(which decoupled task submission from thread execution), as well as callable<t>
and future<t>, which produced higher-level and result-returning variants of runnable
and thread and used generics (also introduced in java 5). Executorservices can exe-
cute both runnables and callables. These features facilitated parallel programming
on the multicore cpus that started to appear the following year. To be honest, nobody
enjoyed working with threads directly!
Later versions of java continued to enhance concurrency support, as it became
increasingly demanded by programmers who needed to program multicore cpus
effectively. As you saw in chapter 7, java 7 added java. Util. Concurrent. Recursive-
task to support fork/join implementation of divide-and-conquer algorithms, and java 8
added support for streams and their parallel processing (building on the newly added
support for lambdas).
Java further enriched its concurrency features by providing support for composing
futures (via the java 8 completablefuture implementation of future, section 15.4
and chapter 16), and java 9, provided explicit support for distributed asynchronous
programming. These apis give you a mental model and toolkit for building the sort
of mashup application mentioned in the introduction to this chapter. There the

1
the executorservice interface extends the executor interface with the submit method to run a
callable; the executor interface merely has an execute method for runnables.
Evolving java support for expressing concurrency 361

application worked by contacting various web services and combining their informa-
tion in real time for a user or to expose it as a further web service. This process is
called reactive programming, and java 9 provides support for it via the publish-subscribe
protocol (specified by the java. Util. Concurrent. Flow interface; see section 15.5 and
chapter 17). A key concept of completablefuture and java. Util. Concurrent. Flow is
to provide programming structures that enable independent tasks to execute concur-
rently wherever possible and in a way that easily exploits as much as possible of the par-
allelism provided by multicore or multiple machines.

15.1.1 threads and higher-level abstractions
many of us learned about threads and processes from a course on operating systems. A
single-cpu computer can support multiple users because its operating system allocates
a process to each user. The operating system gives these processes separate virtual
address spaces so that two users feel like theyre the only users of the computer. The
operating system furthers this illusion by waking periodically to share the cpu among
the processes. A process can request that the operating system allocate it one or more
threadsprocesses that share an address space as their owning process and therefore
can run tasks concurrently and cooperatively.
In a multicore setting, perhaps a single-user laptop running only one user process,
a program can never fully exploit the computing power of the laptop unless it uses
threads. Each core can be used for one or more processes or threads, but if your pro-
gram doesnt use threads, its effectively using only one of the processor cores.
Indeed, if you have a four-core cpu and can arrange for each core to continually
do useful work, your program theoretically runs up to four times faster. (overheads
reduce this result somewhere, of course. ) given an array of numbers of size 1,000,000
storing the number of correct questions answered by students in an example, com-
pare the program
long sum = 0;
for (int i = 0; i < 1_000_000; i++) {
sum += stats[i];
}

running on a single thread, which worked fine in single-core days, with a version that
creates four threads, with the first thread executing
long sum0 = 0;
for (int i = 0; i < 250_000; i++) {
sum0 += stats[i];
}

and to the fourth thread executing
long sum3 = 0;
for (int i = 750_000; i < 1_000_000; i++) {
sum3 += stats[i];
}
362 chapter 15 concepts behind completablefuture and reactive programming

these four threads are complemented by the main program starting them in turn
(. Start() in java), waiting for them to complete (. Join()), and then computing

sum = sum0 + ... + sum3;

the trouble is that doing this for each loop is tedious and error-prone. Also, what can
you do for code that isnt a loop?
Chapter 7 showed how java streams can achieve this parallelism with little program-
mer effort by using internal iteration instead of external iteration (explicit loops):

sum = arrays. Stream(stats). Parallel(). Sum();

the takeaway idea is that parallel stream iteration is a higher-level concept than
explicit use of threads. In other words, this use of streams abstracts a given use pattern
of threads. This abstraction into streams is analogous to a design pattern, but with the
benefit that much of the complexity is implemented inside the library rather than
being boilerplate code. Chapter 7 also explained how to use java. Util. Concurrent
. Recursivetask support in java 7 for the fork/join abstraction of threads to paral-
lelize divide-and-conquer algorithms, providing a higher-level way to sum the array
efficiently on a multicore machine.
Before looking at additional abstractions for threads, we visit the (java 5) idea of
executorservices and the thread pools on which these further abstractions are built.

15.1.2 executors and thread pools
java 5 provided the executor framework and the idea of thread pools as a higher-level
idea capturing the power of threads, which allow java programmers to decouple task
submission from task execution.
Problems with threads
java threads access operating-system threads directly. The problem is that operating-sys-
tem threads are expensive to create and to destroy (involving interaction with page
tables), and moreover, only a limited number exist. Exceeding the number of operating-
system threads is likely to cause a java application to crash mysteriously, so be careful not
to leave threads running while continuing to create new ones.
The number of operating system (and java) threads will significantly exceed the
number of hardware threads2, so all the hardware threads can be usefully occupied
executing code even when some operating-system threads are blocked or sleeping. As
an example, the 2016 intel core i7-6900k server processor has eight cores, each with
two symmetric multiprocessing (smp) hardware threads, leading to 16 hardware
threads, and a server may contain several of these processors, consisting of perhaps 64
hardware threads. By contrast, a laptop may have only one or two hardware threads, so
portable programs must avoid making assumptions about how many hardware threads

2
wed use the word core here, but cpus like the intel i7-6900k have multiple hardware threads per core, so the
cpu can execute useful instructions even for short delays such as a cache miss.
Evolving java support for expressing concurrency 363

are available. Contrarily, the optimum number of java threads for a given program
depends on the number of hardware cores available!
Thread pools and why theyre better
the java executorservice provides an interface where you can submit tasks and obtain
their results later. The expected implementation uses a pool of threads, which can be
created by one of the factory methods, such as the newfixedthreadpool method:

executorservice newfixedthreadpool(int nthreads)

this method creates an executorservice containing nthreads (often called worker
threads) and stores them in a thread pool, from which unused threads are taken to run
submitted tasks on a first-come, first-served basis. These threads are returned to the pool
when their tasks terminate. One great outcome is that its cheap to submit thousands of
tasks to a thread pool while keeping the number of tasks to a hardware-appropriate
number. Several configurations are possible, including the size of the queue, rejection
policy, and priority for different tasks.
Note the wording: the programmer provides a task (a runnable or a callable),
which is executed by a thread.
Thread pools and why theyre worse
thread pools are better than explicit thread manipulation in almost all ways, but you
need to be aware of two gotchas:
 a thread pool with k threads can execute only k tasks concurrently. Any further
task submissions are held in a queue and not allocated a thread until one of the
existing tasks completes. This situation is generally good, in that it allows you to
submit many tasks without accidentally creating an excessive number of
threads, but you have to be wary of tasks that sleep or wait for i/o or network
connections. In the context of blocking i/o, these tasks occupy worker threads
but do no useful work while theyre waiting. Try taking four hardware threads
and a thread pool of size 5 and submitting 20 tasks to it (figure 15.3). You might
expect that the tasks would run in parallel until all 20 have completed. But sup-
pose that three of the first-submitted tasks sleep or wait for i/o. Then only two
threads are available for the remaining 15 tasks, so youre getting only half the
throughput you expected (and would have if you created the thread pool with
eight threads instead). Its even possible to cause deadlock in a thread pool if
earlier task submissions or already running tasks, need to wait for later task sub-
missions, which is a typical use-pattern for futures.
The takeaway is to try to avoid submitting tasks that can block (sleep or wait
for events) to thread pools, but you cant always do so in existing systems.
 Java typically waits for all threads to complete before allowing a return from
main to avoid killing a thread executing vital code. Therefore, its important in
practice and as part of good hygiene to shut down every thread pool before exit-
ing the program (because worker threads for this pool will have been created but
364 chapter 15 concepts behind completablefuture and reactive programming

queued tasks

five worker
treads running
five active tasks

if these three tasks sleep or otherwise block, then these 15 queued tasks
all have to wait for the remaining two workers to run themreducing the
execution parallelism.

Figure 15.3 sleeping tasks reduce the throughput of thread pools.

Not terminated, as theyre waiting for another task submission). In practice, its
common to have a long-running executorservice that manages an always-
running internet service.
Java does provide the thread. Setdaemon method to control this behavior,
which we discuss in the next section.

15.1.3 other abstractions of threads: non-nested with method calls
to explain why the forms of concurrency used in this chapter differ from those used
in chapter 7 (parallel stream processing and the fork/join framework), well note that
the forms used in chapter 7 have one special property: whenever any task (or thread)
is started within a method call, the same method call waits for it to complete before
returning. In other words, thread creation and the matching join() happen in a way
that nests properly within the call-return nesting of method calls. This idea, called
strict fork/join, is depicted in figure 15.4.
Its relatively innocuous to have a more relaxed form of fork/join in which a
spawned task escapes from an internal method call but is joined in an outer call, so that
the interface provided to users still appears to be a normal call,3 as shown in figure 15.5.
3
compare thinking functionally (chapter 18) in which we discuss having a side-effect-free interface to a
method that internally uses side-effects!
Evolving java support for expressing concurrency 365

join
call return

fork

figure 15.4 strict fork/join. Arrows denote threads, circles represent forks
and joins, and rectangles represent method calls and returns.

Escaping spawned thread

call return

fork join
figure 15.5 relaxed fork/join

in this chapter, we focus on richer forms of concurrency in which threads created (or
tasks spawned) by a users method call may outlive the call, as shown in figure 15.6.

Ongoing thread

call return
fork
figure 15.6 an asynchronous method
method

this type of method is often called an asynchronous method, particularly when the
ongoing spawned task continues to do work thats helpful to the method caller. We
explore java 8 and 9 techniques for benefiting from such methods later in this chap-
ter, starting in section 15.2, but first, check the dangers:
 the ongoing thread runs concurrently with the code following the method call
and therefore requires careful programming to avoid data races.
 What happens if the java main() method returns before the ongoing thread has
terminated? There are two answers, both rather unsatisfactory:
 wait for all such outstanding threads before exiting the application.
 Kill all outstanding threads and then exit.
366 chapter 15 concepts behind completablefuture and reactive programming

the former solution risks a seeming application crash by never terminating due to a
forgotten thread; the latter risks interrupting a sequence of i/o operations writing to
disk, thereby leaving an external data in an inconsistent state. To avoid both of these
problems, ensure that your program keeps track of all threads it creates and joins
them all before exiting (including shutting down any thread pools).
Java threads can be labeled as daemon4 or nondaemon, using the setdaemon()
method call. Daemon threads are killed on exit (and therefore are useful for services
that dont leave the disk in an inconsistent state), whereas returning from main contin-
ues to wait for all threads that arent daemons to terminate before exiting the program.

15.1.4 what do you want from threads?
What you want is to be able to structure your program so that whenever it can benefit
from parallelism, enough tasks are available to occupy all the hardware threads, which
means structuring your program to have many smaller tasks (but not too small
because of the cost of task switching). You saw how to do this for loops and divide-
conquer algorithms in chapter 7, using parallel stream processing and fork/join, but
in the rest of this chapter (and in chapters 16 and 17), you see how to do it for
method calls without writing swaths of boilerplate thread-manipulation code.

15.2 synchronous and asynchronous apis
chapter 7 showed you that java 8 streams give you a way to exploit parallel hardware.
This exploitation happens in two stages. First, you replace external iteration (explicit
for loops) with internal iteration (using stream methods). Then you can use the
parallel() method on streams to allow the elements to be processed in parallel by
the java runtime library instead of rewriting every loop to use complex thread-creation
operations. An additional advantage is that the runtime system is much better informed
about the number of available threads when the loop is executed than is the program-
mer, who can only guess.
Situations other than loop-based computations can also benefit from parallelism.
An important java development that forms the background of this chapter and chap-
ters 16 and 17 is asynchronous apis.
Lets take for a running example the problem of summing the results of calls to
methods f and g with signatures:

int f(int x);
int g(int x);

for emphasis, well refer to these signatures as a synchronous api, as they return their
results when they physically return, in a sense that will soon become clear. You might

4
etymologically, daemon and demon arise from the same greek word, but daemon captures the idea of a helpful
spirit, whereas demon captures the idea of an evil spirit. Unix coined the word daemon for computing pur-
poses, using it for system services such as sshd, a process or thread that listens for incoming ssh connections.
Synchronous and asynchronous apis 367

invoke this api with a code fragment that calls them both and prints the sum of
their results:
int y = f(x);
int z = g(x);
system. Out. Println(y + z);

now suppose that methods f and g execute for a long time. (these methods could
implement a mathematical optimization task, such as gradient descent, but in chap-
ters 16 and 17, we consider more-practical cases in which they make internet queries. )
in general, the java compiler can do nothing to optimize this code because f and g
may interact in ways that arent clear to the compiler. But if you know that f and g dont
interact, or you dont care, you want to execute f and g in separate cpu cores, which
makes the total execution time only the maximum of that of the calls to f and g instead
of the sum. All you need to do is run the calls to f and g in separate threads. This idea
is a great one, but it complicates5 the simple code from before:
class threadexample {

public static void main(string[] args) throws interruptedexception {
int x = 1337;
result result = new result();

thread t1 = new thread(() -> { result. Left = f(x); } );
thread t2 = new thread(() -> { result. Right = g(x); });
t1. Start();
t2. Start();
t1. Join();
t2. Join();
system. Out. Println(result. Left + result. Right);
}

private static class result {
private int left;
private int right;
}
}

you can simplify this code somewhat by using the future api interface instead of
runnable. Assuming that you previously set up a thread pool as an executorservice
(such as executorservice), you can write
public class executorserviceexample {
public static void main(string[] args)
throws executionexception, interruptedexception {

int x = 1337;

5
some of the complexity here has to do with transferring results back from the thread. Only final outer-object
variables can be used in lambdas or inner classes, but the real problem is all the explicit thread manipulation.
368 chapter 15 concepts behind completablefuture and reactive programming

executorservice executorservice = executors. Newfixedthreadpool(2);
future<integer> y = executorservice. Submit(() -> f(x));
future<integer> z = executorservice. Submit(() -> g(x));
system. Out. Println(y. Get() + z. Get());

executorservice. Shutdown();
}
}

but this code is still polluted by the boilerplate code involving explicit calls to submit.
You need a better way of expressing this idea, analogous to how internal iteration on
streams avoided the need to use thread-creation syntax to parallelize external iteration.
The answer involves changing the api to an asynchronous api.6 instead of allowing
a method to return its result at the same time that it physically returns to the caller
(synchronously), you allow it to return physically before producing its result, as shown
in figure 15.6. Thus, the call to f and the code following this call (here, the call to g)
can execute in parallel. You can achieve this parallelism by using two techniques, both
of which change the signatures of f and g.
The first technique uses java futures in a better way. Futures appeared in java 5 and
were enriched into completablefuture in java 8 to make them composable; we explain
this concept in section 15.4 and explore the java api in detail with a worked java code
example in chapter 16. The second technique is a reactive-programming style that uses
the java 9 java. Util. Concurrent. Flow interfaces, based on the publish-subscribe pro-
tocol explained in section 15.5 and exemplified with practical code in chapter 17.
How do these alternatives affect the signatures of f and g?

15.2.1 future-style api
in this alternative, change the signature of f and g to
future<integer> f(int x);
future<integer> g(int x);

and change the calls to
future<integer> y = f(x);
future<integer> z = g(x);
system. Out. Println(y. Get() + z. Get());

the idea is that method f returns a future, which contains a task that continues to
evaluate its original body, but the return from f happens as quickly as possible after
the call. Method g similarly returns a future, and the third code line uses get() to wait
for both futures to complete and sums their results.

6
synchronous apis are also known as blocking apis, as the physical return is delayed until the result is ready
(clearest when considering a call to an i/o operation), whereas asynchronous apis can naturally implement
nonblocking i/o (where the api call merely initiates the i/o operation without waiting for the result, pro-
vided that the library at hand, such as netty, supports nonblocking i/o operations).
Synchronous and asynchronous apis 369

in this case, you could have left the api and call of g unchanged without reducing
parallelismonly introducing futures for f. You have two reasons not to do so in big-
ger programs:
 other uses of g may require a future-style version, so you prefer a uniform
api style.
 To enable parallel hardware to execute your programs as fast as possible, its
useful to have more and smaller tasks (within reason).

15.2.2 reactive-style api
in the second alternative, the core idea is to use callback-style programming by chang-
ing the signature of f and g to

void f(int x, intconsumer dealwithresult);

this alternative may seem to be surprising at first. How can f work if it doesnt
return a value? The answer is that you instead pass a callback 7 (a lambda) to f as an
additional argument, and the body of f spawns a task that calls this lambda with the
result when its ready instead of returning a value with return. Again, f returns
immediately after spawning the task to evaluate the body, which results in the follow-
ing style of code:

public class callbackstyleexample {
public static void main(string[] args) {

int x = 1337;
result result = new result();

f(x, (int y) -> {
result. Left = y;
system. Out. Println((result. Left + result. Right));
} );

g(x, (int z) -> {
result. Right = z;
system. Out. Println((result. Left + result. Right));
});

}
}

ah, but this isnt the same! Before this code prints the correct result (the sum of the
calls to f and g), it prints the fastest value to complete (and occasionally instead prints

7
some authors use the term callback to mean any lambda or method reference passed as an argument to a
method, such as the argument to stream. Filter or stream. Map. We use it only for those lambda and
method references that can be called after the method has returned.
370 chapter 15 concepts behind completablefuture and reactive programming

the sum twice, as theres no locking here, and both operands to + could be updated
before either of the println calls is executed). There are two answers:
 you could recover the original behavior by invoking println after testing with
if-then-else that both callbacks have been called, perhaps by counting them with
appropriate locking.
 This reactive-style api is intended to react to a sequence of events, not to single
results, for which futures are more appropriate.

Note that this reactive style of programming allows methods f and g to invoke their
callback dealwithresult multiple times. The original versions of f and g were obliged
to use a return that can be performed only once. Similarly, a future can be completed
only once, and its result is available to get(). In a sense, the reactive-style asynchronous
api naturally enables a sequence (which we will later liken to a stream) of values,
whereas the future-style api corresponds to a one-shot conceptual framework.
In section 15.5, we refine this core-idea example to model a spreadsheet call con-
taining a formula such as =c1+c2.
You may argue that both alternatives make the code more complex. To some
extent, this argument is correct; you shouldnt thoughtlessly use either api for every
method. But apis keep code simpler (and use higher-level constructs) than explicit
thread manipulation does. Also, careful use of these apis for method calls that (a)
cause long-running computations (perhaps longer than several milliseconds) or (b)
wait for a network or for input from a human can significantly improve the efficiency
of your application. In case (a), these techniques make your program faster without
the explicit ubiquitous use of threads polluting your program. In case (b), theres the
additional benefit that the underlying system can use threads effectively without clog-
ging up. We turn to the latter point in the next section.

15.2.3 sleeping (and other blocking operations) considered harmful
when youre interacting with a human or an application that needs to restrict the rate
at which things happen, one natural way to program is to use the sleep() method. A
sleeping thread still occupies system resources, however. This situation doesnt matter
if you have only a few threads, but it matters if you have many threads, most of which
are sleeping. (see the discussion in section 15.2.1 and figure 15.3. )
the lesson to remember is that tasks sleeping in a thread pool consume resources
by blocking other tasks from starting to run. (they cant stop tasks already allocated to
a thread, as the operating system schedules these tasks. )
its not only sleeping that can clog the available threads in a thread pool, of course.
Any blocking operation can do the same thing. Blocking operations fall into two
classes: waiting for another task to do something, such as invoking get() on a future;
and waiting for external interactions such as reads from networks, database servers, or
human interface devices such as keyboards.
Synchronous and asynchronous apis 371

what can you do? One rather totalitarian answer is never to block within a task or
at least to do so with a small number of exceptions in your code. (see section 15.2.4
for a reality check. ) the better alternative is to break your task into two partsbefore
and afterand ask java to schedule the after part only when it wont block.
Compare code a, shown as a single task

work1();
thread. Sleep(10000); sleep for 10 seconds.
Work2();

with code b:
public class scheduledexecutorserviceexample {
public static void main(string[] args) {
scheduledexecutorservice scheduledexecutorservice
= executors. Newscheduledthreadpool(1);

work1();
scheduledexecutorservice. Schedule(
scheduledexecutorserviceexample: : work2, 10, timeunit. Seconds);

scheduledexecutorservice. Shutdown();
schedule a separate task
}
for work2() 10 seconds
after work1() finishes.
Public static void work1(){
system. Out. Println("hello from work1! ");
}

public static void work2(){
system. Out. Println("hello from work2! ");
}
}

think of both tasks being executed within a thread pool.
Consider how code a executes. First, its queued to execute in the thread pool,
and later, it starts executing. Halfway through, however, it blocks in the call to sleep,
occupying a worker thread for 10 whole seconds doing nothing. Then it executes
work2() before terminating and releasing the worker thread. Code b, by comparison,
executes work1() and then terminatesbut only after having queued a task to do
work2() 10 seconds later.
Code b is better, but why? Code a and code b do the same thing. The difference is
that code a occupies a precious thread while it sleeps, whereas code b queues another
task to execute (with a few bytes of memory and no requirement for a thread) instead
of sleeping.
This effect is something that you should always bear in mind when creating tasks.
Tasks occupy valuable resources when they start executing, so you should aim to keep
them running until they complete and release their resources. Instead of blocking,
a task should terminate after submitting a follow-up task to complete the work it
intended to do.
372 chapter 15 concepts behind completablefuture and reactive programming

whenever possible, this guideline applies to i/o, too. Instead of doing a classical
blocking read, a task should issue a nonblocking start a read method call and terminate
after asking the runtime library to schedule a follow-up task when the read is complete.
This design pattern may seem to lead to lots of hard-to-read code. But the java
completablefuture interface (section 15.4 and chapter 16) abstracts this style of code
within the runtime library, using combinators instead of explicit uses of blocking
get() operations on futures, as we discussed earlier.
As a final remark, well note that code a and code b would be equally effective if
threads were unlimited and cheap. But they arent, so code b is the way to go when-
ever you have more than a few tasks that might sleep or otherwise block.

15.2.4 reality check
if youre designing a new system, designing it with many small, concurrent tasks so
that all possible blocking operations are implemented with asynchronous calls is
probably the way to go if you want to exploit parallel hardware. But reality needs to
intrude into this everything asynchronous design principle. (remember, the best
is the enemy of the good. ) java has had nonblocking io primitives (java. Nio)
since java 1.4 in 2002, and theyre relatively complicated and not well known. Prag-
matically, we suggest that you try to identify situations that would benefit from javas
enhanced concurrency apis, and use them without worrying about making every
api asynchronous.
You may also find it useful to look at newer libraries such as netty (https: //netty. Io/),
which provides a uniform blocking/nonblocking api for network servers.

15.2.5 how do exceptions work with asynchronous apis?
In both future-based and reactive-style asynchronous apis, the conceptual body of the
called method executes in a separate thread, and the callers execution is likely to have
exited the scope of any exception handler placed around the call. Its clear that unusual
behavior that would have triggered an exception needs to perform an alternative
action. But what might this action be? In the completablefuture implementation of
futures, the api includes provision for exposing exceptions at the time of the get()
method and also provides methods such as exceptionally() to recover from excep-
tions, which we discuss in chapter 16.
For reactive-style asynchronous apis, you have to modify the interface by introduc-
ing an additional callback, which is called instead of an exception being raised, as the
existing callback is called instead of a return being executed. To do this, include mul-
tiple callbacks in the reactive api, as in this example:
void f(int x, consumer<integer> dealwithresult,
consumer<throwable> dealwithexception);

then the body of f might perform

dealwithexception(e);
the box-and-channel model 373

if there are multiple callbacks, instead of supplying them separately, you can equiva-
lently wrap them as methods in a single object. The java 9 flow api, for example,
wraps these multiple callbacks within a single object (of class subscriber<t> contain-
ing four methods interpreted as callbacks). Here are three of them:

void oncomplete()
void onerror(throwable throwable)
void onnext(t item)

separate callbacks indicate when a value is available (onnext), when an exception
arose while trying to make a value available (onerror), and when an oncomplete call-
back enables the program to indicate that no further values (or exceptions) will be
produced. For the preceding example, the api for f would now be

void f(int x, subscriber<integer> s);

and the body of f would now indicate an exception, represented as throwable t, by
performing

s. Onerror(t);

compare this api containing multiple callbacks with reading numbers from a file or
keyboard device. If you think of such a device as being a producer rather than a passive
data structure, it produces a sequence of heres a number or heres a malformed
item instead of a number items, and finally a there are no more characters left
(end-of-file) notification.
Its common to refer to these calls as messages, or events. You might say, for example,
that the file reader produced the number events 3, 7, and 42, followed by a malformed-
number event, followed by the number event 2 and then by the end-of-file event.
When seeing these events as part of an api, its important to note that the api signi-
fies nothing about the relative ordering of these events (often called the channel protocol).
In practice, the accompanying documentation specifies the protocol by using phases
such as after an oncomplete event, no more events will be produced.

15.3 the box-and-channel model
often, the best way to design and think about concurrent systems is pictorially. We call
this technique the box-and-channel model. Consider a simple situation involving inte-
gers, generalizing the earlier example of calculating f(x) + g(x). Now you want to call
method or function p with argument x, pass its result to functions q1 and q2, call
method or function r with the results of these two calls, and then print the result. (to
avoid clutter in this explanation, were not going to distinguish between a method m of
class c and its associated function c: : m. ) pictorially, this task is simple, as shown in fig-
ure 15.7.
374 chapter 15 concepts behind completablefuture and reactive programming

q1

x p r

q2
figure 15.7 a simple box-and-
channel diagram

look at two ways of coding figure 15.7 in java to see the problems they cause. The first
way is
int t = p(x);
system. Out. Println( r(q1(t), q2(t)) );

this code appears to be clear, but java runs the calls to q1 and q2 in turn, which is
what you want to avoid when trying to exploit hardware parallelism.
Another way is to use futures to evaluate f and g in parallel:
int t = p(x);
future<integer> a1 = executorservice. Submit(() -> q1(t));
future<integer> a2 = executorservice. Submit(() -> q2(t));
system. Out. Println( r(a1. Get(), a2. Get()));

note: we didnt wrap p and r in futures in this example because of the shape of the
box-and-channel diagram. P has to be done before everything else and r after every-
thing else. This would no longer be the case if we changed the example to mimic

system. Out. Println( r(q1(t), q2(t)) + s(x) );

in which wed need to wrap all five functions (p, q1, q2, r, and s) in futures to maxi-
mize concurrency.
This solution works well if the total amount of concurrency in the system is small. But
what if the system becomes large, with many separate box-and-channel diagrams, and
with some of the boxes themselves internally using their own boxes and channels? In this
situation, many tasks might be waiting (with a call to get()) for a future to complete,
and as discussed in section 15.1.2, the result may be underexploitation of hardware paral-
lelism or even deadlock. Moreover, it tends to be hard to understand such large-scale sys-
tem structure well enough to work out how many tasks are liable to be waiting for a
get(). The solution that java 8 adopts (completablefuture; see section 15.4 for details)
is to use combinators. Youve already seen that you can use methods such as compose() and
andthen() on two functions to get another function (see chapter 3). Assuming that
add1 adds 1 to an integer and that dble doubles an integer, for example, you can write

function<integer, integer> myfun = add1. Andthen(dble);
completablefuture and combinators for concurrency 375

to create a function that doubles its argument and adds 2 to the result. But box-and-
channel diagrams can be also coded directly and nicely with combinators. Figure 15.7
could be captured succinctly with java functions p, q1, q2 and bifunction r as

p. Thenboth(q1, q2). Thencombine(r)

unfortunately, neither thenboth nor thencombine is part of the java function and
bifunction classes in exactly this form.
In the next section, you see how similar ideas of combinators work for completable-
future and prevent tasks from ever to have to wait using get().
Before leaving this section, we want to emphasize the fact that the box-and-chan-
nel model can be used to structure thoughts and code. In an important sense, it
raises the level of abstraction for constructing a larger system. You draw boxes (or use
combinators in programs) to express the computation you want, which is later exe-
cuted, perhaps more efficiently than you might have obtained by hand-coding the
computation. This use of combinators works not only for mathematical functions,
but also for futures and reactive streams of data. In section 15.5, we generalize these
box-and-channel diagrams into marble diagrams in which multiple marbles (repre-
senting messages) are shown on every channel. The box-and-channel model also
helps you change perspective from directly programming concurrency to allowing
combinators to do the work internally. Similarly, java 8 streams change perspective
from the coder having to iterate over a data structure to combinators on streams
doing the work internally.

15.4 completablefuture and combinators for concurrency
one problem with the future interface is that its an interface, encouraging you to
think of and structure your concurrent coding tasks as futures. Historically, however,
futures have provided few actions beyond futuretask implementations: creating a
future with a given computation, running it, waiting for it to terminate, and so on.
Later versions of java provided more structured support (such as recursivetask, dis-
cussed in chapter 7).
What java 8 brings to the party is the ability to compose futures, using the
completablefuture implementation of the future interface. So why call it
completablefuture rather than, say, composablefuture? Well, an ordinary future is
typically created with a callable, which is run, and the result is obtained with a get().
But a completablefuture allows you to create a future without giving it any code to
run, and a complete() method allows some other thread to complete it later with a
value (hence the name) so that get() can access that value. To sum f(x) and g(x)
concurrently, you can write
public class cfcomplete {

public static void main(string[] args)
throws executionexception, interruptedexception {
376 chapter 15 concepts behind completablefuture and reactive programming

executorservice executorservice = executors. Newfixedthreadpool(10);
int x = 1337;

completablefuture<integer> a = new completablefuture<>();
executorservice. Submit(() -> a. Complete(f(x)));
int b = g(x);
system. Out. Println(a. Get() + b);

executorservice. Shutdown();
}
}

or you can write
public class cfcomplete {

public static void main(string[] args)
throws executionexception, interruptedexception {
executorservice executorservice = executors. Newfixedthreadpool(10);
int x = 1337;

completablefuture<integer> a = new completablefuture<>();
executorservice. Submit(() -> b. Complete(g(x)));
int a = f(x);
system. Out. Println(a + b. Get());

executorservice. Shutdown();
}
}

note that both these code versions can waste processing resources (recall section 15.2.3)
by having a thread blocked waiting for a getthe former if f(x) takes longer, and the
latter if g(x) takes longer. Using java 8s completablefuture enables you to avoid this
situation; but first a quiz.

Quiz 15.1:
before reading further, think how you might write tasks to exploit threads perfectly in
this case: two active threads while both f(x) and g(x) are executing, and one thread
starting from when the first one completes up to the return statement.
The answer is that youd use one task to execute f(x), a second task to execute
g(x), and a third task (a new one or one of the existing ones) to calculate the sum,
and somehow, the third task cant start before the first two finish. How do you solve
this problem in java?
The solution is to use the idea of composition on futures.

First, refresh your memory about composing operations, which youve seen twice
before in this book. Composing operations is a powerful program-structuring idea
used in many other languages, but it took off in java only with the addition of lambdas
completablefuture and combinators for concurrency 377

in java 8. One instance of this idea of composition is composing operations on
streams, as in this example:

mystream. Map(... ). Filter(... ). Sum()

another instance of this idea is using methods such as compose() and andthen() on
two functions to get another function (see section 15.5).
This gives you a new and better way to add the results of your two computations by
using the thencombine method from completablefuture<t>. Dont worry too much
about the details at the moment; we discuss this topic more comprehensively in chap-
ter 16. The method thencombine has the following signature (slightly simplified to
prevent the clutter associated with generics and wildcards):

completablefuture<v> thencombine(completablefuture<u> other,
bifunction<t, u, v> fn)

the method takes two completablefuture values (with result types t and u) and cre-
ates a new one (with result type v). When the first two complete, it takes both their
results, applies fn to both results, and completes the resulting future without block-
ing. The preceding code could now be rewritten in the following form:

public class cfcombine {

public static void main(string[] args) throws executionexception,
interruptedexception {

executorservice executorservice = executors. Newfixedthreadpool(10);
int x = 1337;

completablefuture<integer> a = new completablefuture<>();
completablefuture<integer> b = new completablefuture<>();
completablefuture<integer> c = a. Thencombine(b, (y, z)-> y + z);
executorservice. Submit(() -> a. Complete(f(x)));
executorservice. Submit(() -> b. Complete(g(x)));

system. Out. Println(c. Get());
executorservice. Shutdown();

}
}

the thencombine line is critical: without knowing anything about computations in the
futures a and b, it creates a computation thats scheduled to run in the thread pool
only when both of the first two computations have completed. The third computation,
c, adds their results and (most important) isnt considered to be eligible to execute on
a thread until the other two computations have completed (rather than starting to
execute early and then blocking). Therefore, no actual wait operation is performed,
378 chapter 15 concepts behind completablefuture and reactive programming

which was troublesome in the earlier two versions of this code. In those versions, if the
computation in the future happens to finish second, two threads in the thread pool
are still active, even though you need only one! Figure 15.8 shows this situation dia-
grammatically. In both earlier versions, calculating y+z happens on the same fixed
thread that calculates f(x) or g(x)with a potential wait in between. By contrast,
using thencombine schedules the summing computation only after both f(x) and
g(x) have completed.

Y
f(x)

z
g(x)

y+z
figure 15.8 timing diagram showing
three computations: f(x), g(x) and
time adding their results

to be clear, for many pieces of code, you dont need to worry about a few threads
being blocked waiting for a get(), so pre-java 8 futures remain sensible programming
options. In some situations, however, you want to have a large number of futures (such
as for dealing with multiple queries to services). In these cases, using completable-
future and its combinators to avoid blocking calls to get() and possible loss of paral-
lelism or deadlock is often the best solution.

15.5 publish-subscribe and reactive programming
the mental model for a future and completablefuture is that of a computation that
executes independently and concurrently. The result of the future is available with
get() after the computation completes. Thus, futures are one-shot, executing code
that runs to completion only once.
By contrast, the mental model for reactive programming is a future-like object that,
over time, yields multiple results. Consider two examples, starting with a thermometer
object. You expect this object to yield a result repeatedly, giving you a temperature value
every few seconds. Another example is an object representing the listener component of
a web server; this object waits until an http request appears over the network and sim-
ilarly yields with the data from the request. Then other code can process the result: a
temperature or data from an http request. Then the thermometer and listener objects
go back to sensing temperatures or listening before potentially yielding further results.
Note two points here. The core point is that these examples are like futures but
differ in that they can complete (or yield) multiple times instead of being one-shot.
Another point is that in the second example, earlier results may be as important as
ones seen later, whereas for a thermometer, most users are interested only in the
publish-subscribe and reactive programming 379

most-recent temperature. But why is this type of a programming called reactive? The
answer is that another part of the program may want to react to a low temperature
report (such as by turning on a heater).
You may think that the preceding idea is only a stream. If your program fits naturally
into the stream model, a stream may be the best implementation. In general, though,
the reactive-programming paradigm is more expressive. A given java stream can be con-
sumed by only one terminal operation. As we mention in section 15.3, the stream para-
digm makes it hard to express stream-like operations that can split a sequence of values
between two processing pipelines (think fork) or process and combine items from two
separate streams (think join). Streams have linear processing pipelines.
Java 9 models reactive programming with interfaces available inside java. Util
. Concurrent. Flow and encodes whats known as the publish-subscribe model (or pro-
tocol, often shortened to pub-sub). You learn about the java 9 flow api in more detail
in chapter 17, but we provide a short overview here. There are three main concepts:
 a publisher to which a subscriber can subscribe.
 The connection is known as a subscription.
 Messages (also known an events) are transmitted via the connection.

Figure 15.9 shows the idea pictorially, with subscriptions as channels and publishers and
subscribers as ports on boxes. Multiple components can subscribe to a single publisher,
a component can publish multiple separate streams, and a component can subscribe to
multiple publishers. In this next section, we show you how this idea works step by step,
using the nomenclature of the java 9 flow interface.

Subscriptions

component
(an object)

component

component

subscribers

publishers
figure 15.9 the publish-subscribe model
380 chapter 15 concepts behind completablefuture and reactive programming

15.5.1 example use for summing two flows
a simple but characteristic example of publish-subscribe combines events from two
sources of information and publishes them for others to see. This process may sound
obscure at first, but its what a cell containing a formula in a spreadsheet does concep-
tually. Model a spreadsheet cell c3, which contains the formula "=c1+c2". Whenever
cell c1 or c2 is updated (by a human or because the cell contains a further formula),
c3 is updated to reflect the change. The following code assumes that the only opera-
tion available is adding the values of cells.
First, model the concept of a cell that holds a value:
private class simplecell {
private int value = 0;
private string name;

public simplecell(string name) {
this. Name = name;
}
}

at the moment, the code is simple, and you can initialize a few cells, as follows:
simplecell c2 = new simplecell("c2");
simplecell c1 = new simplecell("c1");

how do you specify that when the value of c1 or c2 changes, c3 sums the two values?
You need a way for c1 and c2 to subscribe c3 to their events. To do so, introduce the
interface publisher<t>, which at its core looks like this:
interface publisher<t> {
void subscribe(subscriber<? Super t> subscriber);
}

this interface takes a subscriber as an argument that it can communicate with. The
subscriber<t> interface includes a simple method, onnext, that takes that informa-
tion as an argument and then is free to provide a specific implementation:

interface subscriber<t> {
void onnext(t t);
}

how do you bring these two concepts together? You may realize that a cell is in fact
both a publisher (can subscribe cells to its events) and a subscriber (reacts to events
from other cells). The implementation of the cell class now looks like this:
private class simplecell implements publisher<integer>, subscriber<integer> {
private int value = 0;
private string name;
private list<subscriber> subscribers = new arraylist<>();
publish-subscribe and reactive programming 381

public simplecell(string name) {
this. Name = name;
}

@override
public void subscribe(subscriber<? Super integer> subscriber) {
reacts to a subscribers. Add(subscriber);
new value } this method notifies
from a cell it is all the subscribers
subscribed to with a new value.
By updating private void notifyallsubscribers() {
its value subscribers. Foreach(subscriber -> subscriber. Onnext(this. Value));
}

notifies all @override
subscribers prints the value in
public void onnext(integer newvalue) {
about the the console but
this. Value = newvalue;
updated could be rendering
system. Out. Println(this. Name + ": " + this. Value); the updated cell as
value notifyallsubscribers(); part of an ui
}
}

try a simple example:
simplecell c3 = new simplecell("c3");
simplecell c2 = new simplecell("c2");
simplecell c1 = new simplecell("c1");

c1. Subscribe(c3);

c1. Onnext(10); // update value of c1 to 10
c2. Onnext(20); // update value of c2 to 20

this code outputs the following result because c3 is directly subscribed to c1:
c1:10
c3:10
c2:20

how do you implement the behavior of "c3=c1+c2" ? You need to introduce a sepa-
rate class thats capable of storing two sides of an arithmetic operation (left and right):
public class arithmeticcell extends simplecell {

private int left;
private int right;

public arithmeticcell(string name) {
super(name);
}

public void setleft(int left) { update the cell value
this. Left = left; and notify any
onnext(left + this. Right); subscribers.
}
382 chapter 15 concepts behind completablefuture and reactive programming

public void setright(int right) { update the cell value
this. Right = right; and notify any
onnext(right + this. Left); subscribers.
}
}

now you can try a more-realistic example:
arithmeticcell c3 = new arithmeticcell("c3");
simplecell c2 = new simplecell("c2");
simplecell c1 = new simplecell("c1");

c1. Subscribe(c3: : setleft);
c2. Subscribe(c3: : setright);

c1. Onnext(10); // update value of c1 to 10
c2. Onnext(20); // update value of c2 to 20
c1. Onnext(15); // update value of c1 to 15

the output is
c1:10
c3:10
c2:20
c3:30
c1:15
c3:35

by inspecting the output, you see that when c1 was updated to 15, c3 immediately
reacted and updated its value as well. Whats neat about the publisher-subscriber interac-
tion is the fact that you can set up a graph of publishers and subscribers. You could create
another cell c5 that depends on c3 and c4 by expressing "c5=c3+c4", for example:
arithmeticcell c5 = new arithmeticcell("c5");
arithmeticcell c3 = new arithmeticcell("c3");
simplecell c4 = new simplecell("c4");
simplecell c2 = new simplecell("c2");
simplecell c1 = new simplecell("c1");

c1. Subscribe(c3: : setleft);
c2. Subscribe(c3: : setright);

c3. Subscribe(c5: : setleft);
c4. Subscribe(c5: : setright);

then you can perform various updates in your spreadsheet:
c1. Onnext(10); // update value of c1 to 10
c2. Onnext(20); // update value of c2 to 20
c1. Onnext(15); // update value of c1 to 15
c4. Onnext(1); // update value of c4 to 1
c4. Onnext(3); // update value of c4 to 3
publish-subscribe and reactive programming 383

these actions result in the following output:
c1:10
c3:10
c5:10
c2:20
c3:30
c5:30
c1:15
c3:35
c5:35
c4:1
c5:36
c4:3
c5:38

in the end, the value of c5 is 38 because c1 is 15, c2 is 20, and c4 is 3.

Nomenclature
because data flows from publisher (producer) to subscriber (consumer), developers
often use words such as upstream and downstream. In the preceding code examples,
the data newvalue received by the upstream onnext() methods is passed via the
call to notifyallsubscribers() to the downstream onnext() call.

Thats the core idea of publish-subscribe. Weve left out a few things, however, some of
which are straightforward embellishments, and one of which (backpressure) is so vital
that we discuss it separately in the next section.
First, well discuss the straightforward things. As we remark in section 15.2, practi-
cal programming of flows may want to signal things other than an onnext event, so
subscribers (listeners) need to define onerror and oncomplete methods so that the
publisher can indicate exceptions and terminations of data flow. (perhaps the exam-
ple of a thermometer has been replaced and will never produce more values via
onnext. ) the methods onerror and oncomplete are supported in the actual sub-
scriber interface in the java 9 flow api. These methods are among the reasons why
this protocol is more powerful than the traditional observer pattern.
Two simple but vital ideas that significantly complicate the flow interfaces are pres-
sure and backpressure. These ideas can appear to be unimportant, but theyre vital
for thread utilization. Suppose that your thermometer, which previously reported a
temperature every few seconds, was upgraded to a better one that reports a tempera-
ture every millisecond. Could your program react to these events sufficiently quickly,
or might some buffer overflow and cause a crash? (recall the problems giving thread
pools large numbers of tasks if more than a few tasks might block. ) similarly, suppose
that you subscribe to a publisher that furnishes all the sms messages onto your
phone. The subscription might work well on my newish phone with only a few sms
messages, but what happens in a few years when there are thousands of messages, all
384 chapter 15 concepts behind completablefuture and reactive programming

potentially sent via calls to onnext in less than a second? This situation is often known
as pressure.
Now think of a vertical pipe containing messages written on balls. You also need a
form of backpressure, such as a mechanism that restricts the number of balls being
added to the column. Backpressure is implemented in the java 9 flow api by a
request() method (in a new interface called subscription) that invites the pub-
lisher to send the next item(s), instead of items being sent at an unlimited rate (the
pull model instead of the push model). We turn this topic in the next section.

15.5.2 backpressure
youve seen how to pass a subscriber object (containing onnext, onerror, and
oncomplete methods) to a publisher, which the publisher calls when appropriate.
This object passes information from publisher to subscriber. You want to limit the
rate at which this information is sent via backpressure (flow control), which requires
you to send information from subscriber to publisher. The problem is that the
publisher may have multiple subscribers, and you want backpressure to affect only
the point-to-point connection involved. In the java 9 flow api, the subscriber inter-
face includes a fourth method

void onsubscribe(subscription subscription);

thats called as the first event sent on the channel established between publisher and
subscriber. The subscription object contains methods that enable the subscriber
to communicate with the publisher, as follows:
interface subscription {
void cancel();
void request(long n);
}

note the usual this seems backward effect with callbacks. The publisher creates the
subscription object and passes it to the subscriber, which can call its methods to
pass information from the subscriber back to the publisher.

15.5.3 a simple form of real backpressure
to enable a publish-subscribe connection to deal with events one at a time, you need
to make the following changes:
 arrange for the subscriber to store the subscription object passed by
onsubscribe locally, perhaps as a field subscription.
 Make the last action of onsubscribe, onnext, and (perhaps) onerror be a call
to channel. Request(1) to request the next event (only one event, which stops
the subscriber from being overwhelmed).
 Change the publisher so that notifyallsubscribers (in this example) sends
an onnext or onerror event along only the channels that made a request.
Reactive systems vs. Reactive programming 385

(typically, the publisher creates a new subscription object to associate
with each subscriber so that multiple subscribers can each process data at
their own rate. )

although this process seems to be simple, implementing backpressure requires think-
ing about a range of implementation trade-offs:
 do you send events to multiple subscribers at the speed of the slowest, or do
you have a separate queue of as-yet-unsent data for each subscriber?
 What happens when these queues grow excessively?
 Do you drop events if the subscriber isnt ready for them?

The choice depends on the semantics of the data being sent. Losing one temperature
report from a sequence may not matter, but losing a credit in your bank account cer-
tainly does!
You often hear this concept referred to as reactive pull-based backpressure. The
concept is called reactive pull-based because it provides a way for the subscriber to
pull (request) more information from the publisher via events (reactive). The result
is a backpressure mechanism.

15.6 reactive systems vs. Reactive programming
increasingly in the programming and academic communities, you may hear about
reactive systems and reactive programming, and its important to realize that these
terms express quite different ideas.
A reactive system is a program whose architecture allows it to react to changes in its
runtime environments. Properties that reactive systems should have are formalized in
the reactive manifesto (http: //www. Reactivemanifesto. Org) (see chapter 17). Three of
these properties can be summarized as responsive, resilient, and elastic.
Responsive means that a reactive system can respond to inputs in real time rather
delaying a simple query because the system is processing a big job for someone else.
Resilient means that a system generally doesnt fail because one component fails; a bro-
ken network link shouldnt affect queries that dont involve that link, and queries to
an unresponsive component can be rerouted to an alternative component. Elastic
means that a system can adjust to changes in its workload and continue to execute effi-
ciently. As you might dynamically reallocate staff in a bar between serving food and
serving drinks so that wait times in both lines are similar, you might adjust the number
of worker threads associated with various software services so that no worker is idle
while ensuring that each queue continues to be processed.
Clearly, you can achieve these properties in many ways, but one main approach is
to use reactive programming style, provided in java by interfaces associated with java
. Util. Concurrent. Flow. The design of these interfaces reflects the fourth and final
property of the reactive manifesto: being message-driven. Message-driven systems have
internal apis based on the box-and-channel model, with components waiting for
386 chapter 15 concepts behind completablefuture and reactive programming

inputs that are processed, with the results sent as messages to other components to
enable the system to be responsive.

15.7 road map
chapter 16 explores the completablefuture api with a real java example, and chap-
ter 17 explores the java 9 flow (publish-subscribe) api.

Summary
 support for concurrency in java has evolved and continues to evolve. Thread
pools are generally helpful but can cause problems when you have many tasks
that can block.
 Making methods asynchronous (returning before all their work is done) allows
additional parallelism, complementary to that used to optimize loops.
 You can use the box-and-channel model to visualize asynchronous systems.
 The java 8 completablefuture class and the java 9 flow api can both represent
box-and-channel diagrams.
 The completablefuture class expresses one-shot asynchronous computations.
Combinators can be used to compose asynchronous computations without the
risk of blocking thats inherent in traditional uses of futures.
 The flow api is based on the publish-subscribe protocol, including backpres-
sure, and forms the basis for reactive programming in java.
 Reactive programming can be used to implement a reactive system.
Completablefuture:
composable asynchronous
programming

this chapter covers
 creating an asynchronous computation and
retrieving its result
 increasing throughput by using nonblocking
operations
 designing and implementing an asynchronous api
 consuming asynchronously a synchronous api
 pipelining and merging two or more asynchronous
operations
 reacting to the completion of an asynchronous
operation

chapter 15 explored the modern concurrency context: that multiple processing
resources (cpu cores and the like) are available, and you want your programs to
exploit as many of these resources as possible in a high-level manner (rather than
litter your programs with ill-structured, unmaintainable operations on threads).
We noted that parallel streams and fork/join parallelism provide higher-level
constructs for expressing parallelism in programs iterating over collections and
in programs involving divide-and-conquer, but that method invocations provide

387
388 chapter 16 completablefuture: composable asynchronous programming

additional opportunities for executing code in parallel. Java 8 and 9 introduce two
specific apis for this purpose: completablefuture and the reactive-programming
paradigm. This chapter explains, through practical code examples, how the java 8
completablefuture implementation of the future interface gives you additional weap-
ons in your programming armory. It also discusses additions introduced in java 9.

16.1 simple use of futures
the future interface was introduced in java 5 to model a result made available at
some point in the future. A query to a remote service wont be available immediately
when the caller makes the request, for example. The future interface models an asyn-
chronous computation and provides a reference to its result that becomes available
when the computation itself is completed. Triggering a potentially time-consuming
action inside a future allows the caller thread to continue doing useful work instead of
waiting for the operations result. You can think of this process as being like taking a bag
of clothes to your favorite dry cleaner. The cleaner gives you a receipt to tell you when
your clothes will be cleaned (a future); in the meantime, you can do some other activi-
ties. Another advantage of future is that its friendlier to work with than lower-level
threads. To work with a future, you typically have to wrap the time-consuming opera-
tion inside a callable object and submit it to an executorservice. The following list-
ing shows an example written before java 8.

Listing 16.1 executing a long-lasting operation asynchronously in a future

submit a callable to create an executorservice allowing you
the executorservice. To submit tasks to a thread pool.
Executorservice executor = executors. Newcachedthreadpool();
future<double> future = executor. Submit(new callable<double>() {
public double call() {
return dosomelongcomputation(); do something else while
}}); the asynchronous
dosomethingelse(); operation is progressing.
Try {
double result = future. Get(1, timeunit. Seconds);
retrieve the result
} catch (executionexception ee) { of the asynchronous
// the computation threw an exception operation, blocking
} catch (interruptedexception ie) { if it isnt available
// the current thread was interrupted while waiting yet but waiting for
} catch (timeoutexception te) { 1 second at most
// the timeout expired before the future completion before timing out.
}

execute a long operation
asynchronously in a
separate thread.

As depicted in figure 16.1, this style of programming allows your thread to perform
some other tasks while the long-lasting operation is executed concurrently in a sepa-
rate thread provided by the executorservice. Then, when you cant do any other
simple use of futures 389

meaningful work without having the result of that asynchronous operation, you can
retrieve it from the future by invoking its get method. This method immediately
returns the result of the operation if its already completed or blocks your thread,
waiting for its result to be available.

Your executor
thread thread

idle
submit

dosomethingelse dosomelongcomputation
get

blocked
result

idle

figure 16.1 using a future to execute a long operation asynchronously

note the problem with this scenario. What if the long operation never returns? To
handle this possibility, its almost always a good idea to use the two-argument version
of get, which takes a timeout specifying the maximum time (along with its time unit)
that your thread is willing to wait for the futures result (as in listing 16.1). The zero-
argument version of get would instead wait indefinitely.

16.1.1 understanding futures and their limitations
this first small example shows that the future interface provides methods for check-
ing whether the asynchronous computation is complete (by using the isdone
method), waiting for its completion, and retrieving its result. But these features arent
enough to let you write concise concurrent code. Its difficult, for example, to express
dependencies among results of a future. Declaratively, its easy to specify, when the
result of the long computation is available, please send its result to another long com-
putation, and when thats done, combine its result with the result from another
query. Implementing this specification with the operations available in a future is a
different story, which is why it would be useful to have more declarative features in the
implementation, such as these:
 combining two asynchronous computations both when theyre independent
and when the second depends on the result of the first
 waiting for the completion of all tasks performed by a set of futures
 waiting for the completion of only the quickest task in a set of futures (possibly
because the futures are trying to calculate the same value in different ways)
and retrieving its result
390 chapter 16 completablefuture: composable asynchronous programming

 programmatically completing a future (that is, by providing the result of the
asynchronous operation manually)
 reacting to a future completion (that is, being notified when the completion
happens and then being able to perform a further action with the result of the
future instead of being blocked while waiting for its result)
in the rest of this chapter, you learn how the completablefuture class (which imple-
ments the future interface) makes all these things possible in a declarative way by means
of java 8s new features. The designs of stream and completablefuture follow similar
patterns, because both use lambda expressions and pipelining. For this reason, you could
say that completablefuture is to a plain future what stream is to a collection.

16.1.2 using completablefutures to build an asynchronous application
to explore the completablefuture features, in this section you incrementally develop
a best-price-finder application that contacts multiple online shops to find the lowest
price for a given product or service. Along the way, you learn several important skills:
 how to provide an asynchronous api for your customers (useful if youre the
owner of one of the online shops).
 How to make your code nonblocking when youre a consumer of a synchronous
api. You discover how to pipeline two subsequent asynchronous operations,
merging them into a single asynchronous computation. This situation arises,
for example, when the online shop returns a discount code along with the orig-
inal price of the item you wanted to buy. You have to contact a second remote
discount service to find out the percentage discount associated with this dis-
count code before calculating the actual price of that item.
 How to reactively process events representing the completion of an asynchro-
nous operation and how doing so allows the best-price-finder application to
constantly update the best-buy quote for the item you want to buy as each shop
returns its price, instead of waiting for all the shops to return their respective
quotes. This skill also averts the scenario in which the user sees a blank screen
forever if one of the shops servers is down.

Synchronous vs. Asynchronous api
the phrase synchronous api is another way of talking about a traditional call to a
method: you call it, the caller waits while the method computes, the method returns,
and the caller continues with the returned value. Even if the caller and callee were
executed on different threads, the caller would still wait for the callee to complete.
This situation gives rise to the phrase blocking call.
By contrast, in an asynchronous api the method returns immediately (or at least
before its computation is complete), delegating its remaining computation to a
thread, which runs asynchronously to the callerhence, the phrase nonblocking call.
The remaining computation gives its value to the caller by calling a callback method,
implementing an asynchronous api 391

or the caller invokes a further wait until the computation is complete method. This
style of computation is common in i/o systems programming: you initiate a disc
access, which happens asynchronously while you do more computation, and when
you have nothing more useful to do, you wait until the disc blocks are loaded into
memory. Note that blocking and nonblocking are often used for specific implementa-
tions of i/o by the operating system. However, these terms tend to be used inter-
changeably with asynchronous and synchronous even in non-i/o contexts.

16.2 implementing an asynchronous api
to start implementing the best-price-finder application, define the api that each shop
should provide. First, a shop declares a method that returns the price of a product,
given its name:
public class shop {
public double getprice(string product) {
// to be implemented
}
}

the internal implementation of this method would query the shops database, but
probably also perform other time-consuming tasks, such as contacting other exter-
nal services (such as the shops suppliers or manufacturer-related promotional dis-
counts). To fake such a long-running method execution, in the rest of this chapter
you use a delay method, which introduces an artificial delay of 1 second, as defined
in the following listing.

Listing 16.2 a method to simulate a 1-second delay

public static void delay() {
try {
thread. Sleep(1000l);
} catch (interruptedexception e) {
throw new runtimeexception(e);
}
}

for the purposes of this chapter, you can model the getprice method by calling
delay and then returning a randomly calculated value for the price, as shown in the
next listing. The code for returning a randomly calculated price may look like a bit of
a hack; it randomizes the price based on the product name by using the result of
charat as a number.

Listing 16.3 introducing a simulated delay in the getprice method

public double getprice(string product) {
return calculateprice(product);
}
392 chapter 16 completablefuture: composable asynchronous programming

private double calculateprice(string product) {
delay();
return random. Nextdouble() * product. Charat(0) + product. Charat(1);
}

this code implies that when the consumer of this api (in this case, the best-price-finder
application) invokes this method, it remains blocked and then is idle for 1 second while
waiting for its synchronous completion. This situation is unacceptable, especially consid-
ering that the best-price-finder application has to repeat this operation for all the shops
in its network. In the subsequent sections of this chapter, you discover how to resolve
this problem by consuming this synchronous api in an asynchronous way. But for the
purposes of learning how to design an asynchronous api, you continue in this section to
pretend to be on the other side of the barricade. Youre a wise shop owner who realizes
how painful this synchronous api is for its users, and you want to rewrite it as an asyn-
chronous api to make your customers lives easier.

16.2.1 converting a synchronous method into an asynchronous one
to achieve this goal, you first have to turn the getprice method into a getpriceasync
method and change its return value, as follows:

public future<double> getpriceasync(string product) { ... }

as we mentioned in the introduction of this chapter, the java. Util. Concurrent. Future
interface was introduced in java 5 to represent the result of an asynchronous computa-
tion. (that is, the caller thread is allowed to proceed without blocking. ) a future is a
handle for a value that isnt available yet but that can be retrieved by invoking its get
method after its computation finally terminates. As a result, the getpriceasync method
can return immediately, giving the caller thread a chance to perform other useful com-
putations in the meantime. The java 8 completablefuture class gives you various possi-
bilities to implement this method easily, as shown in the following listing.

Listing 16.4 implementing the getpriceasync method

execute the computation asynchronously create the completablefuture that will
in a different thread. Contain the result of the computation.
Public future<double> getpriceasync(string product) {
completablefuture<double> futureprice = new completablefuture<>();
new thread( () -> {
double price = calculateprice(product); set the value
futureprice. Complete(price); returned by the long
}). Start(); computation on the
return futureprice; future when it
return the future without waiting becomes available.
}
for the computation of the result it
contains to be completed.

Here, you create an instance of completablefuture, representing an asynchronous
computation and containing a result when it becomes available. Then you fork a
implementing an asynchronous api 393

different thread that will perform the actual price calculation and return the future
instance without waiting for that long-lasting calculation to terminate. When the price
of the requested product is finally available, you can complete the completablefuture,
using its complete method to set the value. This feature also explains the name of
this java 8 implementation of future. A client of this api can invoke it, as shown in
the next listing.

Listing 16.5 using an asynchronous api
query the shop to retrieve
shop shop = new shop("bestshop");
the price of a product.
Long start = system. Nanotime();
future<double> futureprice = shop. Getpriceasync("my favorite product");
long invocationtime = ((system. Nanotime() - start) / 1_000_000);
system. Out. Println("invocation returned after " + invocationtime
+ " msecs");
// do some more tasks, like querying other shops
dosomethingelse();
// while the price of the product is being calculated
try { read the price from the
double price = futureprice. Get(); future or block until it
system. Out. Printf("price is %.2f%n", price); becomes available.
} catch (exception e) {
throw new runtimeexception(e);
}
long retrievaltime = ((system. Nanotime() - start) / 1_000_000);
system. Out. Println("price returned after " + retrievaltime + " msecs");

as you can see, the client asks the shop to get the price of a certain product. Because
the shop provides an asynchronous api, this invocation almost immediately returns
the future, through which the client can retrieve the products price later. Then the
client can perform other tasks, such as querying other shops, instead of remaining
blocked, waiting for the first shop to produce the requested result. Later, when the cli-
ent can do no other meaningful jobs without having the product price, it can invoke
get on the future. By doing so, the client unwraps the value contained in the future
(if the asynchronous task is finished) or remains blocked until that value is available.
The output produced by the code in listing 16.5 could be something like this:
invocation returned after 43 msecs
price is 123.26
price returned after 1045 msecs

you can see that the invocation of the getpriceasync method returns far sooner than
when the price calculation eventually finishes. In section 16.4, you learn that its also
possible for the client to avoid any risk of being blocked. Instead, the client can be
notified when the future is complete and can execute a callback code, defined
through a lambda expression or a method reference, only when the result of the com-
putation is available. For now, well address another problem: how to manage an error
during the execution of the asynchronous task.
394 chapter 16 completablefuture: composable asynchronous programming

16.2.2 dealing with errors
the code youve developed so far works correctly if everything goes smoothly. But
what happens if the price calculation generates an error? Unfortunately, in this case
you get a particularly negative outcome: the exception raised to signal the error
remains confined in the thread, which is trying to calculate the product price, and
ultimately kills the thread. As a consequence, the client remains blocked forever, wait-
ing for the result of the get method to arrive.
The client can prevent this problem by using an overloaded version of the get
method that also accepts a timeout. Its good practice to use a timeout to prevent similar
situations elsewhere in your code. This way, the client at least avoids waiting indefinitely,
but when the timeout expires, its notified with a timeoutexception. As a consequence,
the client wont have a chance to discover what caused that failure inside the thread
that was trying to calculate the product price. To make the client aware of the reason
why the shop wasnt able to provide the price of the requested product, you have to
propagate the exception that caused the problem inside the completablefuture
through its completeexceptionally method. Applying this idea to listing 16.4 pro-
duces the code shown in the following listing.

Listing 16.6 propagating an error inside the completablefuture

public future<double> getpriceasync(string product) {
completablefuture<double> futureprice = new completablefuture<>();
new thread( () -> {
try {
if the price calculation double price = calculateprice(product);
completed normally, futureprice. Complete(price);
complete the future otherwise,
} catch (exception ex) { complete the future
with the price. Futureprice. Completeexceptionally(ex); exceptionally with
} the exception that
}). Start(); caused the failure.
Return futureprice;
}

now the client will be notified with an executionexception (which takes an exception
parameter containing the causethe original exception thrown by the price calcula-
tion method). If that method throws a runtimeexception saying that product isnt
available, for example, the client gets an executionexception like the following:
exception in thread "main" java. Lang. Runtimeexception:
java. Util. Concurrent. Executionexception: java. Lang. Runtimeexception:
product not available
at java89inaction. Chap16. Asyncshopclient. Main(asyncshopclient. Java:16)
caused by: java. Util. Concurrent. Executionexception: java. Lang. Runtimeexception:
product not available
at java. Base/java. Util. Concurrent. Completablefuture. Reportget
(completablefuture. Java:395)
at java. Base/java. Util. Concurrent. Completablefuture. Get
(completablefuture. Java:1999)
implementing an asynchronous api 395

at java89inaction. Chap16. Asyncshopclient. Main(asyncshopclient. Java:14)
caused by: java. Lang. Runtimeexception: product not available
at java89inaction. Chap16. Asyncshop. Calculateprice(asyncshop. Java:38)
at java89inaction. Chap16. Asyncshop. Lambda$0(asyncshop. Java:33)
at java. Base/java. Util. Concurrent. Completablefuture$asyncsupply. Run
(completablefuture. Java:1700)
at java. Base/java. Util. Concurrent. Completablefuture$asyncsupply. Exec
(completablefuture. Java:1692)
at java. Base/java. Util. Concurrent. Forkjointask. Doexec(forkjointask. Java:283)
at java. Base/java. Util. Concurrent. Forkjoinpool. Runworker
(forkjoinpool. Java:1603)
at java. Base/java. Util. Concurrent. Forkjoinworkerthread. Run
(forkjoinworkerthread. Java:175)

creating a completablefuture with the supplyasync factory method
up to now, youve created completablefutures and completed them programmati-
cally when it seemed convenient to do so, but the completablefuture class comes
with lots of handy factory methods that can make this process far easier and less ver-
bose. The supplyasync method, for example, lets you rewrite the getpriceasync
method in listing 16.4 with a single statement, as shown in the next listing.

Listing 16.7 creating a completablefuture with the supplyasync factory method

public future<double> getpriceasync(string product) {
return completablefuture. Supplyasync(() -> calculateprice(product));
}

the supplyasync method accepts a supplier as argument and returns a completable-
future that will be asynchronously completed with the value obtained by invoking
that supplier. This supplier is run by one of the executors in the forkjoinpool, but
you can specify a different executor by passing it as a second argument to the over-
loaded version of this method. More generally, you can pass an executor to all other
completablefuture factory methods. You use this capability in section 16.3.4, where
we demonstrate that using an executor that fits the characteristics of your application
can have a positive effect on its performance.
Also note that the completablefuture returned by the getpriceasync method in
listing 16.7 is equivalent to the one you created and completed manually in listing 16.6,
meaning that it provides the same error management you carefully added.
For the rest of this chapter, well suppose that you have no control of the api imple-
mented by the shop class and that it provides only synchronous blocking methods. This
situation typically happens when you want to consume an http api provided by some
service. You see how its still possible to query multiple shops asynchronously, thus
avoiding becoming blocked on a single request and thereby increasing the perfor-
mance and the throughput of your best-price-finder application.
396 chapter 16 completablefuture: composable asynchronous programming

16.3 making your code nonblocking
youve been asked to develop a best-price-finder application, and all the shops you
have to query provide only the same synchronous api implemented as shown at the
beginning of section 16.2. In other words, you have a list of shops, like this one:
list<shop> shops = list. Of(new shop("bestprice"),
new shop("letssavebig"),
new shop("myfavoriteshop"),
new shop("buyitall"));

you have to implement a method with the following signature, which, given the name
of a product, returns a list of strings. Each string contains the name of a shop and the
price of the requested product in that shop, as follows:

public list<string> findprices(string product);

your first idea probably will be to use the stream features you learned in chapters 4, 5,
and 6. You may be tempted to write something like this next listing. (yes, its good if
youre already thinking that this first solution is bad! )

listing 16.8 a findprices implementation sequentially querying all the shops

public list<string> findprices(string product) {
return shops. Stream()
. Map(shop -> string. Format("%s price is %.2f",
shop. Getname(), shop. Getprice(product)))
. Collect(tolist());
}

this solution is straightforward. Now try to put the method findprices to work with
the only product you want madly these days: the myphone27s. In addition, record
how long the method takes to run, as shown in the following listing. This information
lets you compare the methods performance with that of the improved method you
develop later.

Listing 16.9 checking findprices correctness and performance

long start = system. Nanotime();
system. Out. Println(findprices("myphone27s"));
long duration = (system. Nanotime() - start) / 1_000_000;
system. Out. Println("done in " + duration + " msecs");

the code in listing 16.9 produces output like this:
[bestprice price is 123.26, letssavebig price is 169.47, myfavoriteshop price
is 214.13, buyitall price is 184.74]
done in 4032 msecs

as you may have expected, the time that the findprices method takes to run is a few
milliseconds longer than 4 seconds, because the four shops are queried sequentially
making your code nonblocking 397

and blocking one after the other, and each shop takes 1 second to calculate the price
of the requested product. How can you improve on this result?

16.3.1 parallelizing requests using a parallel stream
after reading chapter 7, the first and quickest improvement that should occur to you
would be to avoid this sequential computation by using a parallel stream instead of a
sequential, as shown in the next listing.

Listing 16.10 parallelizing the findprices method

use a parallel stream to
public list<string> findprices(string product) { retrieve the prices from the
return shops. Parallelstream() different shops in parallel.
. Map(shop -> string. Format("%s price is %.2f",
shop. Getname(), shop. Getprice(product)))
. Collect(tolist());
}

find out whether this new version of findprices is any better by again running the
code in listing 16.9:
[bestprice price is 123.26, letssavebig price is 169.47, myfavoriteshop price
is 214.13, buyitall price is 184.74]
done in 1180 msecs

well done! It looks like this idea is simple but effective. Now the four shops are que-
ried in parallel, so the code takes a bit more than a second to complete.
Can you do even better? Try to turn all the synchronous invocations to the shops in
the findprices method into asynchronous invocations, using what youve learned so
far about completablefutures.

16.3.2 making asynchronous requests with completablefutures
you saw earlier that you can use the factory method supplyasync to create completa-
blefuture objects. Now use it:
list<completablefuture<string>> pricefutures =
shops. Stream()
. Map(shop -> completablefuture. Supplyasync(
() -> string. Format("%s price is %.2f",
shop. Getname(), shop. Getprice(product))))
. Collect(tolist());

with this approach, you obtain a list<completablefuture<string>>, where each
completablefuture in the list contains the string name of a shop when its computa-
tion is complete. But because the findprices method youre trying to reimplement with
completablefutures has to return a list<string>, youll have to wait for the comple-
tion of all these futures and extract the values they contain before returning the list.
398 chapter 16 completablefuture: composable asynchronous programming

to achieve this result, you can apply a second map operation to the original
list<completablefuture<string>>, invoking a join on all the futures in the list
and then waiting for their completion one by one. Note that the join method of the
completablefuture class has the same meaning as the get method also declared in
the future interface, the only difference being that join doesnt throw any checked
exception. By using join, you dont have to bloat the lambda expression passed to this
second map with a try/catch block. Putting everything together, you can rewrite the
findprices method as shown in the listing that follows.

Listing 16.11 implementing the findprices method with completablefutures

public list<string> findprices(string product) { calculate each price
list<completablefuture<string>> pricefutures = asynchronously with
shops. Stream() a completablefuture.
. Map(shop -> completablefuture. Supplyasync(
() -> shop. Getname() + " price is " +
shop. Getprice(product)))
. Collect(collectors. Tolist());
return pricefutures. Stream() wait for the completion
. Map(completablefuture: : join) of all asynchronous
. Collect(tolist()); operations.
}

note that you use two separate stream pipelines instead of putting the two map opera-
tions one after the other in the same stream-processing pipelineand for a good rea-
son. Given the lazy nature of intermediate stream operations, if youd processed the
stream in a single pipeline, youd have succeeded only in executing all the requests to
different shops synchronously and sequentially. The creation of each completable-
future to interrogate a given shop would start only when the computation of the pre-
vious one completed, letting the join method return the result of that computation.
Figure 16.2 clarifies this important detail.
The top half of figure 16.2 shows that processing the stream with a single pipe-
line implies that the evaluation order (identified by the dotted line) is sequential. In
fact, a new completablefuture is created only after the former one is completely
evaluated. Conversely, the bottom half of the figure demonstrates how first gather-
ing the completablefutures in a list (represented by the oval) allows all of them to
start before waiting for their completion.
Running the code in listing 16.11 to check the performance of this third version of
the findprices method, you could obtain output along these lines:
[bestprice price is 123.26, letssavebig price is 169.47, myfavoriteshop price
is 214.13, buyitall price is 184.74]
done in 2005 msecs

this result is quite disappointing, isnt it? With a runtime of more than 2 seconds, this
implementation with completablefutures is faster than the original naive sequential
making your code nonblocking 399

supplyasync(() ->
shop1 future1. Join() price1
shop1. Getprice(product))

supplyasync(() ->
shop2 future2. Join() price2
shop2. Getprice(product))

supplyasync(() ->
shop3 future3. Join() price3
shop3. Getprice(product))

sequential

parallel

supplyasync(() ->
shop1 future1 future1. Join() price1
shop1. Getprice(product))

supplyasync(() ->
shop2 future2 future2. Join() price2
shop2. Getprice(product))

supplyasync(() ->
shop3 future3 future3. Join() price3
shop3. Getprice(product))

figure 16.2 why streams laziness causes a sequential computation and how to avoid it

and blocking implementation from listing 16.8. But its also almost twice as slow as the
previous implementation using a parallel stream. Its even more disappointing consid-
ering the fact that you obtained the parallel stream version with a trivial change to the
sequential version.
The newer version with completablefutures required quite a bit of work. But is
using completablefutures in this scenario a waste of time? Or are you overlooking
something important? Take a few minutes before going forward, particularly recalling
that youre testing the code samples on a machine thats capable of running four
threads in parallel.1

16.3.3 looking for the solution that scales better
the parallel stream version performs so well only because it can run four tasks in paral-
lel, so its able to allocate exactly one thread for each shop. What happens if you decide
to add a fifth shop to the list of shops crawled by your best-price-finder application? Not

1
if youre using a machine thats capable of running more threads in parallel (say, eight), you need more shops
and processes in parallel to reproduce the behavior shown in these pages.
400 chapter 16 completablefuture: composable asynchronous programming

surprisingly, the sequential version requires a bit more than 5 seconds to run, as shown
in the following output:

[bestprice price is 123.26, letssavebig price is 169.47, myfavoriteshop price
is 214.13, buyitall price is 184.74, shopeasy price is 166.08]
done in 5025 msecs
the output of the program
using a sequential stream

unfortunately, the parallel stream version also requires a whole second more than
before, because all four threads that it can run in parallel (available in the common
thread pool) are now busy with the first four shops. The fifth query has to wait for the
completion of one of the former operations to free a thread, as follows:

[bestprice price is 123.26, letssavebig price is 169.47, myfavoriteshop price
is 214.13, buyitall price is 184.74, shopeasy price is 166.08]
done in 2167 msecs
the output of the program
using a parallel stream

what about the completablefuture version? Give it a try with the fifth shop:

[bestprice price is 123.26, letssavebig price is 169.47, myfavoriteshop price
is 214.13, buyitall price is 184.74, shopeasy price is 166.08]
done in 2006 msecs
the output of the program
using completablefutures

the completablefuture version seems to be a bit faster than the one using parallel
stream, but this version isnt satisfying either. If you try to run your code with nine
shops, the parallel stream version takes 3143 milliseconds, whereas the completable-
future cersion requires 3009 milliseconds. The versions look equivalent for a good
reason: both internally use the same common pool that by default has a fixed num-
ber of threads equal to the one returned by runtime. Getruntime(). Available-
processors(). Nevertheless, the completablefutures version has an advantage: by
contrast with the parallel streams api, it allows you to specify a different executor
to submit tasks to. You can configure this executor, and size its thread pool, in a way
that better fits the requirements of your application. In the next section, you trans-
late this better level of configurability into practical performance gain for your
application.

16.3.4 using a custom executor
in this case, a sensible choice seems to be to create an executor with a number of
threads in its pool that takes into account the actual workload you could expect in
your application. How do you size this executor correctly?
Making your code nonblocking 401

sizing thread pools
in the great book java concurrency in practice (addison-wesley, 2006; http: //jcip. Net),
brian goetz and his co-authors give some advice on finding the optimal size for a
thread pool. This advice is important because if the number of threads in the pool is
too big, the threads end up competing for scarce cpu and memory resources, wast-
ing their time performing context switching. Conversely, if this number is too small
(as it likely is in your application), some of the cores of the cpu will remain
underused. Goetz suggests that you can calculate the right pool size to approximate
a desired cpu use rate with the following formula:

nthreads = ncpu * ucpu * (1 + w/c)

in this formula, ncpu is the number of cores, available through runtime. Get-
runtime(). Availableprocessors()

 ucpu is the target cpu use (between 0 and 1).
 W/c is the ratio of wait time to compute time.

The application is spending about 99 percent of its time waiting for the shops
responses, so you could estimate a w/c ratio of 100. If your target is 100 percent cpu
use, you should have a pool with 400 threads. In practice, its wasteful to have more
threads than shops, because youll have threads in your pool that are never used. For
this reason, you need to set up an executor with a fixed number of threads equal to
the number of shops you have to query, so that you have one thread for each shop.
Also set an upper limit of 100 threads to avoid a server crash for a larger number of
shops, as shown in the following listing.

Listing 16.12 a custom executor fitting the best-price-finder application

private final executor executor =
executors. Newfixedthreadpool(math. Min(shops. Size(), 100),
use daemon threads, (runnable r) -> {
which dont prevent thread t = new thread(r);
the termination of t. Setdaemon(true); create a thread pool with a number
the program. Return t; of threads equal to the minimum of
} 100 and the number of shops.
);

note that youre creating a pool made of daemon threads. A java program cant termi-
nate or exit while a normal thread is executing, so a leftover thread waiting for a
never-satisfiable event causes problems. By contrast, marking a thread as a daemon
means that it can be killed on program termination. Theres no performance differ-
ence. Now you can pass the new executor as the second argument of the supplyasync
402 chapter 16 completablefuture: composable asynchronous programming

factory method. In addition, now create the completablefuture that retrieves the
price of the requested product from a given shop as follows:
completablefuture. Supplyasync(() -> shop. Getname() + " price is " +
shop. Getprice(product), executor);

after this improvement, the completablefutures solution takes 1021 milliseconds to
process five shops and 1022 milliseconds to process nine shops. This trend carries on
until the number of shops reaches the threshold of 400 that you calculated earlier.
This example demonstrates the fact that its a good idea to create an executor that fits
the characteristics of your application and to use completablefutures to submit tasks
to it. This strategy is almost always effective and something to consider when you make
intensive use of asynchronous operations.

Parallelism: via streams or completablefutures?
Youve seen two ways to do parallel computing on a collection: convert the collection
to a parallel stream and use operations such as map on it, or iterate over the collection
and spawn operations within a completablefuture. The latter technique provides
more control by resizing thread pools, which ensures that your overall computation
doesnt block because all your (fixed number of) threads are waiting for i/o.
Our advice for using these apis is as follows:
 if youre doing computation-heavy operations with no i/o, the stream inter-
face provides the simplest implementation and the one thats likely to be
most efficient. (if all threads are compute-bound, theres no point in having
more threads than processor cores. )
 if your parallel units of work involve waiting for i/o (including network connec-
tions), the completablefutures solution provides more flexibility and allows
you to match the number of threads to the wait/computer (w/c) ratio, as dis-
cussed previously. Another reason to avoid using parallel streams when i/o
waits are involved in the stream-processing pipeline is that the laziness of
streams can make it harder to reason about when the waits happen.

Youve learned how to take advantage of completablefutures to provide an asynchro-
nous api to your clients and to function as the client of a synchronous but slow server,
but you performed only a single time-consuming operation in each future. In the
next section, you use completablefutures to pipeline multiple asynchronous opera-
tions in a declarative style similar to what you learned by using the streams api.

16.4 pipelining asynchronous tasks
suppose that all the shops have agreed to use a centralized discount service. This
service uses five discount codes, each of which has a different discount percentage.
You represent this idea by defining a discount. Code enumeration, as shown in the
next listing.
Pipelining asynchronous tasks 403

listing 16.13 an enumeration defining the discount codes

public class discount {
public enum code {
none(0), silver(5), gold(10), platinum(15), diamond(20);
private final int percentage;
code(int percentage) {
this. Percentage = percentage;
}
}
// discount class implementation omitted, see listing 16.14
}

also suppose that the shops have agreed to change the format of the result of the get-
price method, which now returns a string in the format shopname: price: discount-
code. Your sample implementation returns a random discount. Code together with the
random price already calculated, as follows:
public string getprice(string product) {
double price = calculateprice(product);
discount. Code code = discount. Code. Values()[
random. Nextint(discount. Code. Values(). Length)];
return string. Format("%s: %.2f: %s", name, price, code);
}
private double calculateprice(string product) {
delay();
return random. Nextdouble() * product. Charat(0) + product. Charat(1);
}

invoking getprice might then return a string such as

bestprice:123.26: gold

16.4.1 implementing a discount service
your best-price-finder application should now obtain the prices from the shops; parse
the resulting strings; and, for each string, query the discount servers needs. This
process determines the final discounted price of the requested product. (the actual
discount percentage associated with each discount code could change, which is why
you query the server each time. ) the parsing of the strings produced by the shop is
encapsulated in the following quote class:
public class quote {
private final string shopname;
private final double price;
private final discount. Code discountcode;
public quote(string shopname, double price, discount. Code code) {
this. Shopname = shopname;
this. Price = price;
this. Discountcode = code;
}
404 chapter 16 completablefuture: composable asynchronous programming

public static quote parse(string s) {
string[] split = s. Split(": ");
string shopname = split[0];
double price = double. Parsedouble(split[1]);
discount. Code discountcode = discount. Code. Valueof(split[2]);
return new quote(shopname, price, discountcode);
}
public string getshopname() { return shopname; }
public double getprice() { return price; }
public discount. Code getdiscountcode() { return discountcode; }
}

you can obtain an instance of the quote classwhich contains the name of the shop,
the nondiscounted price, and the discount codeby passing the string produced by
a shop to the static parse factory method.
The discount service also has an applydiscount method that accepts a quote
object and returns a string stating the discounted price for the shop that produced
that quote, as shown in the following listing.

Listing 16.14 listing 16.14 the discount service

public class discount {
public enum code {
// source omitted ...
} apply the
public static string applydiscount(quote quote) { discount code to
return quote. Getshopname() + " price is " +
the original price.
Discount. Apply(quote. Getprice(),
quote. Getdiscountcode());
} simulate a delay
private static double apply(double price, code code) { in the discount
delay(); service response.
Return format(price * (100 - code. Percentage) / 100);
}
}

16.4.2 using the discount service
because the discount service is a remote service, you again add a simulated delay of 1
second to it, as shown in the next listing. As you did in section 16.3, first try to reimple-
ment the findprices method to fit these new requirements in the most obvious (but,
sadly, sequential and synchronous) way.

Listing 16.15 simplest findprices implementation that uses the discount service

public list<string> findprices(string product) {
transform retrieve the nondiscounted
return shops. Stream()
the strings price from each shop.
. Map(shop -> shop. Getprice(product))
returned by . Map(quote: : parse)
the shops
. Map(discount: : applydiscount)
in quote contact the discount service to
. Collect(tolist()); apply the discount on each quote.
Objects.
}
pipelining asynchronous tasks 405

you obtain the desired result by pipelining three map operations on the stream
of shops:
 the first operation transforms each shop into a string that encodes the price
and discount code of the requested product for that shop.
 The second operation parses those strings, converting each of them in a
quote object.
 The third operation contacts the remote discount service, which calculates the
final discounted price and returns another string containing the name of the
shop with that price.
As you might imagine, the performance of this implementation is far from optimal.
But try to measure it as usual by running your benchmark:
[bestprice price is 110.93, letssavebig price is 135.58, myfavoriteshop price
is 192.72, buyitall price is 184.74, shopeasy price is 167.28]
done in 10028 msecs

as expected, this code takes 10 seconds to run, because the 5 seconds required to query
the five shops sequentially is added to the 5 seconds consumed by the discount service
in applying the discount code to the prices returned by the five shops. You already know
that you can improve this result by converting the stream to a parallel one. But you also
know (from section 16.3) that this solution doesnt scale well when you increase the
number of shops to be queried, due to the fixed common thread pool on which streams
rely. Conversely, you learned that you can better use your cpu by defining a custom
executor that schedules the tasks performed by the completablefutures.

16.4.3 composing synchronous and asynchronous operations
in this section, you try to reimplement the findprices method asynchronously, again
using the features provided by completablefuture. This next listing shows the code.
Dont worry if something looks unfamiliar; we explain the code in this section.

Listing 16.16 implementing the findprices method with completablefutures

transform the string returned by a shop into asynchronously
a quote object when it becomes available. Retrieve the
public list<string> findprices(string product) { nondiscounted
list<completablefuture<string>> pricefutures = price from each
shops. Stream() shop.
. Map(shop -> completablefuture. Supplyasync(
() -> shop. Getprice(product), executor))
. Map(future -> future. Thenapply(quote: : parse))
. Map(future -> future. Thencompose(quote ->
completablefuture. Supplyasync(
() -> discount. Applydiscount(quote), executor)))
. Collect(tolist());
return pricefutures. Stream() compose the resulting future with another
asynchronous task, applying the discount code.
406 chapter 16 completablefuture: composable asynchronous programming

. Map(completablefuture: : join)
wait for all the futures in the
. Collect(tolist()); stream to be completed and
} extract their respective results.

Things look a bit more complex this time, so try to understand whats going on step-
by-step. Figure 16.3 depicts the sequence of these three transformations.

Your executor
thread thread

shop
supplyasync

task1

shop. Getprice()

thenapply

new quote(price)

thencompose
task2

applydiscount(quote)

join
price figure 16.3 composing synchronous
operations and asynchronous tasks

youre performing the same three map operations that you did in the synchronous
solution of listing 16.15, but you make those operations asynchronous when necessary,
using the feature provided by the completablefuture class.
Getting the prices
youve seen the first of these three operations in various examples in this chapter; you
query the shop asynchronously by passing a lambda expression to the supplyasync
factory method. The result of this first transformation is a stream<completable-
future<string>>, where each completablefuture contains, when complete, the
string returned by the corresponding shop. Note that you configure the completa-
blefutures with the custom executor developed in listing 16.12.
Parsing the quotes
now you have to convert those strings to quotes with a second transformation. But
because this parsing operation isnt invoking any remote service or doing any i/o in gen-
eral, it can be performed almost instantaneously and can be done synchronously without
introducing any delay. For this reason, you implement this second transformation by
pipelining asynchronous tasks 407

invoking the thenapply method on the completablefutures produced by the first step
and passing to it a function converting a string to an instance of quote.
Note that using the thenapply method doesnt block your code until the
completablefuture on which youre invoking it is complete. When the completable-
future finally completes, you want to transform the value that it contains by using
the lambda expression passed to the then-apply method, thus transforming each
completablefuture<string> in the stream into a corresponding completablefuture
<quote>. You can see this process as building a recipe that specifies what to do with
the result of the completablefuture, as when you worked with a stream pipeline.
Composing the futures for calculating the discounted price
the third map operation involves contacting the remote discount service to apply
the appropriate discount percentage to the nondiscounted prices received from the
shops. This transformation is different from the previous one because it has to be exe-
cuted remotely (or, in this case, has to simulate the remote invocation with a delay),
and for this reason, you also want to perform it asynchronously.
To achieve this goal, as you did with the first invocation of supplyasync with get-
price, you pass this operation as a lambda expression to the supplyasync factory
method, which returns another completablefuture. At this point you have two asyn-
chronous operations, modeled with two distinct completablefutures, that you want
to perform in a cascade:
 retrieve the price from a shop and then transform it into a quote.
 Take this quote and pass it to the discount service to obtain the final dis-
counted price.
The java 8 completablefuture api provides the thencompose method specifically for
this purpose, allowing you to pipeline two asynchronous operations, passing the result
of the first operation to the second operation when it becomes available. In other
words, you can compose two completablefutures by invoking the thencompose
method on the first completablefuture and passing to it a function. This function
has as an argument the value returned by that first completablefuture when it com-
pletes, and it returns a second completablefuture that uses the result of the first as
input for its computation. Note that with this approach, while the futures are retriev-
ing the quotes from the shops, the main thread can perform other useful operations,
such as responding to ui events.
Collecting the elements of the stream resulting from these three map operations
into a list, you obtain a list<completablefuture<string>>. Finally, you can wait
for the completion of those completablefutures and extract their values by using
join, exactly as you did in listing 16.11. This new version of the findprices method
implemented in listing 16.8 might produce output like this:
[bestprice price is 110.93, letssavebig price is 135.58, myfavoriteshop price
is 192.72, buyitall price is 184.74, shopeasy price is 167.28]
done in 2035 msecs
408 chapter 16 completablefuture: composable asynchronous programming

the thencompose method you used in listing 16.16, like other methods of the
completablefuture class, has a variant with an async suffix, thencomposeasync. In gen-
eral, a method without the async suffix in its name executes its task in the same thread
as the previous task, whereas a method terminating with async always submits the suc-
ceeding task to the thread pool, so each of the tasks can be handled by a different
thread. In this case, the result of the second completablefuture depends on the first,
so it makes no difference to the final result or to its broad-brush timing whether you
compose the two completablefutures with one or the other variant of this method.
You chose to use the one with thencompose only because its slightly more efficient
due to less thread-switching overhead. Note, however, that it may not always be clear
which thread is being used, especially if you run an application that manages its own
thread pool (such as spring).

16.4.4 combining two completablefutures: dependent and independent
in listing 16.16, you invoked the thencompose method on one completablefuture
and passed to it a second completablefuture, which needed as input the value result-
ing from the execution of the first. In another common case, you need to combine
the results of the operations performed by two independent completablefutures,
and you dont want to wait for the first to complete before starting the second.
In situations like this one, use the thencombine method. This method takes as a sec-
ond argument a bifunction, which defines how the results of the two completable-
futures are to be combined when both become available. Like thencompose, the
thencombine method comes with an async variant. In this case, using the then-
combineasync method causes the combination operation defined by the bifunction to
be submitted to the thread pool and then executed asynchronously in a separate task.
Turning to this chapters running example, you may know that one of the shops pro-
vides prices in  (eur), but you always want to communicate them to your customers in
$(usd). You can asynchronously ask the shop the price of a given product and separately
retrieve, from a remote exchange-rate service, the current exchange rate between  and
$. After both requests have completed, you can combine the results by multiplying the
price by the exchange rate. With this approach, you obtain a third completablefuture
that completes when the results of the two completablefutures are both available and
have been combined by means of the bifunction, as shown in the following listing.

Listing 16.17 combining two independent completablefutures
create a first task querying the shop
future<double> futurepriceinusd =
to obtain the price of a product.
Completablefuture. Supplyasync(() -> shop. Getprice(product))
. Thencombine(
combine the completablefuture. Supplyasync(
price and () -> exchangeservice. Getrate(money. Eur, money. Usd)),
exchange rate (price, rate) -> price * rate
by multiplying )); create a second independent task to retrieve
them. The conversion rate between usd and eur.
Pipelining asynchronous tasks 409

here, because the combination operation is a simple multiplication, performing it in
a separate task would have been a waste of resources, so you need to use the then-
combine method instead of its asynchronous thencombineasync counterpart. Fig-
ure 16.4 shows how the tasks created in listing 16.17 are executed on the different
threads of the pool and how their results are combined.

Your executor executor
thread thread thread 2

shop

supplyasync supplyasync
task1 task2

shop. Getprice() getrate(eur, usd)

thencombine thencombine

(price, rate) -> price * rate

join
price

figure 16.4 combining two independent asynchronous tasks

16.4.5 reflecting on future vs. Completablefuture
the last two examples in listings 16.16 and 16.17 clearly show one of the biggest
advantages of completablefutures over the other pre-java 8 future implementations.
Completablefutures use lambda expressions to provide a declarative api. This api
allows you to easily combine and compose various synchronous and asynchronous
tasks to perform a complex operation in the most effective way. To get a more tangible
idea of the code-readability benefits of completablefuture, try to obtain the result of
listing 16.17 purely in java 7. The next listing shows you how.

Listing 16.18 combining two futures in java 7
create an executorservice allowing you
to submit tasks to a thread pool.
Executorservice executor = executors. Newcachedthreadpool();
final future<double> futurerate = executor. Submit(new callable<double>() {
public double call() {
410 chapter 16 completablefuture: composable asynchronous programming

return exchangeservice. Getrate(money. Eur, money. Usd);
}});
future<double> futurepriceinusd = executor. Submit(new callable<double>() {
public double call() {
double priceineur = shop. Getprice(product); create a future
find the price retrieving the
of the requested return priceineur * futurerate. Get();
exchange rate
product for a }});
between eur
given shop in a multiply the price and exchange rate in and usd.
Second future. The same future used to find the price.

In listing 16.18, you create a first future, submitting a callable to an executor que-
rying an external service to find the exchange rate between eur and usd. Then you
create a second future, retrieving the price in eur of the requested product for a
given shop. Finally, as you did in listing 16.17, you multiply the exchange rate by the
price in the same future that also queried the shop to retrieve the price in eur. Note
that using thencombineasync instead of thencombine in listing 16.17 would have been
equivalent to performing the price by rate multiplication in a third future in listing
16.18. The difference between these two implementations may seem to be small only
because youre combining two futures.

16.4.6 using timeouts effectively
as mentioned in section 16.2.2, its always a good idea to specify a timeout when trying to
read the value calculated by a future to avoid being blocked indefinitely while waiting
for the computation of that value. Java 9 introduced a couple of convenient methods that
enrich the timeout capabilities provided by the completablefutures. The ortimeout
method uses a scheduledthreadexecutor to complete the completablefuture with a
timeoutexception after the specified timeout has elapsed, and it returns another
completablefuture. By using this method, you can further chain your computation
pipeline and deal with the timeoutexception by providing a friendly message back. You
can add a timeout to the future in listing 16.17 and make it throw a timeoutexception
if not completed after 3 seconds by adding this method at the end of the methods
chain, as shown in the next listing. The timeout duration should match your business
requirements, of course.

Listing 16.19 adding a timeout to completablefuture

future<double> futurepriceinusd =
completablefuture. Supplyasync(() -> shop. Getprice(product))
. Thencombine(
completablefuture. Supplyasync(
() -> exchangeservice. Getrate(money. Eur, money. Usd)),
(price, rate) -> price * rate
)) make the future throw a timeout-
. Ortimeout(3, timeunit. Seconds); exception if not completed after
3 seconds. Asynchronous timeout
management was added in java 9.
Reacting to a completablefuture completion 411

sometimes, its also acceptable to use a default value in case a service is momentar-
ily unable to respond in a timely manner. You might decide that in listing 16.19, you
want to wait for the exchange service to provide the current eur-to-usd exchange
rate for no more than 1 second, but if the request takes longer to complete, you
dont want to abort the whole the computation with an exception. Instead, you can
fall back by using a predefined exchange rate. You can easily add this second kind
of timeout by using the completeontimeout method, also introduced in java 9 (the
following listing).

Listing 16.20 completing a completablefuture with a default value after a timeout

future<double> futurepriceinusd =
completablefuture. Supplyasync(() -> shop. Getprice(product))
. Thencombine(
completablefuture. Supplyasync(
() -> exchangeservice. Getrate(money. Eur, money. Usd))
. Completeontimeout(default_rate, 1, timeunit. Seconds),
(price, rate) -> price * rate
)) use a default exchange rate if
. Ortimeout(3, timeunit. Seconds); the exchange service doesnt
provide a result in 1 second.

Like the ortimeout method, the completeontimeout method returns a completable-
future, so you can chain it with other completablefuture methods. To recap, youve
configured two kinds of timeouts: one that makes the whole computation fail if it
takes more than 3 seconds, and one that expires in 1 second but completes the
future with a predetermined value instead of causing a failure.
Youre almost finished with your best-price-finder application, but one ingredient
is still missing. Youd like to show your users the prices provided by the shops as soon
as they become available (as car insurance and flight-comparison websites typically
do), instead of waiting for all the price requests to complete, as youve done up to
now. In the next section, you discover how to achieve this goal by reacting to the com-
pletion of a completablefuture instead of invoking get or join on it and thereby
remaining blocked until the completablefuture itself completes.

16.5 reacting to a completablefuture completion
in all the code examples youve seen in this chapter, youve simulated methods that
do remote invocations with a 1-second delay in their response. In a real-world sce-
nario, the remote services you need to contact from your application are likely to have
unpredictable delays caused by everything from server load to network delays, and
perhaps by how valuable the server regards your applications business to be com-
pared with that of applications that pay more per query.
For these reasons, its likely the prices of the products you want to buy will be
available for some shops far earlier than for others. In the next listing, you simulate
412 chapter 16 completablefuture: composable asynchronous programming

this scenario by introducing a random delay of 0.5 to 2.5 seconds, using the random-
delay method instead of the delay method that waits 1 second.

Listing 16.21 a method to simulate a random delay between 0.5 and 2.5 seconds

private static final random random = new random();
public static void randomdelay() {
int delay = 500 + random. Nextint(2000);
try {
thread. Sleep(delay);
} catch (interruptedexception e) {
throw new runtimeexception(e);
}
}

up to now, youve implemented the findprices method so that it shows the prices
provided by the shops only when all of them are available. Now you want to have the
best-price-finder application display the price for a given shop as soon as it becomes
available without waiting for the slowest one (which may even time out). How can you
achieve this further improvement?

16.5.1 refactoring the best-price-finder application
the first thing to avoid is waiting for the creation of a list that already contains all
the prices. You need to work directly with the stream of completablefutures, in
which each completablefuture is executing the sequence of operations necessary
for a given shop. In the next listing, you refactor the first part of the implementa-
tion from listing 16.16 into a findpricesstream method to produce this stream of
completablefutures.

Listing 16.22 refactoring the findprices method to return a stream of futures

public stream<completablefuture<string>> findpricesstream(string product) {
return shops. Stream()
. Map(shop -> completablefuture. Supplyasync(
() -> shop. Getprice(product), executor))
. Map(future -> future. Thenapply(quote: : parse))
. Map(future -> future. Thencompose(quote ->
completablefuture. Supplyasync(
() -> discount. Applydiscount(quote), executor)));
}

at this point, you add a fourth map operation on the stream returned by the find-
pricesstream method to the three already performed inside that method. This new
operation registers an action on each completablefuture; this action consumes the
value of the completablefuture as soon as it completes. The java 8 completable-
future api provides this feature via the thenaccept method, which takes as an argu-
ment a consumer of the value with which it completes. In this case, this value is the
reacting to a completablefuture completion 413

string returned by the discount services and containing the name of a shop together
with the discounted price of the requested product for that shop. The only action that
you want to perform to consume this value is to print it:

findpricesstream("myphone"). Map(f -> f. Thenaccept(system. Out: : println));

as youve seen for the thencompose and thencombine methods, the thenaccept method
has an async variant named thenacceptasync. The async variant schedules the exe-
cution of the consumer passed to it on a new thread from the thread pool instead of
performing it directly, using the same thread that completed the completable-
future. Because you want to avoid an unnecessary context switch, and because
(more important) you want to react to the completion of the completablefuture as
soon as possible instead waiting for a new thread to be available, you dont use this
variant here.
Because the thenaccept method already specifies how to consume the result pro-
duced by the completablefuture when it becomes available, it returns a completable-
future<void>. As a result, the map operation returns a stream<completablefuture
<void>>. You cant do much with a completablefuture<void> except wait for its com-
pletion, but this is exactly what you need. You also want to give the slowest shop a
chance to provide its response and print its returned price. To do so, you can put all
the completablefuture<void>s of the stream into an array and then wait for all of
them to complete, as shown in this next listing.

Listing 16.23 reacting to completablefuture completion

completablefuture[] futures = findpricesstream("myphone")
. Map(f -> f. Thenaccept(system. Out: : println))
. Toarray(size -> new completablefuture[size]);
completablefuture. Allof(futures). Join();

the allof factory method takes as input an array of completablefutures and returns a
completablefuture<void> thats completed only when all the completablefutures
passed have completed. Invoking join on the completablefuture returned by the
allof method provides an easy way to wait for the completion of all the completable-
futures in the original stream. This technique is useful for the best-price-finder applica-
tion because it can display a message such as all shops returned results or timed out
so that a user doesnt keep wondering whether more prices might become available.
In other applications, you may want to wait for the completion of only one of the
completablefutures in an array, perhaps if youre consulting two currency-exchange
servers and are happy to take the result of the first to respond. In this case, you can
use the anyof factory method. As a matter of detail, this method takes as input an
array of completablefutures and returns a completablefuture<object> that com-
pletes with the same value as the first-to-complete completablefuture.
414 chapter 16 completablefuture: composable asynchronous programming

16.5.2 putting it all together
as discussed at the beginning of section 16.5, now suppose that all the methods simu-
lating a remote invocation use the randomdelay method of listing 16.21, introducing a
random delay distributed between 0.5 and 2.5 seconds instead of a delay of 1 second.
Running the code in listing 16.23 with this change, you see that the prices provided by
the shops dont appear all at the same time, as happened before, but are printed
incrementally as soon as the discounted price for a given shop is available. To make
the result of this change more obvious, the code is slightly modified to report a time-
stamp showing the time taken for each price to be calculated:
long start = system. Nanotime();
completablefuture[] futures = findpricesstream("myphone27s")
. Map(f -> f. Thenaccept(
s -> system. Out. Println(s + " (done in " +
((system. Nanotime() - start) / 1_000_000) + " msecs)")))
. Toarray(size -> new completablefuture[size]);
completablefuture. Allof(futures). Join();
system. Out. Println("all shops have now responded in "
+ ((system. Nanotime() - start) / 1_000_000) + " msecs");

running this code produces output similar to the following:
buyitall price is 184.74 (done in 2005 msecs)
myfavoriteshop price is 192.72 (done in 2157 msecs)
letssavebig price is 135.58 (done in 3301 msecs)
shopeasy price is 167.28 (done in 3869 msecs)
bestprice price is 110.93 (done in 4188 msecs)
all shops have now responded in 4188 msecs

you can see that, due to the effect of the random delays, the first price now prints
more than twice as fast as the last!

16.6 road map
chapter 17 explores the java 9 flow api, which generalizes the idea of computable-
future (one-shot, either computing or terminated-with-a-value) by enabling computa-
tions to produce a series of values before optionally terminating.

Summary
 executing relatively long-lasting operations by using asynchronous tasks can
increase the performance and responsiveness of your application, especially if it
relies on one or more remote external services.
 You should consider providing an asynchronous api to your clients. You can
easily implement one by using completablefutures features.
 A completablefuture allows you to propagate and manage errors generated
within an asynchronous task.
 You can asynchronously consume from a synchronous api by wrapping its invo-
cation in a completablefuture.
Summary 415

 you can compose or combine multiple asynchronous tasks when theyre inde-
pendent and when the result of one of them is used as the input to another.
 You can register a callback on a completablefuture to reactively execute some
code when the future completes and its result becomes available.
 You can determine when all values in a list of completablefutures have com-
pleted, or you can wait for only the first to complete.
 Java 9 added support for asynchronous timeouts on completablefuture via the
ortimeout and completeontimeout methods.
Reactive programming

this chapter covers
 defining reactive programming and discussing the
principles of the reactive manifesto
 reactive programming at the application and
system levels
 showing example code using reactive streams
and the java 9 flow api
 introducing rxjava, a widely used reactive library
 exploring the rxjava operations to transform and
combine multiple reactive streams
 presenting marble diagrams that visually
document operations on reactive streams

before we dig into what reactive programming is and how it works, its helpful to
clarify why this new paradigm is of growing importance. A few years ago, the larg-
est applications had tens of servers and gigabytes of data; response times of sev-
eral seconds and offline-maintenance times measured in hours were considered
to be acceptable. Nowadays, this situation is changing rapidly for at least three
reasons:

416
the reactive manifesto 417

 big databig data usually is measured in petabytes and increasing daily.
 Heterogeneous environmentsapplications are deployed in diverse environments
ranging from mobile devices to cloud-based clusters running thousands of mul-
ticore processors.
 Use patternsusers expect millisecond response times and 100 percent uptime.

These changes imply that todays demands arent being met by yesterdays software
architectures. This situation has become evident especially now that mobile devices
are the biggest source of internet traffic, and things can only worsen in the near future
when such traffic is overtaken by the internet of things (iot).
Reactive programming addresses these issues by allowing you to process and combine
streams of data items coming from different systems and sources in an asynchronous way.
In fact, applications written following this paradigm react to data items as they occur,
which allows them to be more responsive in their interactions with users. Moreover, the
reactive approach can be applied not only to building a single component or application,
but also to coordinating many components into a whole reactive system. Systems engi-
neered in this way can exchange and route messages in varying network conditions
and provide availability under heavy load while taking into consideration failures and
outages. (note that although developers traditionally see their systems or applications
as being built from components, in this new mashup, loosely coupled style of building
systems, these components are often whole applications themselves. Hence, compo-
nents and applications are near synonyms. )
the features and advantages that characterize reactive applications and systems
are crystallized in the reactive manifesto, which we discuss in the next section.

17.1 the reactive manifesto
the reactive manifesto (https: //www. Reactivemanifesto. Org)developed in 2013 and
2014 by jonas boner, dave farley, roland kuhn, and martin thompsonformalized a
set of core principles for developing reactive applications and systems. The manifesto
identified four characteristic features:
 responsivea reactive system has a fast and, even more important, consistent,
predictable response time. As a result, the user knows what to expect. This fact
in turn increases user confidence, which is without a doubt the key aspect of a
usable application.
 Resilienta system has to remain responsive despite failures. The reactive man-
ifesto suggests different techniques to achieve resiliency, including replicating
the execution of components, decoupling these components in time (sender
and receiver have independent life cycles) and space (sender and receiver run
in different processes), and letting each component asynchronously delegate
tasks to other components.
 Elasticanother issue that harms the responsiveness of applications is the fact
that they can be subject to different workloads during their life cycles. Reactive
418 chapter 17 reactive programming

systems are designed to react automatically to a heavier workload by increasing
the number of resources allocated to the affected components.
 Message-drivenresilience and elasticity require the boundaries of the compo-
nents that form the system to be clearly defined to ensure loose coupling, isola-
tion, and location transparency. Communication across these boundaries is
performed through asynchronous message passing. This choice enables both
resiliency (by delegating failures as messages) and elasticity (by monitoring the
number of exchanged messages and then scaling the number of the resources
intended to manage them accordingly).
Figure 17.1 shows how these four features are related and dependent on one another.
These principles are valid at different scales, from structuring the internals of a small
application to determining how these applications have to be coordinated to build a
large system. Specific points concerning the level of granularity where these ideas are
applied, however, deserve to be discussed in further detail.

The system responds in
a timely manner if at all
possible. Responsiveness
the system stays
is the cornerstone of usability.
Responsive in the
face of failure.
Responsive

elastic resilient

message-driven

the system stays responsive
under varying workload. It can
react to changes in the input rate the system relies on asynchronous message
by increasing or decreasing the passing to establish a boundary between
resources allocated to service components that ensures loose coupling,
these inputs. Isolation, and location transparency.

Figure 17.1 the key features of a reactive system

17.1.1 reactive at application level
the main feature of reactive programming for application-level components allows
tasks to be executed asynchronously. As we discuss in the rest of this chapter, process-
ing streams of events in an asynchronous and nonblocking way is essential for maxi-
mizing the use rate of modern multicore cpus and, more precisely, of the threads
competing for their use. To achieve this goal, the reactive frameworks and libraries
share threads (relatively expensive and scarce resources) among lighter constructs
such as futures; actors; and (more commonly) event loops dispatching a sequence of
callbacks intended to aggregate, transform, and manage the events to be processed.
The reactive manifesto 419

background knowledge check
if youre puzzled by terms such as event, message, signal, and event loop (or publish-
subscribe, listener, and backpressure, which are used later in this chapter), please
read the gentler introduction in chapter 15. If not, read on.

These techniques not only have the benefit of being cheaper than threads, but also have
a major advantage from developers point of view: they raise the level of abstraction of
implementing concurrent and asynchronous applications, allowing developers to con-
centrate on the business requirements instead of dealing with typical problems of low-
level multithreading issues such as synchronization, race conditions, and deadlocks.
The most important thing to pay attention to when using these thread-multiplex-
ing strategies is to never perform blocking operations inside the main event loop. Its
helpful to include as blocking operations all i/o-bound operations such as accessing
a database or the file system or calling a remote service that may take a long or unpre-
dictable time to complete. Its easy and interesting to explain why you should avoid
blocking operations by providing a practical example.
Imagine a simplified yet typical multiplexing scenario with a pool of two threads
processing three streams of events. Only two streams can be processed at the same
time and the streams have to compete to share those two threads as fairly and effi-
ciently as possible. Now suppose that processing one streams event triggers a poten-
tially slow i/o operation, such as writing into the file system or retrieving data from a
database by using a blocking api. As figure 17.2 shows, in this situation thread 2 is
wastefully blocked waiting for the i/o operation to complete, so although the thread
1 can process the first stream, the third stream cant be processed before the blocking
operation finishes.

Processing events

stream 1
thread 1

stream 2
thread 2

blocking i/o

stream 3 events of stream 3 cannot be processed
by thread 2, which is wastefully blocked

figure 17.2 a blocking operation wastefully keeps a thread busy preventing
it from performing other computations.
420 chapter 17 reactive programming

to overcome this problem, most reactive frameworks (such as rxjava and akka) allow
blocking operations to be executed by means of a separate dedicated thread pool. All
the threads in the main pool are free to run uninterruptedly, keeping all the cores of
the cpu at the highest possible use rate. Keeping separate thread pools for cpu-
bound and i/o-bound operations has the further benefit of allowing you to size and
configure the pools with a finer granularity and to monitor the performance of these
two kinds of tasks more precisely.
Developing applications by following the reactive principles is only one aspect of
reactive programming and often not even the hardest one. Having a set of beautifully
designed reactive applications performing efficiently in isolation is at least as import-
ant as making them cooperate in a well-coordinated reactive system.

17.1.2 reactive at system level
a reactive system is a software architecture that allows multiple applications to work as a
single coherent, resilient platform and also allows these applications to be sufficiently
decoupled so when one of them fails, it doesnt bring down the whole system. The main
difference between reactive applications and systems is that the former type usually per-
form computations based on ephemeral streams of data and are called event-driven.
The latter type are intended to compose the applications and facilitate communication.
Systems with this property are often referred to as being message-driven.
The other important distinction between messages and events is the fact that mes-
sages are directed toward a defined single destination, whereas events are facts that will
be received by the components that are registered to observe them. In reactive systems,
its also essential for these messages to be asynchronous to keep the sending and the
receiving operations decoupled from the sender and receiver, respectively. This decou-
pling is a requirement for full isolation between components and is fundamental for
keeping the system responsive under both failures (resilience) and heavy load (elasticity).
More precisely, resilience is achieved in reactive architectures by isolating failures
in the components where they happen to prevent the malfunctions from being propa-
gated to adjacent components and from there in a catastrophic cascade to the rest of
the system. Resilience in this reactive sense is more than fault-tolerance. The system
doesnt gracefully degrade but fully recovers from failures by isolating them and
bringing the system back to a healthy state. This magic is obtained by containing the
errors and reifying them as messages that are sent to other components acting as
supervisors. In this way, the management of the problem can be performed from a
safe context external to the failing component itself.
As isolation and decoupling are key for resilience, the main enabler for elasticity is
location transparency, which allows any component of a reactive system to communicate
with any other service, regardless of where the recipient resides. Location transpar-
ency in turn allows the system to replicate and (automatically) scale any application
depending on the current workload. Such location-agnostic scaling shows another dif-
ference between reactive applications (asynchronous and concurrent and decoupled
reactive streams and the flow api 421

in time) and reactive systems (which can become decoupled in space through loca-
tion transparency).
In the rest of this chapter, you put some of these ideas into practice with a few
examples of reactive programming, and in particular, you explore java 9s flow api.

17.2 reactive streams and the flow api
reactive programming is programming that uses reactive streams. Reactive streams are a
standardized technique (based on the publish-subscribe, or pub-sub, protocol explained
in chapter 15) to process potentially unbounded streams of data asynchronously, in
sequence and with mandatory nonblocking backpressure. Backpressure is a flow-
control mechanism used in publish-subscribe to prevent a slow consumer of the
events in the stream from being overwhelmed by one or more faster producers. When
this situation occurs, its unacceptable for the component under stress to fail cata-
strophically or to drop events in an uncontrolled fashion. The component needs a
way to ask the upstream producers to slow down or to tell them how many events it
can accept and process at a given time before receiving more data.
Its worth noting that the requirement for built-in backpressure is justified by the
asynchronous nature of the stream processing. In fact, when synchronous invocations
are being performed, the system is implicitly backpressured by the blocking apis.
Unfortunately, this situation prevents you from executing any other useful task until
the blocking operation is complete, so you end up wasting a lot of resources by wait-
ing. Conversely, with asynchronous apis you can maximize the use rate of your hard-
ware, but run the risk of overwhelming some other slower downstream component.
Backpressure or flow-control mechanisms come into play in this situation; they estab-
lish a protocol that prevents data recipients from being overwhelmed without having
to block any threads.
These requirements and the behavior that they imply were condensed in the reac-
tive streams1 project (www. Reactive-streams. Org), which involved engineers from net-
flix, red hat, twitter, lightbend, and other companies, and produced the definition
of four interrelated interfaces representing the minimal set of features that any reac-
tive streams implementation has to provide. These interfaces are now part of java 9,
nested within the new java. Util. Concurrent. Flow class, and implemented by many
third-party libraries, including akka streams (lightbend), reactor (pivotal), rxjava
(netflix), and vert. X (red hat). In the next section, we examine in detail the methods
declared by these interfaces and clarify how theyre expected to be used to express
reactive components.

17.2.1 introducing the flow class
java 9 adds one new class for reactive programming: java. Util. Concurrent. Flow.
This class contains only static components and cant be instantiated. The flow class

1
we capitalize for the reactive streams project, but use reactive streams for the concept.
422 chapter 17 reactive programming

contains four nested interfaces to express the publish-subscribe model of reactive pro-
gramming as standardized by the reactive streams project:
 publisher
 subscriber
 subscription
 processor

the flow class allows interrelated interfaces and static methods to establish flow-
controlled components, in which publishers produce items consumed by one or
more subscribers, each managed by a subscription. The publisher is a provider of
a potentially unbounded number of sequenced events, but its constrained by the
backpressure mechanism to produce them according to the demand received from its
subscriber(s). The publisher is a java functional interface (declares only one single
abstract method) that allows a subscriber to register itself as a listener of the events
issued by the publisher; flow control, including backpressure, between publishers
and subscribers is managed by a subscription. These three interfaces, along with
the processor interface, are captured in listings 17.1, 17.2, 17.3, and 17.4.

Listing 17.1 the flow. Publisher interface

@functionalinterface
public interface publisher<t> {
void subscribe(subscriber<? Super t> s);
}

on the other side, the subscriber interface has four callback methods that are invoked
by the publisher when it produces the corresponding events.

Listing 17.2 the flow. Subscriber interface

public interface subscriber<t> {
void onsubscribe(subscription s);
void onnext(t t);
void onerror(throwable t);
void oncomplete();
}

those events have to be published (and the corresponding methods invoked) strictly
following the sequence defined by this protocol:

onsubscribe onnext* (onerror | oncomplete)?

This notation means that onsubscribe is always invoked as the first event, followed by
an arbitrary number of onnext signals. The stream of events can go on forever, or it
can be terminated by an oncomplete callback to signify that no more elements will be
produced or by an onerror if the publisher experiences a failure. (compare reading
from a terminal when you get a string or an indication of an end-of-file or i/o error. )
reactive streams and the flow api 423

when a subscriber registers itself on a publisher, the publishers first action
is to invoke the onsubscribe method to pass back a subscription object. The
subscription interface declares two methods. The subscriber can use the first
method to notify the publisher that its ready to process a given number of events;
the second method allows it to cancel the subscription, thereby telling the publisher
that its no longer interested in receiving its events.

Listing 17.3 the flow. Subscription interface

public interface subscription {
void request(long n);
void cancel();
}

the java 9 flow specification defines a set of rules through which the implementa-
tions of these interfaces should cooperate. These rules can be summarized as follows:
 the publisher must send the subscriber a number of elements no greater
than that specified by the subscriptions request method. A publisher, how-
ever, may send fewer onnext than requested and terminate the subscription
by calling oncomplete if the operation terminated successfully or onerror if it
failed. In these cases, when a terminal state has been reached (oncomplete or
onerror), the publisher cant send any other signal to its subscribers, and the
subscription has to be considered to be canceled.
 The subscriber must notify the publisher that its ready to receive and pro-
cess n elements. In this way, the subscriber exercises backpressure on the
publisher preventing the subscriber from being overwhelmed by too many
events to manage. Moreover, when processing the oncomplete or onerror sig-
nals, the subscriber isnt allowed to call any method on the publisher or
subscription and must consider the subscription to be canceled. Finally, the
subscriber must be prepared to receive these terminal signals even without
any preceding call of the subscription. Request() method and to receive one
or more onnext even after having called subscription. Cancel().
 The subscription is shared by exactly one publisher and subscriber and rep-
resents the unique relationship between them. For this reason, it must allow the
subscriber to call its request method synchronously from both the onsubscribe
and onnext methods. The standard specifies that the implementation of the
subscription. Cancel() method has to be idempotent (calling it repeatedly
has the same effect as calling it once) and thread-safe so that, after the first time
it has been called, any other additional invocation on the subscription has no
effect. Invoking this method asks the publisher to eventually drop any references
to the corresponding subscriber. Resubscribing with the same subscriber object
is discouraged, but the specification doesnt mandate an exception being raised
in this situation because all previously canceled subscriptions would have to be
stored indefinitely.
424 chapter 17 reactive programming

figure 17.3 shows the typical life cycle of an application implementing the interfaces
defined by the flow api.

Main publisher subscriber subscription

subscribe(subscriber)
backpressure
onsubscribe(subscription)
request(int)

onnext(data)

onnext(data)

request(int)

onnext(data)

oncomplete/onerror

onsubscribe onnext* (onerror | oncomplete)
figure 17.3 the life cycle of a reactive application using the flow api

the fourth and final member of the flow class is the processor interface, which
extends both publisher and subscriber without requiring any additional method.

Listing 17.4 the flow. Processor interface

public interface processor<t, r> extends subscriber<t>, publisher<r> { }

in fact, this interface represents a transformation stage of the events processed through
the reactive stream. When receiving an error, the processor can choose to recover from it
(and then consider the subscription to be canceled) or immediately propagate the
onerror signal to its subscriber(s). The processor should also cancel its upstream sub-
scription when its last subscriber cancels its subscription to propagate the cancella-
tion signal (even though this cancellation isnt strictly required by the specification).
The java 9 flow api/reactive streams api mandates that any implementation of
all the methods of the subscriber interface should never block the publisher, but it
doesnt specify whether these methods should process the events synchronously or
asynchronously. Note, however, that all methods defined by these interfaces return
void so that they can be implemented in a completely asynchronous way.
In this next section, you try to put to work what youve learned so far through a
simple, practical example.

17.2.2 creating your first reactive application
the interfaces defined in the flow class are, in most cases, not intended to be imple-
mented directly. Unusually, the java 9 library doesnt provide classes that implement
reactive streams and the flow api 425

them either! These interfaces are implemented by the reactive libraries that weve
already mentioned (akka, rxjava, and so on). The java 9 specification of java. Util
. Concurrency. Flow works both as a contract to which all those libraries must adhere
and a lingua franca allowing reactive applications developed on top of different reac-
tive libraries to cooperate and talk to one another. Moreover, those reactive libraries
typically offer many more features (classes and methods that transform and merge
reactive streams beyond the minimal subset specified by the java. Util. Concurrency
. Flow interface).
That being said, it makes sense for you to develop a first reactive application directly
on top of the java 9 flow api to get a feeling for how the four interfaces discussed in the
preceding sections work together. To this end, youll write a simple temperature-report-
ing program using reactive principles. This program has two components:
 tempinfo, which mimics a remote thermometer (constantly reporting randomly
chosen temperatures between 0 and 99 degrees fahrenheit, which is appropri-
ate for u. S. Cities most of the time)
 tempsubscriber, which listens to these reports and prints the stream of tem-
peratures reported by a sensor installed in a given city

the first step is defining a simple class that conveys the currently reported tempera-
ture, as shown in the following listing.

Listing 17.5 a java bean conveying the currently reported temperature

import java. Util. Random;

public class tempinfo {

public static final random random = new random();

private final string town;
private final int temp; tempinfo instance
for a given town is
public tempinfo(string town, int temp) {
created via a static
factory method.
This. Town = town;
this. Temp = temp;
} fetching the current
temperature may
public static tempinfo fetch(string town) { randomly fail one
if (random. Nextint(10) == 0) time out of ten.
Throw new runtimeexception("error! ");
return new tempinfo(town, random. Nextint(100));
returns a random
} temperature in
the range 0 to 99
@override degrees fahrenheit
public string tostring() {
return town + " : " + temp;
}
426 chapter 17 reactive programming

public int gettemp() {
return temp;
}

public string gettown() {
return town;
}
}

after defining this simple domain model, you can implement a subscription for the
temperatures of a given town that sends a temperature report whenever this report is
requested by its subscriber as shown in the following listing.

Listing 17.6 a subscription sending a stream of tempinfo to its subscriber

import java. Util. Concurrent. Flow. *;

public class tempsubscription implements subscription {

private final subscriber<? Super tempinfo> subscriber;
private final string town;

public tempsubscription( subscriber<? Super tempinfo> subscriber,
string town ) {
this. Subscriber = subscriber;
this. Town = town;
}
loops once
@override per request
public void request( long n ) { made by the
for (long i = 0l; i < n; i++) { subscriber
try {
sends the current subscriber. Onnext( tempinfo. Fetch( town ) );
temperature to } catch (exception e) {
the subscriber subscriber. Onerror( e );
in case of a failure while fetching
break; the temperature propagates the
} error to the subscriber
}
}
if the subscription is
@override canceled, send a
public void cancel() { completion (oncomplete)
subscriber. Oncomplete(); signal to the subscriber.
}
}

the next step is creating a subscriber that, every time it gets a new element, prints the
temperatures received from the subscription and asks for a new report as shown in the
next listing.
Reactive streams and the flow api 427

listing 17.7 a subscriber printing the received temperatures

import java. Util. Concurrent. Flow. *;

public class tempsubscriber implements subscriber<tempinfo> {

private subscription subscription; stores the
subscription
@override and sends a
public void onsubscribe( subscription subscription ) { first request
this. Subscription = subscription;
subscription. Request( 1 );
}
prints the received
@override temperature and
public void onnext( tempinfo tempinfo ) { requests a further one
system. Out. Println( tempinfo );
subscription. Request( 1 );
}
prints the error
@override message in case
public void onerror( throwable t ) { of an error
system. Err. Println(t. Getmessage());
}

@override
public void oncomplete() {
system. Out. Println("done! ");
}
}

the next listing puts your reactive application to work with a main class that creates a
publisher and then subscribes to it by using tempsubscriber.

Listing 17.8 a main class: creating a publisher and subscribing
tempsubscriber to it

import java. Util. Concurrent. Flow. *; creates a new publisher of temperatures
in new york and subscribes the
public class main { tempsubscriber to it
public static void main( string[] args ) {
gettemperatures( "new york" ). Subscribe( new tempsubscriber() );
}

private static publisher<tempinfo> gettemperatures( string town ) {
return subscriber -> subscriber. Onsubscribe(
new tempsubscription( subscriber, town ) );
}
} returns a publisher that sends a tempsubscription
to the subscriber that subscribes to it

here, the gettemperatures method returns a lambda expression that takes a
subscriber as an argument and invokes its onsubscribe method, passing to it a new
428 chapter 17 reactive programming

tempsubscription instance. Because the signature of this lambda is identical to the
only abstract method of the publisher functional interface, the java compiler can
automatically convert the lambda to a publisher (as you learned in chapter 3). The
main method creates a publisher for the temperatures in new york and then sub-
scribes a new instance of the tempsubscriber class to it. Running main produces out-
put something like this:
new york : 44
new york : 68
new york : 95
new york : 30
error!

In the preceding run, tempsubscription successfully fetched the temperature in new
york four times but failed on the fifth reading. It seems that you correctly imple-
mented the problem by using three of the four interfaces of the flow api. But are you
sure that there arent any mistakes in the code? Give this question some thought by
completing the following quiz.

Quiz 17.1:
the example developed so far has a subtle problem. This problem, however, is hid-
den by the fact that at some point, the stream of temperatures will be interrupted by
the error randomly generated inside the tempinfo factory method. Can you guess
what will happen if you comment out the statement generating the random error and
let your main run long enough?
Answer:
the problem with what youve done so far is that every time the tempsubscriber
receives a new element into its onnext method, it sends a new request to the temp-
subscription, and then the request method sends another element to the temp-
subscriber itself. These recursive invocations are pushed onto the stack one after the
other until the stack overflows, generating stackoverflowerror like the following:
exception in thread "main" java. Lang. Stackoverflowerror
at java. Base/java. Io. Printstream. Print(printstream. Java:666)
at java. Base/java. Io. Printstream. Println(printstream. Java:820)
at flow. Tempsubscriber. Onnext(tempsubscriber. Java:36)
at flow. Tempsubscriber. Onnext(tempsubscriber. Java:24)
at flow. Tempsubscription. Request(tempsubscription. Java:60)
at flow. Tempsubscriber. Onnext(tempsubscriber. Java:37)
at flow. Tempsubscriber. Onnext(tempsubscriber. Java:24)
at flow. Tempsubscription. Request(tempsubscription. Java:60)

...

What can you do to fix this problem and avoid overflowing the stack? One possible
solution is to add an executor to the tempsubscription and then use it to send new
elements to the tempsubscriber from a different thread. To achieve this goal, you can
reactive streams and the flow api 429

modify the tempsubscription as shown in the next listing. (the class is incomplete;
the full definition uses the remaining definitions from listing 17.6. )

listing 17.9 adding an executor to the tempsubscription

import java. Util. Concurrent. Executorservice; unmodified
import java. Util. Concurrent. Executors; code of original
tempsubscription
public class tempsubscription implements subscription { has been omitted.

Private static final executorservice executor =
executors. Newsinglethreadexecutor();

@override
public void request( long n ) { sends the next elements
executor. Submit( () -> { to the subscriber from a
for (long i = 0l; i < n; i++) { different thread
try {
subscriber. Onnext( tempinfo. Fetch( town ) );
} catch (exception e) {
subscriber. Onerror( e );
break;
}
}
});
}
}

so far, youve used only three of the four interfaces defined by the flow api. What
about the processor interface? A good example of how to use that interface is to cre-
ate a publisher that reports the temperatures in celsius instead of fahrenheit (for
subscribers outside the united states).

17.2.3 transforming data with a processor
as described in section 17.2.1, a processor is both a subscriber and a publisher. In
fact, its intended to subscribe to a publisher and republish the data that it receives
after transforming that data. As a practical example, implement a processor that sub-
scribes to a publisher that emits temperatures in fahrenheit and republishes them
after converting them to celsius, as shown in this next listing.

Listing 17.10 a processor transforming temperatures from fahrenheit to celsius

a processor transforming a
import java. Util. Concurrent. Flow. *; tempinfo into another tempinfo
public class tempprocessor implements processor<tempinfo, tempinfo> {

private subscriber<? Super tempinfo> subscriber;

@override
public void subscribe( subscriber<? Super tempinfo> subscriber ) {
430 chapter 17 reactive programming

this. Subscriber = subscriber;
} republishes the tempinfo
after converting the
@override temperature to celsius
public void onnext( tempinfo temp ) {
subscriber. Onnext( new tempinfo( temp. Gettown(),
(temp. Gettemp() - 32) * 5 / 9) );
}

@override
public void onsubscribe( subscription subscription ) {
subscriber. Onsubscribe( subscription );
}

@override all other signals
public void onerror( throwable throwable ) { are delegated
subscriber. Onerror( throwable ); unchanged to
} the upstream
subscriber.
@override
public void oncomplete() {
subscriber. Oncomplete();
}
}

note that the only method of the tempprocessor that contains some business logic is
onnext, which republishes temperatures after converting them from fahrenheit to
celsius. All other methods that implement the subscriber interface merely pass on
unchanged (delegate) all received signals to the upstream subscriber, and the
publishers subscribe method registers the upstream subscriber into the processor.
The next listing puts the tempprocessor to work by using it in your main class.

Listing 17.11 main class: create a publisher and subscribe tempsubscriber to it

import java. Util. Concurrent. Flow. *; creates a new publisher
of celsius temperatures
public class main { for new york
public static void main( string[] args ) {
getcelsiustemperatures( "new york" ) subscribes the
. Subscribe( new tempsubscriber() ); tempsubscriber
} to the publisher

public static publisher<tempinfo> getcelsiustemperatures(string town) {
return subscriber -> {
tempprocessor processor = new tempprocessor();
processor. Subscribe( subscriber );
processor. Onsubscribe( new tempsubscription(processor, town) );
};
} creates a tempprocessor and puts it between
} the subscriber and returned publisher
using the reactive library rxjava 431

this time, running main produces the following output, with temperatures that are
typical of the celsius scale:
new york : 10
new york : -12
new york : 23
error!

In this section, you directly implemented the interfaces defined in the flow api, and
in doing so, you became familiar with asynchronous stream processing via the publish-
subscribe protocol that forms the core idea of the flow api. But there was something
slightly unusual about this example, which we turn to in the next section.

17.2.4 why doesnt java provide an implementation of the flow api?
The flow api in java 9 is rather odd. The java library generally provides interfaces
and implementations for them, but here, youve implemented the flow api yourself.
Lets make a comparison with the list api. As you know, java provides the list<t>
interface thats implemented by many classes, including arraylist<t>. More pre-
cisely (and rather invisibly to the user) the class arraylist<t> extends the abstract
class abstractlist<t>, which implements the interface list<t>. By contrast, java 9
declares the interface publisher<t> and provides no implementation, which is why
you had to define your own (apart from the learning benefit you got from implement-
ing it). Lets face itan interface on its own may help you structure your program-
ming thoughts, but it doesnt help you write programs any faster!
Whats going on? The answer is historic: there were multiple java code libraries of
reactive streams (such as akka and rxjava). Originally, these libraries were developed
separately, and although they implemented reactive programming via publish-sub-
scribe ideas, they used different nomenclature and apis. During the standardization
process of java 9, these libraries evolved so that their classes formally implemented the
interfaces in java. Util. Concurrent. Flow, as opposed to merely implementing the
reactive concepts. This standard enables more collaboration among different libraries.
Note that building a reactive-streams implementation is complex, so most users
will merely use an existing one. Like many classes that implement an interface, they
typically provide richer functionality than is required for a minimal implementation.
In the next section, you use one of the most widely used libraries: the rxjava (reac-
tive extensions to java) library developed by netflix, specifically the current rxjava 2.0
version, which implements the java 9 flow interfaces.

17.3 using the reactive library rxjava
rxjava was among one the first libraries to develop reactive applications in java. It was
born at netflix as a port of the reactive extensions (rx) project, originally developed
by microsoft in the. Net environment. Rxjava version 2.0 was adjusted to adhere to the
reactive streams api explained earlier in this chapter and adopted by java 9 as java
. Util. Concurrent. Flow.
432 chapter 17 reactive programming

when you use an external library in java, this fact is apparent from the imports.
You import the java flow interfaces, for example, including publisher with a line
such as this:

import java. Lang. Concurrent. Flow. *;

but you also need to import the appropriate implementing classes with a line such as

import io. Reactivex. Observable;

if you want to use the observable implementation of publisher, as youll choose to
do later in this chapter.
We must emphasize one architectural issue: good systems-architectural style avoids
making visible throughout the system any fine-details concepts that are used in only one
part of the system. Accordingly, its good practice to use an observable only where the
additional structure of an observable is required and otherwise use its interface of
publisher. Note that you observe this guideline with the list interface without think-
ing. Even though a method may have been passed a value that you know to be an
arraylist, you declare the parameter for this value to be of type list, so you avoid
exposing and constraining the implementation details. Indeed, you allow a later change
of implementation from arraylist to linkedlist not to require ubiquitous changes.
In the rest of this section, you define a temperature-reporting system by using
rxjavas implementation of reactive streams. The first issue you come across is that
rxjava provides two classes, both of which implement flow. Publisher.
On reading the rxjava documentation, you find that one class is the io. Reactivex
. Flowable class, which includes the reactive pull-based backpressure feature of java 9
flow (using request) exemplified in listings 17.7 and 17.9. Backpressure prevents a
subscriber from being overrun by data being produced by a fast publisher. The
other class is the original rxjava io. Reactivex. Observable version of publisher,
which didnt support backpressure. This class is both simpler to program and more
appropriate for user-interface events (such as mouse movements); these events are
streams that cant be reasonably backpressured. (you cant ask the user to slow down
or stop moving the mouse! ) for this reason, rxjava provides these two implementing
classes for the common idea stream of events.
The rxjava advice is to use the nonbackpressured observable when you have a
stream of no more than a thousand elements or when youre are dealing with gui
events such as mouse moves or touch events, which are impossible to backpressure
and arent frequent anyway.
Because we analyzed the backpressure scenario while discussing the flow api in
the preceding section, we wont discuss flowable anymore; instead, well demonstrate
the observable interface at work in a use case without backpressure. Its worth noting
that any subscriber can effectively turn off backpressuring by invoking request(long
. Max_value)on the subscription, even if this practice isnt advisable unless youre
using the reactive library rxjava 433

sure that the subscriber will always be able to process all the received events in a
timely manner.

17.3.1 creating and using an observable
the observable and flowable classes come with many convenient factory methods that
allow you to create many types of reactive streams. (both observable and flowable
implement publisher, so these factory methods publish reactive streams. )
the simplest observable that you may want to create is made of a fixed number of
predetermined elements, as follows:
observable<string> strings = observable. Just( "first", "second" );

here, the just() factory method2 converts one or more elements to an observable
that emits those elements. A subscriber to this observable receives onnext("first"),
onnext("second"), and oncomplete() messages, in that order.
Another observable factory method thats quite common, especially when your
application interacts with a user in real time, emits events at a fixed time rate:
observable<long> onepersec = observable. Interval(1, timeunit. Seconds);

the interval factory method returns an observable, named onepersec, that emits an
infinite sequence of ascending values of type long, starting at zero, at a fixed time interval
of your choosing (1 second in this example). Now plan to use onepersec as the basis of
another observable that emits the temperature reported for a given town each second.
As an intermediate step toward this goal, you can print those temperatures each
second. To do so, you need to subscribe to onepersec to be notified by it every time a
second has passed and then fetch and print the temperatures of the town of interest.
In rxjava, the observable3 plays the role of the publisher in the flow api, so the
observer similarly corresponds to flows subscriber interface. The rxjava observer
interface declares the same methods as the java 9 subscriber given in listing 17.2,
with the difference that the onsubscribe method has a disposable argument rather
than a subscription. As we mentioned earlier, observable doesnt support backpres-
sure, so it doesnt have a request method that forms part of a subscription. The full
observer interface is
public interface observer<t> {
void onsubscribe(disposable d);
void onnext(t t);
void onerror(throwable t);
void oncomplete();
}

2
this naming convention is slightly unfortunate, because java 8 started using of() for similar factory methods
as popularized by the stream and optional apis.
3
note that the observer interface and the observable class have been deprecated since java 9. New code
should use the flow api. It remains to be seen how rxjava will evolve.
434 chapter 17 reactive programming

note, however, that rxjavas api are more flexible (have more overloaded variants)
than the native java 9 flow api. You can subscribe to an observable, for example, by
passing a lambda expression with the signature of the onnext method and omitting
the other three methods. In other words, you can subscribe to an observable with an
observer that implements only the onnext method with a consumer of the received
event, leaving the other methods defaulting to a no-op for completion and error han-
dling. By using this feature, you can subscribe to the observable onepersec and use it
to print the temperatures in new york once a second, all in a single line of code:

onepersec. Subscribe(i -> system. Out. Println(tempinfo. Fetch( "new york" )));

in this statement, the onepersec observable emits one event per second. And on
receipt of this message, the subscriber fetches the temperature in new york and
prints it. If you put this statement in a main method and try to execute it, however, you
see nothing because the observable publishing one event per second is executed in a
thread that belongs to rxjavas computation thread pool, which is made up of dae-
mon threads.4 but your main program terminates immediately and, in doing so, kills
the daemon thread before it can produce any output.
As a bit of a hack, you can prevent this immediate termination by putting a thread
sleep after the preceding statement. Better, you could use the blockingsubscribe
method that calls the callbacks on the current thread (in this case, the main thread).
For the purposes of a running demonstration, blockingsubscribe is perfectly suit-
able. In a production context, however, you normally use the subscribe method,
as follows:
onepersec. Blockingsubscribe(
i -> system. Out. Println(tempinfo. Fetch( "new york" ))
);

you may obtain output such as the following:
new york : 87
new york : 18
new york : 75
java. Lang. Runtimeexception: error!
At flow. Common. Tempinfo. Fetch(tempinfo. Java:18)
at flow. Main. Lambda$main$0(main. Java:12)
at io. Reactivex. Internal. Observers. Lambdaobserver
. Onnext(lambdaobserver. Java:59)
at io. Reactivex. Internal. Operators. Observable
. Observableinterval$intervalobserver. Run(observableinterval. Java:74)

unfortunately, the temperature fetching may, by design, fail randomly (and indeed
does after three readings). Because your observer implements only the happy path

4
this fact doesnt seem to be clear from the documentation, although you can find statements to this effect in
the stackoverflow. Com online developer community.
Using the reactive library rxjava 435

and doesnt have any sort of error management, such as onerror, this failure blows up
in the users face as an uncaught exception.
Its time to raise the bar and start complicating this example a bit. You dont want
to add only error management. You also have to generalize what you have. You dont
want to print the temperatures immediately but provide users a factory method that
returns an observable emitting those temperatures once a second for (say) at most
five times before completing. You can achieve this goal easily by using a factory
method named create that creates an observable from a lambda, taking as an argu-
ment another observer and returning void, as shown in the following listing.

Listing 17.12 creating an observable emitting temperature once a second

creates an observable from an observable emitting an
a function consuming an infinite sequence of ascending
observer longs, one per second
public static observable<tempinfo> gettemperature(string town) {
return observable. Create(emitter ->
observable. Interval(1, timeunit. Seconds)
. Subscribe(i -> {
if (! Emitter. Isdisposed()) {
do something only if
if ( i >= 5 ) { the consumed observer
if the temperature has been
already emitted five times, emitter. Oncomplete(); hasnt been disposed yet
completes the observer } else { (for a former error).
Terminating the stream try {
emitter. Onnext(tempinfo. Fetch(town));
in case of } catch (exception e) {
error, notifies emitter. Onerror(e); otherwise, sends a
the observer } temperature report
} to the observer
}}));
}

here, youre creating the returned observable from a function that consumes an
observableemitter, sending the desired events to it. The rxjava observableemitter
interface extends the basic rxjava emitter, which you can think of as being an
observer without the onsubscribe method,
public interface emitter<t> {
void onnext(t t);
void onerror(throwable t);
void oncomplete();
}

with a few more methods to set a new disposable on the emitter and check whether
the sequence has been already disposed downstream.
Internally, you subscribe to an observable such as onepersec that publishes an
infinite sequence of ascending longs, one per second. Inside the subscribing function
(passed as an argument to the subscribe method), you first check whether the con-
sumed observer has been already disposed by the isdisposed method provided by
436 chapter 17 reactive programming

the observableemitter interface. (this situation could happen if an error occurred
in an earlier iteration. ) if the temperature has been already emitted five times, the
code completes the observer, terminating the stream; otherwise, it sends the most
recent temperature report for the requested town to the observer in a try/catch
block. If an error occurs during the temperature fetching, it propagates the error to
the observer.
Now its easy to implement a complete observer that will later be used to subscribe
to the observable returned by the gettemperature method and that prints the tem-
peratures it publishes as shown in the next listing.

Listing 17.13 an observer printing the received temperatures

import io. Reactivex. Observer;
import io. Reactivex. Disposables. Disposable;

public class tempobserver implements observer<tempinfo> {
@override
public void oncomplete() {
system. Out. Println( "done! " );
}

@override
public void onerror( throwable throwable ) {
system. Out. Println( "got problem: " + throwable. Getmessage() );
}

@override
public void onsubscribe( disposable disposable ) {
}

@override
public void onnext( tempinfo tempinfo ) {
system. Out. Println( tempinfo );
}
}

this observer is similar to the tempsubscriber class from listing 17.7 (which imple-
ments java 9s flow. Subscriber), but you have a further simplification. Because
rxjavas observable doesnt support backpressure, you dont need to request() fur-
ther elements after processing the published ones.
In the next listing, you create a main program in which you subscribe this observer
to the observable returned by the gettemperature method from listing 17.12.

Listing 17.14 a main class printing the temperatures in new york

creates an observable emitting
public class main { the temperatures reported in
new york once a second
public static void main(string[] args) {
observable<tempinfo> observable = gettemperature( "new york" );
using the reactive library rxjava 437

observable. Blockingsubscribe( new tempobserver() );

subscribes to that observable
} with a simple observer that
} prints the temperatures

supposing that this time, no error occurs while the temperatures are being fetched,
main prints a line per second five times, and then the observable emits the oncomplete
signal, so you might obtain output like the following:

new york : 69
new york : 26
new york : 85
new york : 94
new york : 29
done!

Its time to enrich your rxjava example a bit further and in particular to see how this
library allows you to manipulate one or more reactive streams.

17.3.2 transforming and combining observables
one of the main advantages of rxjava and other reactive libraries in working with
reactive streams, compared with whats offered by the native java 9 flow api, is that
they provide a rich toolbox of functions to combine, create, and filter any of those
streams. As we demonstrated in the preceding sections, a stream can be used as an
input to another one. Also, youve learned about the java 9 flow. Processor used in
section 17.2.3 to transform temperatures in fahrenheit to celsius. But you can also fil-
ter a stream to get another one that has only the elements youre interested in, trans-
form those elements with a given mapping function (both these things can be
achieved with flow. Processor), or even merge or combine two streams in many ways
(which cant be achieved with flow. Processor).
These transforming and combining functions can be quite sophisticated, to the
point that explaining their behavior in plain words may result in awkward, convoluted
sentences. To get an idea, see how rxjava documents its mergedelayerror function:

flattens an observable that emits observables into one observable, in a way that allows
an observer to receive all successfully emitted items from all of the source observables
without being interrupted by an error notification from one of them, while limiting the
number of concurrent subscriptions to these observables.

You must admit that what this function does isnt immediately evident. To alleviate this
problem, the reactive-streams community decided to document the behaviors of these
functions in a visual way, using so-called marble diagrams. A marble diagram, such as
that shown in figure 17.4, represents the temporally ordered sequence of elements in
a reactive stream as geometric shapes on a horizontal line; special symbols represent
438 chapter 17 reactive programming

error and completion signals. Boxes indicate how named operators transform those
elements or combine multiple streams.

This is the timeline of the these are items emitted this vertical line indicates
observable. Time flows by the observable. That the observable has
from left to right. Completed successfully.

|
these dotted lines and
this box indicate that a
transformation is being
flip applied to the observable.
The text inside the box
shows the nature of the
transformation.
X
this observable is if for some reason the observable
the result of the terminates abnormally, with an error, the
transformation. Vertical line is replaced by an x.

Figure 17.4 legend of a marble diagram documenting an operator provided by a typical reactive
library

using this notation, its easy to provide a visual representation of the features of all the
rxjava librarys functions as shown in figure 17.5, which exemplifies map (which trans-
forms the elements published by an observable) and merge (which combines the
events emitted by two or more observables into one).

X
| |

map { } merge

| x
figure 17.5 the marble diagrams for the map and merge functions

you may wonder how you can use map and merge to improve and add features to the
rxjava example that you developed in the preceding section. Using map is a more con-
cise way to achieve the transformation from fahrenheit to celsius that you imple-
mented by using the flow apis processor, as shown in the following listing.
Using the reactive library rxjava 439

listing 17.15 using map on observable to transform fahrenheit into celsius

public static observable<tempinfo> getcelsiustemperature(string town) {
return gettemperature( town )
. Map( temp -> new tempinfo( temp. Gettown(),
(temp. Gettemp() - 32) * 5 / 9) );
}

this simple method takes the observable returned by the gettemperature method of
listing 17.12 and returns another observable that re-emits the temperatures published
(once a second) by the first one after transforming them from fahrenheit to celsius.
To reinforce your understanding of how you can manipulate the elements emitted
by an observable, try to use another transforming function in the following quiz.

Quiz 17.2: filtering only negative temperatures
the filter method of the observable class takes a predicate as an argument and
produces a second observable that emits only the elements that pass the test
defined by that predicate. Suppose that youve been asked to develop a warning
system that alerts the user when theres risk of ice. How can you use this operator
to create an observable that emits the temperature in celsius registered in a town
only in case the temperature is below zero? (the celsius scale conveniently uses zero
for the freezing point of water. )
answer:
its enough to take the observable returned by the method in listing 17.15 and apply
to it the filter operator with a predicate that accepts only negative temperature
as follows:
public static observable<tempinfo> getnegativetemperature(string town) {
return getcelsiustemperature( town )
. Filter( temp -> temp. Gettemp() < 0 );
}

now also imagine that youve been asked to generalize this last method and allow your
users to have an observable that emits the temperatures not only for a single town,
but also for a set of towns. Listing 17.16 satisfies the last requirement by invoking the
method in listing 17.15 once for each town and combining all the observables
obtained from these invocations into a single one through the merge function.

Listing 17.16 merging the temperatures reported for one or more towns

public static observable<tempinfo> getcelsiustemperatures(string... Towns) {
return observable. Merge(arrays. Stream(towns)
. Map(tempobservable: : getcelsiustemperature)
. Collect(tolist()));
}
440 chapter 17 reactive programming

this method takes a varargs argument containing the set of towns for which you
want temperatures. This varargs is converted to a stream of string; then each
string is passed to the getcelsiustemperature method of listing 17.11 (improved
in listing 17.15). This way, each town is transformed into an observable emitting the
temperature of that town each second. Finally, the stream of observables is collected
into a list, and the list is passed to the merge static factory method provided by the
observable class itself. This method takes an iterable of observables and combines
their output so that they act like a single observable. In other words, the resulting
observable emits all the events published by all the observables contained in the
passed iterable, preserving their temporal order.
To test this method, use it in one final main class as shown in the following listing.

Listing 17.17 a main class printing the temperatures in three towns

public class main {

public static void main(string[] args) {
observable<tempinfo> observable = getcelsiustemperatures(
"new york", "chicago", "san francisco" );
observable. Blockingsubscribe( new tempobserver() );
}
}

this main class is identical to the one in listing 17.14 except that youre now sub-
scribing to the observable returned by the getcelsiustemperatures method in
listing 17.16 and printing the temperatures registered for three towns. Running this
main produces output such as this:
new york : 21
chicago : 6
san francisco : -15
new york : -3
chicago : 12
san francisco : 5
got problem: error!

Each second, main prints the temperature of each requested town until one of the
temperature-fetching operations raises an error thats propagated to the observer,
interrupting the stream of data.
The purpose of this chapter wasnt to provide a complete overview of rxjava (or
any other reactive library), for which a complete book would be necessary, but to give
you a feeling for how this kind of toolkit works and to introduce you to the principles
of reactive programming. Weve merely scratched the surface of this programming
style, but we hope that weve demonstrated some of its advantages and stimulated your
curiosity about it.
Summary 441

summary
 the fundamental ideas behind reactive programming are 20 to 30 years old,
but have become popular recently because of the high demands of modern
applications in terms of amount of processed data and users expectations.
 These ideas have been formalized by the reactive manifesto, which states that
reactive software must be characterized by four interrelated features: respon-
siveness, resiliency, elasticity, and that quality of being message-driven.
 The principles of reactive programming can be applied, with some differences,
in implementing a single application and in designing a reactive system that
integrates multiple applications.
 A reactive application is based on asynchronous processing of one or more
flows of events conveyed by reactive streams. Because the role of reactive
streams is so central in development of reactive applications, a consortium of
companies including netflix, pivotal, lightbend, and red hat standardized the
concepts to maximize the interoperability of different implementations.
 Because reactive streams are processed asynchronously, theyve been designed
with a built-in backpressure mechanism. This prevents a slow consumer from
being overwhelmed by faster producers.
 The result of this design and standardization process has been incorporated into
java. The java 9 flow api defines four core interfaces: publisher, subscriber,
subscription, and processor.
 These interfaces arent intended, in most cases, to be implemented directly by
developers, but to act as a lingua franca for the various libraries that implement
the reactive paradigm.
 One of the most commonly used of these toolkits is rxjava, which (in addition
to the basic features defined by the java 9 flow api) provides many useful, pow-
erful operators. Examples include operators that conveniently transform and
filter the elements published by a single reactive stream and operators that com-
bine and aggregate multiple streams.
Part 6

functional programming
and future java evolution

i n the final part of this book, we draw back a little with a tutorial introduction
to writing effective functional-style programs in java, along with a comparison of
java 8 features with those of scala.
Chapter 18 gives a full tutorial on functional programming, introduces some
of its terminology, and explains how to write functional-style programs in java.
Chapter 19 covers more advanced functional programming techniques includ-
ing higher-order functions, currying, persistent data structures, lazy lists, and
pattern matching. You can view this chapter as a mix of practical techniques to
apply in your codebase as well as academic information that will make you a
more knowledgeable programmer.
Chapter 20 follows by discussing how java 8 features compare to features in the
scala languagea language that, like java, is implemented on top of the jvm
and that has evolved quickly to threaten some aspects of javas niche in the pro-
gramming language ecosystem.
Finally, chapter 21 reviews the journey of learning about java 8 and the gen-
tle push toward functional-style programming. In addition, we speculate on what
future enhancements and great new features may be in javas pipeline beyond
java 8 and java 9.
Thinking functionally

this chapter covers
 why functional programming?
 What defines functional programming?
 Declarative programming and referential
transparency
 guidelines for writing functional-style java
 iteration versus recursion

youve seen the term functional quite frequently throughout this book. By now, you
may have some ideas about what being functional entails. Is it about lambdas and
first-class functions or about restricting your right to mutate objects? What do you
achieve from adopting a functional style?
In this chapter, we shed light on the answers to these questions. We explain what
functional programming is and introduce some of its terminology. First, we exam-
ine the concepts behind functional programmingsuch as side effects, immutabil-
ity, declarative programming, and referential transparencyand then we relate
these concepts to java 8. In chapter 19, we look more closely at functional program-
ming techniques such as higher-order functions, currying, persistent data struc-
tures, lazy lists, pattern matching, and combinators.

445
446 chapter 18 thinking functionally

18.1 implementing and maintaining systems
to start, imagine that youve been asked to manage an upgrade of a large software sys-
tem that you havent seen. Should you accept the job of maintaining such a software
system? A seasoned java contractors only slightly tongue-in-cheek maxim for deciding
is start by searching for the keyword synchronized; if you find it, just say no (reflect-
ing the difficulty of fixing concurrency bugs). Otherwise, consider the structure of the
system in more detail. We provide more detail in the following paragraphs. First,
however, well note that as youve seen in previous chapters, java 8s addition of
streams allows you to exploit parallelism without worrying about locking, provided
that you embrace stateless behaviors. (that is, functions in your stream-processing
pipeline dont interact, with one function reading from or writing to a variable thats
written by another. )
what else might you want the program to look like so that its easy to work with?
Youd want it to be well structured, with an understandable class hierarchy reflecting
the structure of the system. You have ways to estimate such a structure by using the
software engineering metrics coupling (how interdependent parts of the system are)
and cohesion (how related the various parts of the system are).
But for many programmers, the key day-to-day concern is debugging during main-
tenance: some code crashed because it observed an unexpected value. But which
parts of the program were involved in creating and modifying this value? Think of
how many of your maintenance concerns fall into this category!1 it turns out that the
concepts of no side effects and immutability, which functional programming promotes,
can help. We examine these concepts in more detail in the following sections.

18.1.1 shared mutable data
ultimately, the reason for the unexpected-variable-value problem discussed in the pre-
ceding section is that shared mutable data structures are read and updated by more
than one of the methods on which your maintenance centers. Suppose that several
classes keep a reference to a list. As a maintainer, you need to establish answers to the
following questions:
 who owns this list?
 What happens if one class modifies the list?
 Do other classes expect this change?
 How do those classes learn about this change?
 Do the classes need to be notified of this change to satisfy all assumptions in this
list, or should they make defensive copies for themselves?
In other words, shared mutable data structures make it harder to track changes in dif-
ferent parts of your program. Figure 18.1 illustrates this idea.

1
we recommend reading working effectively with legacy code, by michael feathers (prentice hall, 2004), for fur-
ther information on this topic.
Implementing and maintaining systems 447

which class
owns the list? List

read write read/write

class a class b class c

figure 18.1 a mutable shared across multiple classes. Its difficult to
understand what owns the list.

Consider a system that doesnt mutate any data structures. This system would be a
dream to maintain because you wouldnt have any bad surprises about some object
somewhere that unexpectedly modifies a data structure. A method that modifies nei-
ther the state of its enclosing class nor the state of any other objects and returns its
entire results by using return is called pure or side-effect-free.
What constitutes a side effect? In a nutshell, a side effect is an action thats not totally
enclosed within the function itself. Here are some examples:
 modifying a data structure in place, including assigning to any field, apart from
initialization inside a constructor (such as setter methods)
 throwing an exception
 performing i/o operations such as writing to a file

another way to look at the idea of no side effects is to consider immutable objects. An
immutable object is an object that cant change its state after its instantiated, so it cant
be affected by the actions of a function. When immutable objects are instantiated,
they can never go into an unexpected state. You can share them without having to
copy them, and theyre thread-safe because they cant be modified.
The idea of no side effects may appear to be a severe restriction, and you may
doubt whether real systems can be built this way. We hope to persuade you that they
can be built by the end of the chapter. The good news is that components of systems
that embrace this idea can use multicore parallelism without using locking, because
the methods can no longer interfere with one another. In addition, this concept is
great for immediately understanding which parts of the program are independent.
These ideas come from functional programming, to which we turn in the next section.

18.1.2 declarative programming
first, we explore the idea of declarative programming, on which functional program-
ming is based.
448 chapter 18 thinking functionally

there are two ways of thinking about implementing a system by writing a program.
One way centers on how things are done. (first do this, then update that, and so on. )
if you want to calculate the most expensive transaction in a list, for example, you typi-
cally execute a sequence of commands. (take a transaction from the list and compare
it with the provisional most expensive transaction; if its more expensive, it becomes
the provisional most expensive; repeat with the next transaction in the list, and so on. )
this how style of programming is an excellent match for classic object-oriented
programming (sometimes called imperative programming), because it has instructions
that mimic the low-level vocabulary of a computer (such as assignment, conditional
branching, and loops), as shown in this code:
transaction mostexpensive = transactions. Get(0);
if(mostexpensive == null)
throw new illegalargumentexception("empty list of transactions");
for(transaction t: transactions. Sublist(1, transactions. Size())){
if(t. Getvalue() > mostexpensive. Getvalue()){
mostexpensive = t;
}
}

the other way centers on whats to be done. You saw in chapters 4 and 5 that by using
the streams api, you could specify this query as follows:
optional<transaction> mostexpensive =
transactions. Stream()
. Max(comparing(transaction: : getvalue));

the fine detail of how this query is implemented is left to the library. We refer to this
idea as internal iteration. The great advantage is that your query reads like the problem
statement, and for that reason, its immediately clear, compared with trying to under-
stand what a sequence of commands does.
This what style is often called declarative programming. You provide rules saying
what you want, and you expect the system to decide how to achieve that goal. This
type of programming is great because it reads closer to the problem statement.

18.1.3 why functional programming?
Functional programming exemplifies this idea of declarative programming (say what
you want using expressions that dont interact, and for which the system can choose
the implementation) and side-effect-free computation, explained earlier in this chap-
ter. These two ideas can help you implement and maintain systems more easily.
Note that certain language features, such as composing operations and passing
behaviors (which we presented in chapter 3 by using lambda expressions), are
required to read and write code in a natural way with a declarative style. Using
streams, you can chain several operations to express a complicated query. These fea-
tures characterize functional programming languages. We look at these features more
carefully under the guise of combinators in chapter 19.
Whats functional programming? 449

to make the discussion tangible and connect it with the new features in java 8, in
the next section we concretely define the idea of functional programming and its rep-
resentation in java. Wed like to impart the fact that by using functional-programming
style, you can write serious programs without relying on side effects.

18.2 whats functional programming?
The oversimplistic answer to what is functional programming? Is programming
with functions. Whats a function?
Its easy to imagine a method taking an int and a double as arguments and pro-
ducing a doubleand also having the side effect of counting the number of times it
has been called by updating a mutable variable, as illustrated in figure 18.2.

Update a field of
another object

input method output

update field in figure 18.2 a function with
this object side effects

in the context of functional programming, however, a function corresponds to a math-
ematical function: it takes zero or more arguments, returns one or more results, and
has no side effects. You can see a function as being a black box that takes some inputs
and produces some outputs, as illustrated in figure 18.3.

Input function output figure 18.3 a function with
no side effects

the distinction between this sort of function and the methods you see in program-
ming languages such as java is central. (the idea that mathematical functions such as
log or sin might have such side effects in unthinkable. ) in particular, mathematical
functions always return the same results when theyre called repeatedly with the same
arguments. This characterization rules out methods such as random. Nextint, and we
further discuss this concept of referential transparency in section 18.2.2.
450 chapter 18 thinking functionally

when we say functional, we mean like mathematics, with no side effects. Now a
programming subtlety appears. Do we mean either: is every function built only with
functions and mathematical ideas such as if-then-else? Or might a function do
nonfunctional things internally as long as it doesnt expose any of these side effects to
the rest of the system? In other words, if programmers perform a side effect that cant
be observed by callers, does that side effect exist? The callers dont need to know or
care, because it cant affect them.
To emphasize the difference, we refer to the former as pure functional program-
ming and the latter as functional-style programming.

18.2.1 functional-style java
in practice, you cant completely program in pure functional style in java. Javas i/o
model consists of side-effecting methods, for example. (calling scanner. Nextline has
the side effect of consuming a line from a file, so calling it twice typically produces differ-
ent results. ) nonetheless, its possible to write core components of your system as though
they were purely functional. In java, youre going to write functional-style programs.
First, theres a further subtlety about no one seeing your side effects and, hence, in
the meaning of functional. Suppose that a function or method has no side effects
except for incrementing a field after entry and decrementing it before exit. From the
point of view of a program that consists of a single thread, this method has no visible
side effects and can be regarded as functional style. On the other hand, if another
thread could inspect the fieldor could call the method concurrentlythe method
wouldnt be functional. You could hide this issue by wrapping the body of this method
with a lock, which would enable you to argue that the method is functional. But in
doing so, youd lose the ability to execute two calls to the method in parallel by using
two cores on your multicore processor. Your side effect may not be visible to a pro-
gram, but its visible to the programmer in terms of slower execution.
Our guideline is that to be regarded as functional style, a function or method can
mutate only local variables. In addition, the objects that it references should be
immutablethat is, all fields are final, and all fields of reference type refer transi-
tively to other immutable objects. Later, you may permit updates to fields of objects
that are freshly created in the method, so they arent visible from elsewhere and arent
saved to affect the result of a subsequent call.
Our guideline is incomplete, however. Theres an additional requirement to being
functional, that a function or method shouldnt throw any exceptions. A justification is
that throwing an exception would mean that a result is being signaled other than via
the function returning a value; see the black-box model of figure 18.2. Theres scope
for debate here, with some authors arguing that uncaught exceptions representing
fatal errors are okay and that its the act of catching an exception that represents non-
functional control flow. Such use of exceptions still breaks the simple pass argu-
ments, return result metaphor pictured in the black-box model, however, leading to
a third arrow representing an exception, as illustrated in figure 18.4.
Whats functional programming? 451

input function output

exception

figure 18.4 a function throwing an exception

functions and partial functions
in mathematics, a function is required to give exactly one result for each possible
argument value. But many common mathematical operations are what should prop-
erly be called partial functions. That is, for some or most input values, they give
exactly one result, but for other input values, theyre undefined and dont give a result
at all. An example is division when the second operand is zero or sqrt when its argu-
ment is negative. We often model these situations in java by throwing an exception.

How might you express functions such as division without using exceptions? Use types
like optional<t>. Instead of having the signature double sqrt(double) but may raise
an exception, sqrt would have the signature optional<double> sqrt(double). Either
it returns a value that represents success, or it indicates in its return value that it couldnt
perform the requested operation. And yes, doing so does mean that the caller needs to
check whether each method call may result in an empty optional. This may sound like
a huge deal, but pragmatically, given our guidance on functional-style programming
versus pure functional programming, you may choose to use exceptions locally but not
expose them via large-scale interfaces, thereby gaining the advantages of functional style
without the risk of code bloat.
To be regarded as functional, your function or method should call only those side-
effecting library functions for which you can hide nonfunctional behavior (that is,
ensuring that any mutations they make in data structures are hidden from your caller,
perhaps by copying first and by catching any exceptions). In section 18.2.4, you hide
the use of a side-effecting library function list. Add inside a method insertall by
copying the list.
You can often mark these prescriptions by using comments or declaring a method
with a marker annotationand match the restrictions you placed on functions passed
to parallel stream-processing operations such as stream. Map in chapters 47.
Finally, for pragmatic reasons, you may find it convenient for functional-style code
to be able to output debugging information to some form of log file. This code cant
be strictly described as functional, but in practice, you retain most of the benefits of
functional-style programming.
452 chapter 18 thinking functionally

18.2.2 referential transparency
the restrictions on no visible side effects (no mutating structure visible to callers, no
i/o, no exceptions) encode the concept of referential transparency. A function is ref-
erentially transparent if it always returns the same result value when its called with the
same argument value. The method string. Replace, for example, is referentially
transparent because "raoul". Replace('r', 'r') always produces the same result
(replace returns a new string with all lowercase rs replaced with uppercase rs)
rather than updating its this object, so it can be considered to be a function.
Put another way, a function consistently produces the same result given the same
input, no matter where and when its invoked. It also explains why random. Nextint isnt
regarded as being functional. In java, using a scanner object to get the input from a
users keyboard violates referential transparency because calling the method nextline
may produce a different result at each call. But adding two final int variables always
produces the same result because the content of the variables can never change.
Referential transparency is a great property for program understanding. It also
encompasses save-instead-of-recompute optimization for expensive or long-lived oper-
ations, a process that goes by the name memoization or caching. Although important,
this topic is a slight tangent here, so we discuss it in chapter 19.
Java has one slight complication with regard referential transparency. Suppose that
you make two calls to a method that returns a list. The two calls may return refer-
ences to distinct lists in memory but containing the same elements. If these lists are to
be seen as mutable object-oriented values (and therefore nonidentical), the method
isnt referentially transparent. If you plan to use these lists as pure (immutable) values,
it makes sense to see the values as being equal, so the function is referentially trans-
parent. In general, in functional-style code, you choose to regard such functions as
being referentially transparent.
In the next section, we explore whether to mutate from a wider perspective.

18.2.3 object-oriented vs. Functional-style programming
we start by contrasting functional-style programming with (extreme) classical object-
oriented programming before observing that java 8 sees these styles as being mere
extremes on the object-oriented spectrum. As a java programmer, without consciously
thinking about it, you almost certainly use some aspects of functional-style program-
ming and some aspects of what well call extreme object-oriented programming. As we
remarked in chapter 1, changes in hardware (such as multicore) and programmer
expectation (such as database-like queries to manipulate data) are pushing java soft-
ware-engineering styles more to the functional end of this spectrum, and one of the
aims of this book is to help you adapt to the changing climate.
At one end of the spectrum is the extreme object-oriented view: everything is an
object, and programs operate by updating fields and calling methods that update their
associated object. At the other end of the spectrum lies the referentially transparent
functional-programming style of no (visible) mutation. In practice, java programmers
whats functional programming? 453

have always mixed these styles. You might traverse a data structure by using an iterator
containing mutable internal state but use it to calculate, say, the sum of values in the
data structure in a functional-style manner. (in java, as discussed earlier, this process
can include mutating local variables. ) one of the aims of this chapter and chapter 19
is to discuss programming techniques and introduce features from functional pro-
gramming to enable you to write programs that are more modular and more suitable
for multicore processors. Think of these ideas as being additional weapons in your
programming armory.

18.2.4 functional style in practice
to start, solve a programming exercise given to beginning students that exemplifies
functional style: given a list<integer> value, such as {1, 4, 9}, construct a list<list
<integer>> value whose members are all the subsets of {1, 4, 9}, in any order. The sub-
sets of {1, 4, 9} are {1, 4, 9}, {1, 4}, {1, 9}, {4, 9}, {1}, {4}, {9}, and {}.
There are eight subsets, including the empty subset, written {}. Each subset is rep-
resented as type list<integer>, which means that the answer is of type list<list
<integer>>.
Students often have problems thinking how to start and need prompting2 with the
remark the subsets of {1, 4, 9} either contain 1 or do not. The ones that dont are
subsets of {4, 9}, and the ones that do can be obtained by taking the subsets of {4, 9}
and inserting 1 into each of them. Theres one subtlety though: we must remember
that the empty set has exactly one subsetitself. This understanding gives you an easy,
natural, top-down, functional-programming-style encoding in java as follows:3

if the input list is empty,
static list<list<integer>> subsets(list<integer> list) { it has one subset: the
if (list. Isempty()) { empty list itself.
List<list<integer>> ans = new arraylist<>();
ans. Add(collections. Emptylist());
return ans; otherwise take out one
} element, fst, and find all
integer fst = list. Get(0); subsets of the rest to
then list<integer> rest = list. Sublist(1, list. Size()); give subans; subans
concatenate list<list<integer>> subans = subsets(rest); forms half the answer.
The two list<list<integer>> subans2 = insertall(fst, subans);
subanswers. Return concat(subans, subans2);
}
the other half of the answer, subans2, consists
of all the lists in subans but adjusted by
prefixing each of these element lists with fst.

2
troublesome (bright! ) students occasionally point out a neat coding trick involving binary representation of
numbers. (the java solution code corresponds to 000,001,010,011,100,101,110,111. ) we tell such students to
calculate instead the list of all permutations of a list; for the example {1, 4, 9}, there are six.
3
for concreteness, the code we give here uses list<integer>, but you can replace it in the method defini-
tions with generic list<t>; then you could apply the updated subsets method to list<string> as well as
list<integer>.
454 chapter 18 thinking functionally

the solution program produces {{}, {9}, {4}, {4, 9}, {1}, {1, 9}, {1, 4}, {1, 4, 9}} when given
{1, 4, 9} as input. Do try it when youve defined the two missing methods.
To review, youve assumed that the missing methods insertall and concat are
themselves functional and deduced that your function subsets is also functional,
because no operation in it mutates any existing structure. (if youre familiar with
mathematics, youll recognize this argument as being by induction. )
now look at defining insertall. Heres the first danger point. Suppose that you
defined insertall so that it mutated its arguments, perhaps by updating all the ele-
ments of subans to contain fst. Then the program would incorrectly cause subans to
be modified in the same way as subans2, resulting in an answer that mysteriously con-
tained eight copies of {1,4,9}. Instead, define insertall functionally as follows:
static list<list<integer>> insertall(integer fst,
list<list<integer>> lists) {
list<list<integer>> result = new arraylist<>();
for (list<integer> list : lists) { copy the list so you
list<integer> copylist = new arraylist<>(); can add to it. You
copylist. Add(fst); wouldnt copy the
copylist. Addall(list); lower-level structure
result. Add(copylist); even if it were
} mutable. (integers
return result;
arent mutuable. )
}

note that youre creating a new list that contains all the elements of subans. You
take advantage of the fact that an integer object is immutable; otherwise, youd have
to clone each element too. The focus caused by thinking of methods like insertall
as being functional gives you a natural place to put all this carefully copied code:
inside insertall rather in its callers.
Finally, you need to define the concat method. In this case, the solution is simple,
but we beg you not to use it; we show it only so that you can compare the different styles:
static list<list<integer>> concat(list<list<integer>> a,
list<list<integer>> b) {
a. Addall(b);
return a;
}

instead, we suggest that you write this code:
static list<list<integer>> concat(list<list<integer>> a,
list<list<integer>> b) {
list<list<integer>> r = new arraylist<>(a);
r. Addall(b);
return r;
}

why? The second version of concat is a pure function. The function may be using
mutation (adding elements to the list r) internally, but it returns a result based on its
recursion vs. Iteration 455

arguments and modifies neither of them. By contrast, the first version relies on the
fact that after the call concat(subans, subans2), no one refers to the value of subans
again. For our definition of subsets, this situation is the case, so surely using the
cheaper version of concat is better. The answer depends on how you value your time.
Compare the time youd spend later searching for obscure bugs compared with the
additional cost of making a copy.
No matter how well you comment that the impure concat is to be used only when
the first argument can be arbitrarily overwritten, and intended to be used only in the
subsets method, and any change to subsets must be reviewed in the light of this
comment, somebody sometime will find it useful in some piece of code where it
apparently seems to work. Your future nightmare debugging problem has been born.
We revisit this issue in chapter 19.
Takeaway point: thinking of programming problems in terms of function-style
methods that are characterized only by their input arguments and output results
(what to do) is often more productive than thinking about how to do it and what to
mutate too early in the design cycle.
In the next section, we discuss recursion in detail.

18.3 recursion vs. Iteration
recursion is a technique promoted in functional programming to let you think in
terms of what-to-do style. Pure functional programming languages typically dont
include iterative constructs such as while and for loops. Such constructs are often
hidden invitations to use mutation. The condition in a while loop needs to be
updated, for example; otherwise, the loop would execute zero times or an infinite
number of times. In many cases, however, loops are fine. Weve argued that for func-
tional style, youre allowed mutation if no one can see you doing it, so its acceptable
to mutate local variables. When you use the for-each loop in java, for(apple apple :
apples) { }, it decodes into this iterator:
iterator<apple> it = apples. Iterator();
while (it. Hasnext()) {
apple apple = it. Next();
// ...
}

this translation isnt a problem because the mutations (changing the state of the
iterator with the next method and assigning to the apple variable inside the while
body) arent visible to the caller of the method where the mutations happen. But
when you use a for-each loop, such as a search algorithm, for example, what fol-
lows is problematic because the loop body is updating a data structure thats shared
with the caller:
public void searchforgold(list<string> l, stats stats){
for(string s: l){
if("gold". Equals(s)){
456 chapter 18 thinking functionally

stats. Incrementfor("gold");
}
}
}

indeed, the body of the loop has a side effect that cant be dismissed as functional style: it
mutates the state of the stats object, which is shared with other parts of the program.
For this reason, pure functional programming languages such as haskell omit
such side-effecting operations. How are you to write programs? The theoretical
answer is that every program can be rewritten to prevent iteration by using recursion
instead, which doesnt require mutability. Using recursion lets you get rid of iteration
variables that are updated step by step. A classic school problem is calculating the fac-
torial function (for positive arguments) in an iterative way and in a recursive way
(assume that the input is > 0), as shown in the following two listings.

Listing 18.1 iterative factorial

static long factorialiterative(long n) {
long r = 1;
for (int i = 1; i <= n; i++) {
r *= i;
}
return r;
}

listing 18.2 recursive factorial
static long factorialrecursive(long n) {
return n == 1 ? 1 : n * factorialrecursive(n-1);
}

the first listing demonstrates a standard loop-based form: the variables r and i are
updated at each iteration. The second listing shows a recursive definition (the func-
tion calls itself) in a more mathematically familiar form. In java, recursive forms typi-
cally are less efficient, as we discuss immediately after the next example.
If youve read the earlier chapters of this book, however, you know that java 8
streams provide an even simpler declarative way of defining factorial, as shown in the
following listing.

Listing 18.3 stream factorial

static long factorialstreams(long n){
return longstream. Rangeclosed(1, n)
. Reduce(1, (long a, long b) -> a * b);
}

now well turn to efficiency. As java users, beware of functional-programming zealots
who tell you that you should always use recursion instead of iteration. In general,
making a recursive function call is much more expensive than issuing the single
recursion vs. Iteration 457

machine-level branch instruction needed to iterate. Every time the factorial-
recursive function is called, a new stack frame is created on the call stack to hold the
state of each function call (the multiplication it needs to do) until the recursion is
done. Your recursive definition of factorial takes memory proportional to its input.
For this reason, if you run factorialrecursive with a large input, youre likely to
receive a stackoverflowerror:

exception in thread "main" java. Lang. Stackoverflowerror

is recursion useless? Of course not! Functional languages provide an answer to this
problem: tail-call optimization. The basic idea is that you can write a recursive definition
of factorial in which the recursive call is the last thing that happens in the function (or
the call is in a tail position). This different form of recursion style can be optimized to
run fast. The next listing provides a tail-recursive definition of factorial.

Listing 18.4 tail-recursive factorial

static long factorialtailrecursive(long n) {
return factorialhelper(1, n);
}
static long factorialhelper(long acc, long n) {
return n == 1 ? Acc : factorialhelper(acc * n, n-1);
}

the function factorialhelper is tail-recursive because the recursive call is the last
thing that happens in the function. By contrast, in the earlier definition of factorial-
recursive, the last thing was a multiplication of n and the result of a recursive call.
This form of recursion is useful because instead of storing each intermediate result
of the recursion in separate stack frames, the compiler can decide to reuse a single
stack frame. Indeed, in the definition of factorialhelper, the intermediate results
(the partial results of the factorial) are passed directly as arguments to the function.
Theres no need to keep track of the intermediate result of each recursive call on a
separate stack frame; its accessible directly as the first argument of factorialhelper.
Figures 18.5 and 18.6 illustrate the difference between the recursive and tail-recursive
definitions of factorial.
The bad news is that java doesnt support this kind of optimization. But adopting
tail recursion may be a better practice than classic recursion because it opens the way
to eventual compiler optimization. Many modern jvm languages such as scala,
groovy, and kotlin can optimize those uses of recursion, which are equivalent to itera-
tion (and execute at the same speed). As a result, pure-functional adherents can have
their purity cake and eat it efficiently too.
The guidance in writing java 8 is that you can often replace iteration with streams
to avoid mutation. In addition, you can replace iteration with recursion when recur-
sion allows you write an algorithm in a more concise, side-effect-free way. Indeed,
458 chapter 18 thinking functionally

factorial(4) first call
24
4 * factorial(3)

factorial(3) second call
6
3 * factorial(2)

factorial(2) third call
2
2 * factorial(1)

factorial(1) fourth call
1

figure 18.5 recursive definition of factorial, which requires several stack frames

factorial(4)

factorialtailrecursive(4, 3)
factorialtailrecursive(12, 2)
factorialtailrecursive(1, 4) factorialtailrecursive(24, 1)
24

figure 18.6 tail-recursive definition of factorial, which can reuse a single stack frame

recursion can make examples easier to read, write, and understand (as in the subsets
example shown earlier in this chapter), and programmer efficiency is often more
important than small differences in execution time.
In this section, we discussed functional-style programming with the idea of a
method being functional; everything we said would have applied to the first version of
java. In chapter 19, we look at the amazing and powerful possibilities offered by the
introduction of first-class functions in java 8.
Summary 459

summary
 reducing shared mutable data structures can help you maintain and debug
your programs in the long term.
 Functional-style programming promotes side-effect-free methods and declara-
tive programming.
 Function-style methods are characterized only by their input arguments and
their output result.
 A function is referentially transparent if it always returns the same result value
when called with the same argument value. Iterative constructs such as while
loops can be replaced by recursion.
 Tail recursion may be a better practice than classic recursion in java because it
opens the way to potential compiler optimization.
Functional
programming techniques

this chapter covers
 first-class citizens, higher-order functions,
currying, and partial application
 persistent data structures
 lazy evaluation and lazy lists as generalizing java
streams
 pattern matching and how to simulate it in java
 referential transparency and caching

in chapter 18, you saw how to think functionally; thinking in terms of side-effect-free
methods can help you write more maintainable code. In this chapter, we introduce
more advanced functional programming techniques. You can view this chapter as
being a mix of practical techniques to apply in your code base, as well as academic
information that will make you a more knowledgeable programmer. We discuss
higher-order functions, currying, persistent data structures, lazy lists, pattern match-
ing, caching with referential transparency, and combinators.

460
functions everywhere 461

19.1 functions everywhere
in chapter 18 we used the phrase functional-style programming to mean that the behavior
of functions and methods should be like that of mathematical-style functions, with no
side effects. Functional-language programmers often use the phrase with more general-
ity to mean that functions may be used like other values: passed as arguments, returned
as results, and stored in data structures. Functions that may be used like other values are
referred to as first-class functions. First-class functions are what java 8 added over previous
versions of java: you may use any method as a function value, using the : : operator
to create a method reference, and lambda expressions (such as (int x) -> x + 1) to
express function values directly. In java 8 its perfectly valid1 to store the method
integer. Parseint in a variable by using a method reference as follows:

function<string, integer> strtoint = integer: : parseint;

19.1.1 higher-order functions
so far, youve mainly used the fact that function values are first-class only to pass them
to java 8 stream-processing operations (as in chapters 47) and to achieve the similar
effect of behavior parameterization when you passed apple: : isgreen-apple as a
function value to filterapples in chapters 1 and 2. Another interesting example was
using the static method comparator. Comparing, which takes a function as a parame-
ter and returns another function (a comparator), as illustrated in the following code
and figure 19.1:

function

comparing comparator figure 19.1 comparing takes a function as
parameter and returns another function.

Comparator<apple> c = comparing(apple: : getweight);

you did something similar when you composed functions in chapter 3 to create a
pipeline of operations:
function<string, string> transformationpipeline
= addheader. Andthen(letter: : checkspelling)
. Andthen(letter: : addfooter);

1
if integer: : parseint is the only method you plan to store in variable strtoint, you might want to make
strtoint have type tointfunction<string> to save boxing. You dont do so here because using such java
primitive applications can get in the way of seeing whats happening, even if those applications improve effi-
ciency for primitive types.
462 chapter 19 functional programming techniques

functions (such as comparator. Comparing) that can do at least one of the following
are called higher-order functions within the functional programming community:
 take one or more functions as a parameter
 return a function as a result

this characterization directly relates to java 8 functions because they can not only be
passed as arguments, but also returned as results, assigned to local variables, or even
inserted into structures. A pocket-calculator program might have a map<string,
function<double, double>> that maps the string "sin" to function<double, double>
to hold the method reference math: : sin. You did something similar when you learned
about the factory design pattern in chapter 8.
Readers who liked the calculus example at the end of chapter 3 can regard the
type of differentiation as being

function<function<double, double>, function<double, double>>

because it takes a function as an argument (such as (double x) -> x * x) and returns a
function as a result (in this example, (double x) -> 2 * x). Weve written this code as a
function type (the leftmost function) to explicitly affirm the fact that you could pass
this differentiating function to another function. But its good to recall that the type
for differentiating and the signature

function<double, double> differentiate(function<double, double> func)

say the same thing.

Side effects and higher-order functions
we noted in chapter 7 that functions passed to stream operations generally are side-
effect-free, and we noted the problems that arise otherwise (such as incorrect results
and even unpredictable results due to race conditions you hadnt thought of). This
principle also applies in general when you use higher-order functions. When youre
writing a higher-order function or method, you dont know in advance what arguments
will be passed to it and, if the arguments have side effects, what these side effects
might do. It becomes far too complicated to reason about what your code does if it
uses functions passed as arguments that make unpredictable changes in the state
of your program; such functions might even interfere with your code in some hard-to-
debug way. Its a good design principle to document what side effects youre willing
to accept from functions passed as parameters. None is best of all!

In the next section, we turn to currying: a technique that can help you modularize
functions and reuse code.
Functions everywhere 463

19.1.2 currying
before we give you the theoretical definition of currying, well present an example.
Applications almost always need to be internationalized, so converting from one set of
units to another set is a problem that comes up repeatedly.
Unit conversion always involves a conversion factor and, from time to time, a base-
line adjustment factor. The formula to convert celsius to fahrenheit, for example, is
ctof(x) = x*9/5 + 32. The basic pattern of all unit conversion is as follows:
1 multiply by the conversion factor.
2 adjust the baseline if relevant.
You can express this pattern with the following general method:
static double converter(double x, double f, double b) {
return x * f + b;
}

here, x is the quantity you want to convert, f is the conversion factor, and b is the
baseline. But this method is a bit too general. Typically, you require a lot of conver-
sions between the same pair of units, such as kilometers to miles. You could call the
converter method with three arguments on each occasion, but supplying the factor
and baseline each time would be tedious, and you might accidentally mistype them.
You could write a new method for each application, but doing so would miss the
reuse of the underlying logic.
Heres an easy way to benefit from the existing logic while tailoring the converter
for particular applications. You can define a factory that manufactures one-argument
conversion functions to exemplify the idea of currying:
static doubleunaryoperator curriedconverter(double f, double b){
return (double x) -> x * f + b;
}

now all you have to do is pass curriedconverter the conversion factor and baseline
(f and b), and it obligingly returns a function (of x) to do what you asked for. Then
you can use the factory to produce any converter you require, as follows:
doubleunaryoperator convertctof = curriedconverter(9.0/5, 32);
doubleunaryoperator convertusdtogbp = curriedconverter(0.6, 0);
doubleunaryoperator convertkmtomi = curriedconverter(0.6214, 0);

because doubleunaryoperator defines a method applyasdouble, you can use your
converters as follows:

double gbp = convertusdtogbp. Applyasdouble(1000);

as a result, your code is more flexible, and it reuses the existing conversion logic!
Reflect on what youre doing here. Instead of passing all the arguments x, f, and b
all at once to the converter method, you ask only for the arguments f and b and
464 chapter 19 functional programming techniques

return another functionwhich, when given an argument x, returns x * f + b. This
two-stage process enables you to reuse the conversion logic and create different func-
tions with different conversion factors.

Formal definition of currying
currying a is a technique in which a function f of two arguments (such as x and y) is
instead viewed as a function g of one argument that returns a function also of one
argument. The value returned by the latter function is the same as the value of the
original functionthat is, f(x, y) = (g(x))(y).
This definition generalizes, of course. You can curry a six-argument function to first
take arguments numbered 2, 4, and 6, which returns a function taking argument 5,
which returns a function taking the remaining arguments, 1 and 3.
When some arguments (but fewer than the full complement of arguments) have been
passed, the function is partially applied.
A
the word currying is unconnected to indian food; the term is named after the logician haskell
brooks curry, who popularized the technique. He attributed it to moses ilyich schonfinkel,
however. Should we refer to currying as schonfinkeling instead?

In the next section, we turn to another aspect of functional-style programming: data
structures. Is it possible to program with data structures if youre forbidden to mod-
ify them?

19.2 persistent data structures
data structures used in functional-style programs have various names, such as func-
tional data structures and immutable data structures, but perhaps the most common
is persistent data structures. (unfortunately, this terminology clashes with the notion
of persistent in databases, meaning outliving one run of the program. )
the first thing to notice is that a functional-style method isnt allowed to update
any global data structure or any structure passed as a parameter. Why? Because calling
it twice is likely to produce different answers, violating referential transparency and
the ability to understand the method as a simple mapping from arguments to results.

19.2.1 destructive updates vs. Functional
consider the problems that can arise. Suppose that you represent train journeys from
a to b as a mutable trainjourney class (a simple implementation of a singly linked
list), with an int field modeling some detail of the journey, such as the price of the
current leg of the journey. Journeys that require changing trains have several linked
trainjourney objects via the onward field; a direct train or the final leg of a journey
has onward being null:
class trainjourney {
public int price;
public trainjourney onward;
persistent data structures 465

public trainjourney(int p, trainjourney t) {
price = p;
onward = t;
}
}

now suppose that you have separate trainjourney objects representing a journey
from x to y and from y to z. You may want to create one journey that links the two
trainjourney objects (that is, x to y to z).
Here is a simple traditional imperative method to link these train journeys:
static trainjourney link(trainjourney a, trainjourney b){
if (a==null) return b;
trainjourney t = a;
while(t. Onward ! = null){
t = t. Onward;
}
t. Onward = b;
return a;
}

this method works by finding the last leg in the trainjourney for a and replacing the
null marking the end of as list with list b. (you need a special case if a has no elements. )
heres the problem: suppose that a variable firstjourney contains the route
from x to y and that a variable secondjourney contains the route from y to z. If you
call link(firstjourney, secondjourney), this code destructively updates first-
journey to also contain secondjourney, so in addition to the single user who
requests a trip from x to z seeing the combined journey as intended, the journey
from x to y has been updated destructively. Indeed, the firstjourney variable is no
longer a route from x to y, but one from x to z, which breaks code that depends on
firstjourneys not being modified! Suppose that firstjourney represented the
early-morning london-to-brussels train, which all subsequent users trying to get to
brussels will be surprised to see requires an onward leg, perhaps to cologne. Weve
all fought battles with such bugs concerning how visible a change in a data structure
should be.
The functional-style approach to this problem is to ban such side-effecting meth-
ods. If you need a data structure to represent the result of a computation, you should
make a new one, not mutate an existing data structure, as youve done previously.
Doing so is often a best practice in standard object-oriented programming, too. A
common objection to the functional approach is that it causes excess copying, and the
programmer says, ill remember or ill document the side effects. But such opti-
mism is a trap for maintenance programmers who have to deal with your code later.
Thus, the functional-style solution is as follows:
static trainjourney append(trainjourney a, trainjourney b){
return a==null ? B : new trainjourney(a. Price, append(a. Onward, b));
}
466 chapter 19 functional programming techniques

this code is clearly functional-style (uses no mutation, even locally) and doesnt mod-
ify any existing data structures. Note, however, that the code doesnt create a new
trainjourney. If a is a sequence of n elements and b is a sequence of m elements, the
code returns a sequence of n+m elements, in which the first n elements are new nodes
and the final m elements share with trainjourney b. Note that users are required not
to mutate the result of append because by doing so, they may corrupt the trains passed
as sequence b. Figures 19.2 and 19.3 illustrate the difference between the destructive
append and the functional-style append.

Destructive append

before
a b

x x

after
a b

x

append(a, b)

figure 19.2 the data structure is destructively updated.

Functional-style append

before
a b

x x

after
a b

x x

the result contains a copy of the first
trainjourney nodes but shares nodes
append(a, b) with the second trainjourney.

Figure 19.3 functional style with no modifications to the data structure
persistent data structures 467

19.2.2 another example with trees
before leaving this topic, consider another data structure: a binary search tree that
might be used to implement a similar interface to a hashmap. The idea is that a tree
contains a string representing a key and an int representing its value, perhaps
names and ages:
class tree {
private string key;
private int val;
private tree left, right;
public tree(string k, int v, tree l, tree r) {
key = k; val = v; left = l; right = r;
}
}
class treeprocessor {
public static int lookup(string k, int defaultval, tree t) {
if (t == null) return defaultval;
if (k. Equals(t. Key)) return t. Val;
return lookup(k, defaultval,
k. Compareto(t. Key) < 0 ? T. Left : t. Right);
}
// other methods processing a tree
}

you want to use the binary search tree to look up string values to produce an int.
Now consider how you might update the value associated with a given key (assuming
for simplicity that the key is already present in the tree):
public static void update(string k, int newval, tree t) {
if (t == null) { /* should add a new node */ }
else if (k. Equals(t. Key)) t. Val = newval;
else update(k, newval, k. Compareto(t. Key) < 0 ? T. Left : t. Right);
}

adding a new node is trickier. The easiest way is to make the method update return
the tree that has been traversed (unchanged unless you had to add a node). Now this
code is slightly clumsier because the user needs to remember that update tries to
update the tree in place, returning the same tree as passed. But if the original tree
were empty, a new node is returned as a result:
public static tree update(string k, int newval, tree t) {
if (t == null)
t = new tree(k, newval, null, null);
else if (k. Equals(t. Key))
t. Val = newval;
else if (k. Compareto(t. Key) < 0)
t. Left = update(k, newval, t. Left);
else
t. Right = update(k, newval, t. Right);
return t;
}
468 chapter 19 functional programming techniques

note that both versions of update mutate the existing tree, meaning that all users of
the map stored in the tree see the mutation.

19.2.3 using a functional approach
how might you program such tree updates functionally? You need to create a new
node for the new key-value pair. You also need to create new nodes on the path from
the root of the tree to the new node, as follows:
public static tree fupdate(string k, int newval, tree t) {
return (t == null) ?
New tree(k, newval, null, null) :
k. Equals(t. Key) ?
New tree(k, newval, t. Left, t. Right) :
k. Compareto(t. Key) < 0 ?
New tree(t. Key, t. Val, fupdate(k, newval, t. Left), t. Right) :
new tree(t. Key, t. Val, t. Left, fupdate(k, newval, t. Right));
}

in general, this code isnt expensive. If the tree is of depth d and reasonably well bal-
anced, it can have around 2 d entries, so you re-create a small fraction of it.
Weve written this code as a single conditional expression instead of using if-then-
else to emphasize the idea that the body is a single expression with no side effects.
But you may prefer to write an equivalent if-then-else chain, each containing a
return.
Whats the difference between update and fupdate? We noted previously that the
method update assumes that every user wants to share the data structure and see
updates caused by any part of the program. Therefore, its vital (but often over-
looked) in nonfunctional code that whenever you add some form of structured value
to a tree, you copy it, because someone may assume later that he can update it. By con-
trast, fupdate is purely functional; it creates a new tree as a result but shares as much
as it can with its argument. Figure 19.4 illustrates this idea. You have a tree consisting

output of
t input t to fupdate
fupdate("will", 26, t)

mary:22 mary:22

emily:20 tian:29 tian:29

figure 19.4 no existing
data structure was harmed
alan:50 georgie:23 raoul:23 will:26 during the making of this
update to tree.
Lazy evaluation with streams 469

of nodes that store a name and an age of a person. Calling fupdate doesnt modify the
existing tree; it creates new nodes living at the side of the tree without harming the
existing data structure.
Such functional data structures are often called persistenttheir values persist and
are isolated from changes happening elsewhereso as a programmer, youre sure
that fupdate wont mutate the data structures passed as its arguments. Theres one
proviso: the other side of the treaty requires all users of persistent data structures to
follow the do-not-mutate requirement. If not, a programmer who disregards this pro-
viso might mutate the result of fupdate (by changing emilys 20, for example). Then
this mutation would be visible as an (almost certainly unwanted) unexpected and
delayed change to the data structure passed as argument to fupdate!
Viewed in these terms, fupdate can be more efficient. The no mutation of exist-
ing structure rule allows structures that differ only slightly (such as the tree seen by
user a and the modified version seen by user b) to share storage for common parts of
their structure. You can get the compiler to help enforce this rule by declaring fields
key, val, left, and right of class tree to be final. But remember that final protects
only a field, not the object pointed to, which may need its own fields to be final to
protect it, and so on.
You might say, i want updates to the tree to be seen by some users (but admittedly
not by some others). You have two choices. One choice is the classical java solution:
be careful when updating something to check whether you need to copy it first. The
other choice is the functional-style solution: you logically make a new data structure
whenever you do an update (so that nothing is ever mutated) and arrange to pass the
correct version of the data structure to users as appropriate. This idea could be
enforced through an api. If certain clients of the data structure need to have updates
visible, they should go through an api that returns the latest version. Clients who
dont want updates visible (such as for long-running statistical analysis) use whatever
copy they retrieve, knowing that it cant be mutated from under them.
This technique is like updating a file on a cd-r, which allows a file to be written
only once by burning with a laser. Multiple versions of the file are stored on the cd
(smart cd-authoring software might even share common parts of multiple versions),
and you pass the appropriate block address of the start of the file (or a filename
encoding the version within its name) to select which version you want to use. In java,
things are rather better than on a cd, in that old versions of the data structure that
can no longer be used are garbage-collected.

19.3 lazy evaluation with streams
you saw in previous chapters that streams are great ways to process a collection of
data. But for various reasons, including efficient implementation, the java 8 designers
added streams to java in a rather specific way. One limitation is that you cant define a
stream recursively because a stream can be consumed only once. In the following sec-
tions, we show you how this situation can be problematic.
470 chapter 19 functional programming techniques

19.3.1 self-defining stream
revisit the example from chapter 6 of generating prime numbers to understand this
idea of a recursive stream. In that chapter, you saw that (perhaps as part of a mymath-
utils class), you can compute a stream of prime numbers as follows:

public static stream<integer> primes(int n) {
return stream. Iterate(2, i -> i + 1)
. Filter(mymathutils: : isprime)
. Limit(n);
}
public static boolean isprime(int candidate) {
int candidateroot = (int) math. Sqrt((double) candidate);
return intstream. Rangeclosed(2, candidateroot)
. Nonematch(i -> candidate % i == 0);
}

but this solution is somewhat awkward. You have to iterate through every number
every time to see whether it can be divided by a candidate number. (in fact, you need
only test numbers that have been already classified as prime. )
ideally, the stream should filter out numbers that are divisible by the prime that
the stream is producing on the go. Heres how this process might work:
1 you need a stream of numbers from which youll select prime numbers.
2 from that stream, take the first number (the head of the stream), which will be
a prime number. (in the initial step, this number is 2. )
3 filter all the numbers that are divisible by that number from the tail of the
stream.
4 the resulting tail is the new stream of numbers that you can use to find prime
numbers. Essentially, you go back to step 1, so this algorithm is recursive.
Note that this algorithm is poor for a few reasons,2 but its simple to reason about algo-
rithms for the purpose of working with streams. In the following sections, you try to
write this algorithm by using the streams api.
Step 1: get a stream of numbers
you can get an infinite stream of numbers starting from 2 by using the method int-
stream. Iterate (which we described in chapter 5) as follows:

static intstream numbers(){
return intstream. Iterate(2, n -> n + 1);
}

2
you can find more information about why the algorithm is poor at www. Cs. Hmc. Edu/~oneill/papers/sieve-
jfp. Pdf.
Lazy evaluation with streams 471

step 2: take the head
an intstream comes with the method findfirst, which you can use to return the
first element:
static int head(intstream numbers){
return numbers. Findfirst(). Getasint();
}

step 3: filter the tail
define a method to get the tail of a stream:
static intstream tail(intstream numbers){
return numbers. Skip(1);
}

given the head of the stream, you can filter the numbers as follows:
intstream numbers = numbers();
int head = head(numbers);
intstream filtered = tail(numbers). Filter(n -> n % head ! = 0);

step 4: recursively create a stream of primes
here comes the tricky part. You may be tempted to try passing back the resulting fil-
tered stream so that you can take its head and filter more numbers, like this:
static intstream primes(intstream numbers) {
int head = head(numbers);
return intstream. Concat(
intstream. Of(head),
primes(tail(numbers). Filter(n -> n % head ! = 0))
);
}

bad news
unfortunately, if you run the code in step 4, you get the following error: java. Lang
. Illegalstateexception: stream has already been operated upon or closed. Indeed,
youre using two terminal operations to split the stream into its head and tail: find-
first and skip. Remember from chapter 4 that after you call a terminal operation on
a stream, its consumed forever!
Lazy evaluation
theres an additional, more important problem: the static method intstream. Concat
expects two instances of a stream, but its second argument is a direct recursive call to
primes, resulting in an infinite recursion! For many java purposes, restrictions on
java 8 streams such as no recursive definitions are unproblematic and give your data-
base-like queries expressivity and the ability to parallelize. Thus, the java 8 designers
chose a sweet spot. Nonetheless, the more general features and models of streams
from functional languages such as scala and haskell can be useful additions to your
programming toolbox. What you need is a way to lazily evaluate the call to the method
472 chapter 19 functional programming techniques

primes in the second argument of concat. (in a more technical programming vocab-
ulary, we refer to this concept as lazy evaluation, nonstrict evaluation or even call by
name. ) only when you need to process the prime numbers (such as with the limit
method) should the stream be evaluated. Scala (which we explore in chapter 20) pro-
vides support for this idea. In scala, you can write the preceding algorithm as follows,
where the operator #: : does lazy concatenation (arguments being evaluated only
when you need to consume the stream):
def numbers(n: int): stream[int] = n #: : numbers(n+1)
def primes(numbers: stream[int]): stream[int] = {
numbers. Head #: : primes(numbers. Tail filter (n => n % numbers. Head ! = 0))
}

dont worry about this code. Its only purpose is to show you an area of difference
between java and other functional programming languages. Its good to reflect for a
moment about how the arguments are evaluated. In java, when you call a method, all
its arguments are fully evaluated immediately. But when you use #: : in scala, the con-
catenation returns immediately, and the elements are evaluated only when needed.
In the next section, we turn to implementing this idea of lazy lists directly in java.

19.3.2 your own lazy list
java 8 streams are often described as being lazy. Theyre lazy in one particular aspect:
a stream behaves like a black box that can generate values on request. When you apply
a sequence of operations to a stream, these operations are merely saved up. Only
when you apply a terminal operation to a stream is anything computed. This delaying
has a great advantage when you apply several operations (perhaps a filter and a map
followed by a terminal operation reduce) to a stream: the stream has to be traversed
only once instead of for each operation.
In this section, you consider the notion of lazy lists, which are forms of a more gen-
eral stream. (lazy lists form a concept similar to stream. ) lazy lists also provide an
excellent way of thinking about higher-order functions. You place a function value in
a data structure so that most of the time, it can sit there unused, but when its called
(on demand), it can create more of the data structure. Figure 19.5 illustrates this idea.

Tail tail
linkedlist

function function
lazylist

figure 19.5 elements of a linkedlist exist (are spread out) in memory.
But elements of a lazylist are created on demand by a function; you
can see them as being spread out in time.
Lazy evaluation with streams 473

next, you see how this concept works. You want to generate an infinite list of prime
numbers by using the algorithm we described earlier.
Creating a basic linked list
recall that you can define a simple linked-list-style class called mylinkedlist in java
by writing it as follows (with a minimal mylist interface):

interface mylist<t> {
t head();
mylist<t> tail();
default boolean isempty() {
return true;
}
}
class mylinkedlist<t> implements mylist<t> {
private final t head;
private final mylist<t> tail;
public mylinkedlist(t head, mylist<t> tail) {
this. Head = head;
this. Tail = tail;
}
public t head() {
return head;
}
public mylist<t> tail() {
return tail;
}
public boolean isempty() {
return false;
}
}
class empty<t> implements mylist<t> {
public t head() {
throw new unsupportedoperationexception();
}
public mylist<t> tail() {
throw new unsupportedoperationexception();
}
}

now you can construct a sample mylinkedlist value as follows:

mylist<integer> l =
new mylinkedlist<>(5, new mylinkedlist<>(10, new empty<>()));

creating a basic lazy list
an easy way to adapt this class to the concept of a lazy list is to cause the tail not to be
present in memory all at once, but to have the supplier<t> that you saw in chapter 3
(you can also see it as being a factory with a function descriptor void -> t) produce
the next node of the list. This design leads to the following code:
474 chapter 19 functional programming techniques

import java. Util. Function. Supplier;
class lazylist<t> implements mylist<t>{
final t head;
final supplier<mylist<t>> tail;
public lazylist(t head, supplier<mylist<t>> tail) {
this. Head = head;
this. Tail = tail;
}
public t head() {
return head;
} note that tail using a supplier
public mylist<t> tail() { encodes laziness, compared
return tail. Get(); with method head.
}
public boolean isempty() {
return false;
}
}

calling the method get from the supplier causes the creation of a node of the lazy-
list (as a factory would create a new object).
Now you can create the infinite lazy list of numbers starting at n as follows. Pass a
supplier as the tail argument of the lazylist constructor, which creates the next
element in the series of numbers:

public static lazylist<integer> from(int n) {
return new lazylist<integer>(n, () -> from(n+1));
}

if you try the following code, you see that it prints 2 3 4. Indeed, the numbers are gen-
erated on demand. To check, insert system. Out. Println appropriately or note that
from(2)would run forever if it tried to calculate all the numbers starting from 2:

lazylist<integer> numbers = from(2);
int two = numbers. Head();
int three = numbers. Tail(). Head();
int four = numbers. Tail(). Tail(). Head();
system. Out. Println(two + " " + three + " " + four);

generating primes again
see whether you can use what youve done so far to generate a self-defining lazy list of
prime numbers (something that you were unable to do with the streams api). If you
were to translate the code that was using the streams api earlier, using the new
lazylist, the code would look like something like this:

public static mylist<integer> primes(mylist<integer> numbers) {
return new lazylist<>(
numbers. Head(),
() -> primes(
numbers. Tail()
lazy evaluation with streams 475

. Filter(n -> n % numbers. Head() ! = 0)
)
);
}

implementing a lazy filter
unfortunately, a lazylist (more accurately, the list interface) doesnt define a filter
method, so the preceding code wont compile! To fix this problem, declare a filter
method, as follows:
public mylist<t> filter(predicate<t> p) { you could return new
return isempty() ? Empty<>() , but using 'this'
this : is as good and empty.
P. Test(head()) ?
New lazylist<>(head(), () -> tail(). Filter(p)) :
tail(). Filter(p);
}

your code compiles and is ready for use! You can calculate the first three prime num-
bers by chaining calls to tail and head, as follows:
lazylist<integer> numbers = from(2);
int two = primes(numbers). Head();
int three = primes(numbers). Tail(). Head();
int five = primes(numbers). Tail(). Tail(). Head();
system. Out. Println(two + " " + three + " " + five);

this code prints 2 3 5, which are the first three prime numbers. Now you can have some
fun. You could print all the prime numbers, for example. (the program will run infinitely
by writing a printall method, which iteratively prints the head and tail of a list. )
static <t> void printall(mylist<t> list){
while (! List. Isempty()){
system. Out. Println(list. Head());
list = list. Tail();
}
}
printall(primes(from(2)));

this chapter being a functional programming chapter, we should explain that this
code can be neatly written recursively:
static <t> void printall(mylist<t> list){
if (list. Isempty())
return;
system. Out. Println(list. Head());
printall(list. Tail());
}

this program wouldnt run infinitely, though. Sadly, it would eventually fail due to stack
overflow because java doesnt support tail call elimination, as discussed in chapter 18.
476 chapter 19 functional programming techniques

review
youve built a whole lot of technology with lazy lists and functions, using them only to
define a data structure containing all the primes. Whats the practical use? Well,
youve seen how to place functions inside data structures (because java 8 allows you
to), and you can use these functions to create parts of the data structure on demand
instead of when the structure is created. This capability may be useful if youre writing
a game-playing program, perhaps for chess; you can have a data structure that notion-
ally represents the whole tree of possible moves (far too big to calculate eagerly) but
that can be created on demand. This data structure would be a lazy tree, as opposed to
a lazy list. Weve concentrated on lazy lists in this chapter because they provide a link
to another java 8 feature, streams, which enabled us to discuss the pros and cons of
streams compared with lazy lists.
There remains the question of performance. Its easy to assume that doing things
lazily is better than doing things eagerly. Surely, its better to calculate only the values
and data structures needed by a program on demand than to create all those values
(and perhaps more), as done in traditional execution. Unfortunately, the real world
isnt so simple. The overhead of doing things lazily (such as the additional suppliers
between items in your lazylist) outweighs the notional benefit unless you explore, say,
less than 10 % of the data structure. Finally, theres a subtle way in which your lazylist
values arent truly lazy. If you traverse a lazylist value such as from(2), perhaps up to
the 10th item, it also creates all the nodes twice, creating 20 nodes rather than 10. This
result is hardly lazy. The issue is that the supplier in tail is repeatedly called on
each on-demand exploration of the lazylist. You can fix this problem by arranging
for the supplier in tail to be called only on the first on-demand exploration, with
the resulting value being cached, in effect solidifying the list at that point. To achieve
this goal, add a private optional<lazylist<t>> alreadycomputed field to your
definition of lazylist and arrange for the tail method to consult and update it
appropriately. The pure functional language haskell arranges that all its data struc-
tures are properly lazy in the latter sense. Read one of the many articles on haskell if
youre interested.
Our guideline is to remember that lazy data structures can be useful weapons in your
programming armory. Use these structures when they make an application easier to
program; rewrite them in more traditional style if they cause unacceptable inefficiency.
The next section deals with another feature of almost all functional programming
languages except java: pattern matching.

19.4 pattern matching
theres one other important aspect to whats generally regarded as functional program-
ming: (structural) pattern matching, which isnt to be confused with pattern matching
and regex. Chapter 1 ended by observing that mathematics can write definitions such as

f(0) = 1
f(n) = n*f(n-1) otherwise
pattern matching 477

whereas in java, you have to write an if-then-else or a switch statement. As data
types become more complex, the amount of code (and clutter) needed to process
them increases. Using pattern matching can reduce this clutter.
To illustrate, take a tree structure that youd like to traverse. Consider a simple
arithmetic language consisting of numbers and binary operations:
class expr { ... }
class number extends expr { int val; ... }
class binop extends expr { string opname; expr left, right; ... }

suppose that youre asked to write a method to simplify some expressions. 5 + 0 can
be simplified to 5, for example. Using our expr class, new binop("+", new number(5),
new number(0)) could be simplified to number(5). You might traverse an expr struc-
ture as follows:
expr simplifyexpression(expr expr) {
if (expr instanceof binop
&& ((binop)expr). Opname. Equals("+"))
&& ((binop)expr). Right instanceof number
&& ... // it's all getting very clumsy
&& ... ) {
return (binop)expr. Left;
}
...
}

you can see that this code rapidly gets ugly!

19.4.1 visitor design pattern
another way to unwrap the data type in java is to use the visitor design pattern. In essence,
you create a separate class that encapsulates an algorithm to visit a specific data type.
The visitor class works by taking as input a specific instance of the data type; then it
can access all its members. Heres an example. First, add the method accept to binop,
which takes simplifyexprvisitor as argument and passes itself to it (and add a simi-
lar method for number):
class binop extends expr{
...
Public expr accept(simplifyexprvisitor v){
return v. Visit(this);
}
}

now the simplifyexprvisitor can access a binop object and unwrap it:
public class simplifyexprvisitor {
...
Public expr visit(binop e){
if("+". Equals(e. Opname) && e. Right instanceof number && ... ){
return e. Left;
}
478 chapter 19 functional programming techniques

return e;
}
}

19.4.2 pattern matching to the rescue
a simpler solution uses a feature called pattern matching. That feature isnt available
in java, so were going to use small examples from the scala programming language to
illustrate pattern matching. The examples give you an idea of what could be possible
in java if pattern matching were supported.
Given data type expr representing arithmetic expressions, in the scala program-
ming language (which we use because its syntax is closest to java), you can write the
following code to decompose an expression:
def simplifyexpression(expr: expr): expr = expr match {
case binop("+", e, number(0)) => e // adding zero
case binop("*", e, number(1)) => e // multiplying by one
case binop("/", e, number(1)) => e // dividing by one
case _ => expr // can't simplify expr
}

this use of pattern matching gives you an extremely concise and expressive way to
manipulate many treelike data structures. Typically, this technique is useful for build-
ing compilers or engines for processing business rules. Note that the scala syntax

expression match { case pattern => expression ... }

is similar to the java syntax

switch (expression) { case constant : statement ... }

with scalas wildcard pattern _ making the final case _ play the role of default: in
java. The main visible syntactic difference is that scala is expression-oriented, whereas
java is more statement-oriented. But for the programmer, the main expressiveness dif-
ference is the fact that java patterns in case labels are restricted to a couple of primi-
tive types, enumerations, a few special classes that wrap certain primitive types, and
strings. One of the biggest practical advantages of using languages with pattern
matching is that you can avoid using big chains of switch or if-then-else statements
interleaved with field-selection operations.
Its clear that scalas pattern matching wins on ease of expressiveness over java,
and you can look forward to a future java allowing more-expressive switch state-
ments. (we make a concrete proposal for this feature in chapter 21. )
in the meantime, we show you how java 8 lambdas can provide an alternative way
of achieving pattern-matching-like code in java. We describe this technique purely to
show you another interesting application of lambdas.
Pattern matching 479

faking pattern matching in java
first, consider how rich scalas pattern-matching match expression form is. The case
def simplifyexpression(expr: expr): expr = expr match {
case binop("+", e, number(0)) => e
...

Means check that expr is a binop, extract its three components (opname, left,
right), and then pattern-match these componentsthe first against the string +, the
second against the variable e (which always matches), and the third against the pat-
tern number(0). In other words, pattern matching in scala (and many other func-
tional languages) is multilevel. Your simulation of pattern matching with java 8s
lambdas produces only single-level pattern matching. In the preceding example, your
simulation would express cases such as binop(op, l, r) or number(n) but not
binop("+", e, number(0)).
First, we make a slightly surprising observation: now that you have lambdas, you
could in principle never use if-then-else in your code. You could replace code such
as condition ? E1 : e2 with a method call, as follows:

myif(condition, () -> e1, () -> e2);

somewhere, perhaps in a library, youd have a definition (generic in type t):
static <t> t myif(boolean b, supplier<t> truecase, supplier<t> falsecase) {
return b ? Truecase. Get() : falsecase. Get();
}

the type t plays the role of the result type of the conditional expression. In principle, you
can perform similar tricks with other control-flow constructs such as switch and while.
In normal code, this encoding would make your code more obscure because if-then-
else captures this idiom perfectly. But weve noted that javas switch and if-then-else
dont capture the idiom of pattern matching, and it turns out that lambdas can encode
(single-level) pattern matchingrather more neatly than the chains of if-then-else.
Returning to pattern-matching values of class expr (which has two subclasses,
binop and number), you can define a method patternmatchexpr (again generic in t,
the result type of the pattern match):
interface trifunction<s, t, u, r>{
r apply(s s, t t, u u);
}
static <t> t patternmatchexpr(
expr e,
trifunction<string, expr, expr, t> binopcase,
function<integer, t> numcase,
supplier<t> defaultcase) {
return
(e instanceof binop) ?
Binopcase. Apply(((binop)e). Opname, ((binop)e). Left,
((binop)e). Right) :
480 chapter 19 functional programming techniques

(e instanceof number) ?
Numcase. Apply(((number)e). Val) :
defaultcase. Get();
}

the result is that the method call
patternmatchexpr(e, (op, l, r) -> {return binopcode; },
(n) -> {return numcode; },
() -> {return defaultcode; });

determines whether e is a binop (and if so, runs binopcode, which has access to the fields
of the binop via identifiers op, l, r) or a number (and if so, runs numcode, which has
access to the value n). The method even makes provision for defaultcode, which would
be executed if someone later created a tree node that was neither a binop nor a number.
The following listing shows you how to start using patternmatchexpr by simplify-
ing addition and multiplication expressions.

Listing 19.1 implementing pattern matching to simplify an expression

deals with a
public static expr simplify(expr e) {
binop expression
trifunction<string, expr, expr, expr> binopcase =
(opname, left, right) -> {
deals with the if ("+". Equals(opname)) {
addition case if (left instanceof number && ((number) left). Val == 0) {
return right;
}
if (right instanceof number && ((number) right). Val == 0) {
return left;
deals with the }
multiplication }
case if ("*". Equals(opname)) {
if (left instanceof number && ((number) left). Val == 1) {
return right;
}
a default case if if (right instanceof number && ((number) right). Val == 1) {
the user provides return left;
an expr thats }
not recognized } deals with
return new binop(opname, left, right); a number
};
function<integer, expr> numcase = val -> new number(val);
supplier<expr> defaultcase = () -> new number(0); applies
return patternmatchexpr(e, binopcase, numcase, defaultcase); pattern
matching

now you can call the simplify method as follows:
expr e = new binop("+", new number(5), new number(0));
expr match = simplify(e);
system. Out. Println(match); prints 5
miscellany 481

youve seen a lot of information so far: higher-order functions, currying, persistent
data structures, lazy lists, and pattern matching. The next section looks at certain sub-
tleties that weve deferred to the end to avoid overcomplicating the text.

19.5 miscellany
in this section, we explore two subtleties of being functional and of having referential
transparency: one about efficiency and the other about returning the same result.
These issues are interesting, but we place them here because the subtleties concern
side effects and arent conceptually central. We also explore the idea of combinators
methods or functions that take two or more functions and return another function.
This idea has inspired many of the additions to the java 8 api and more recently the
java 9 flow api.

19.5.1 caching or memoization
suppose that you have a side-effect-free method computenumberofnodes(range) that
calculates the number of nodes inside a given range in a network with a treelike topol-
ogy. Assume that the network never changes (that is, the structure is immutable), but
that calling the method computenumberofnodes is expensive to calculate because the
structure needs to be traversed recursively. You may want to calculate the results over
and over. If you have referential transparency, you have a clever way to avoid this addi-
tional overhead. One standard solution is memoizationadding a cache (such as a
hashmap) to the method as a wrapper. First, the wrapper consults the cache to see
whether the (argument, result) pair is already in the cache. If so, it can return the
stored result immediately. Otherwise, you call computenumberofnodes, but before
returning from the wrapper, you store the new (argument, result) pair in the cache.
Strictly speaking, this solution isnt purely functional because it mutates a data struc-
ture shared by multiple callers, but the wrapped version of the code is referentially
transparent.
In practice, this code works as follows:

final map<range, integer> numberofnodes = new hashmap<>();
integer computenumberofnodesusingcache(range range) {
integer result = numberofnodes. Get(range);
if (result ! = null){
return result;
}
result = computenumberofnodes(range);
numberofnodes. Put(range, result);
return result;
}

note java 8 enhances the map interface (see appendix b) with a computeif-
absent method for such use cases. You could use computeifabsent to write
clearer code:
482 chapter 19 functional programming techniques

integer computenumberofnodesusingcache(range range) {
return numberofnodes. Computeifabsent(range,
this: : computenumberofnodes);
}

its clear that the method computenumberofnodesusingcache is referentially trans-
parent (assuming that the method computenumberofnodes is also referentially transpar-
ent). But the fact that numberofnodes has mutable shared state and that hashmap
isnt synchronized3 means that this code isnt thread-safe. Even using (lock-protected)
hashtable or (concurrent-without-locking) concurrenthashmap instead of hashmap
may not produce the expected performance if parallel calls are made to numberof-
nodes from multiple cores. Theres a race condition between your finding that range
isnt in the map and inserting the (argument, result) pair back into the map, which
means that multiple processes might compute the same value to add to the map.
Perhaps the best thing to take away from this struggle is the fact that mixing mutable
state with concurrency is trickier than youd imagine. Functional-style programming
avoids this practice except for low-level performance hacks such as caching. A second
takeaway is that apart from implementing tricks such as caching, if you code in func-
tional style, you never need to care whether another functional-style method that you
call is synchronized, because you know that it has no shared mutable state.

19.5.2 what does return the same object mean?
Consider again the binary tree example from section 19.2.3. In figure 19.4, variable t
points to an existing tree, and the figure shows the effect of calling fupdate("will",
26, t) to produce a new tree, which presumably is assigned to variable t2. The figure
makes clear that t and all the data structures reachable from it arent mutated. Now
suppose that you perform a textually identical call in the additional assignment:

t3 = fupdate("will", 26, t);

now t3 points to three more newly created nodes containing the same data as those
in t2. The question is whether fupdate is referentially transparent. Referentially trans-
parent means equal arguments (the case here) imply equal results. The problem is
that t2 and t3 are different references, and therefore (t2 == t3) is false, so it looks
as though youll have to conclude that fupdate isnt referentially transparent. But
when youre using persistent data structures that arent to be modified, no logical dif-
ference exists between t2 and t3.
We can debate this point at length, but the simplest adage is that functional-style
programming generally uses equals to compare structured values rather than == (ref-
erence equality) because data isnt modified, and under this model, fupdate is refer-
entially transparent.

3
this place is one where bugs breed. Its so easy to use hashmap and to forget the fact that the java manual
notes that its not thread-safe (or not to care because *your* program is currently single-threaded).
Miscellany 483

19.5.3 combinators
in functional programming, its common and natural to write a higher-order function
(perhaps written as a method) that accepts, say, two functions and produces another
function that somehow combines these functions. The term combinator generally is
used for this idea. Much of the new java 8 api is inspired by this idea, such as then-
combine in the completablefuture class. You can give this method two completable-
futures and a bifunction to produce another completablefuture.
Although a detailed discussion of combinators in functional programming is
beyond the scope of this book, its worth looking at a couple of special cases to give
you the flavor of how operations that take and return functions are a common and
natural functional programming construct. The following method encodes the idea
of function composition:
static <a, b, c> function<a, c> compose(function<b, c> g, function<a, b> f) {
return x -> g. Apply(f. Apply(x));
}

this method takes functions f and g as arguments and returns a function whose effect
is to do f first and then g. Then you can define an operation that captures internal
iteration as a combinator. Suppose that you want to take data and apply function f to
it repeatedly, n times, as in a loop. Your operation (call it repeat) takes a function, f,
saying what happens in one iteration and returning a function that says what happens
in n iterations. A call such as

repeat(3, (integer x) -> 2*x);

returns x ->(2*(2*(2*x)) or equivalently x -> 8*x.
You can test this code by writing

system. Out. Println(repeat(3, (integer x) -> 2*x). Apply(10));

which prints 80.
You can code the repeat method as follows (noting the special case of a zero-trip
loop):
return the
static <a> function<a, a> repeat(int n, function<a, a> f) { do-nothing
return n==0 ? X -> x identity
: compose(f, repeat(n-1, f)); function if
} n is zero.
Otherwise do f, repeated n-1 times, followed by doing it once more.

Variants of this idea can model richer notions of iteration, including having a func-
tional model of mutable state passed between iterations. But its time to move on. This
chapters role was to give you a summary of functional programming as the basis for
java 8. Many excellent books explore functional programming in greater depth.
484 chapter 19 functional programming techniques

summary
 first-class functions are functions that can be passed as arguments, returned as
results, and stored in data structures.
 A higher-order function takes one or more functions as input or returns another
function. Typical higher-order functions in java include comparing, andthen,
and compose.
 Currying is a technique that lets you modularize functions and reuse code.
 A persistent data structure preserves the previous version of itself when its
modified. As a result, it can prevent unnecessary defensive copying.
 Streams in java cant be self-defined.
 A lazy list is a more-expressive version of a java stream. A lazy list lets you pro-
duce elements of the list on demand by using a supplier that can create more of
the data structure.
 Pattern matching is a functional feature that lets you unwrap data types. You
can view data matching as generalizing javas switch statement.
 Referential transparency allows computations to be cached.
 Combinators are functional ideas that combine two or more functions or other
data structures.
Blending oop and fp:
comparing java and scala

this chapter covers
 an introduction to scala
 how java relates to scala and vice versa
 how functions in scala compare to java
 classes and traits

scala is a programming language that mixes object-oriented and functional pro-
gramming. Its often seen as an alternative language to java for programmers who
want functional features in a statically typed programming language that runs on
the jvm while keeping a java feel. Scala introduces many more features than java: a
more-sophisticated type system, type inference, pattern matching (as presented in
chapter 19), constructs that define domain-specific languages simply, and so on. In
addition, you can access all java libraries within scala code.
You may wonder why we have a chapter about scala in a java book. This book has
largely centered on adopting functional-style programming in java. Scala, like java,
supports the concepts of functional-style processing of collections (that is, streamlike
operations), first-class functions, and default methods. But scala pushes these ideas
further, providing a larger set of features that support these ideas compared with
java. We believe that you may find it interesting to compare scala with the approach

485
486 chapter 20 blending oop and fp: comparing java and scala

taken by java and see javas limitations. This chapter aims to shed light on this matter to
appease your curiosity. We dont necessarily encourage the adoption of scala over java.
Other interesting new programming languages on the jvm, such as kotlin, are also
worth looking at. The purpose of this chapter is to open your horizons to whats avail-
able beyond java. We believe that its important for a well-rounded software engineer to
be knowledgeable about the wider programming-languages ecosystem.
Also keep in mind that the purpose of this chapter isnt to teach you how to write
idiomatic scala code or to tell you everything about scala. Scala supports many fea-
tures (such as pattern matching, for-comprehensions, and implicits) that arent avail-
able in java, and we wont discuss those features. Rather, we focus on comparing the
java and scala features to give you an idea of the bigger picture. Youll find that you
can write more concise and readable code in scala compared with java, for example.
This chapter starts with an introduction to scala: writing simple programs and
working with collections. Next, we discuss functions in scala: first-class functions, clo-
sures, and currying. Finally, we look at classes in scala and at a feature called traits,
which is scalas take on interfaces and default methods.

20.1 introduction to scala
this section briefly introduces basic scala features to give you a feel for simple scala
programs. We start with a slightly modified hello world example written in an
imperative style and a functional style. Then we look at some data structures that scala
supportslist, set, map, stream, tuple, and optionand compare them with java.
Finally, we present traits, scalas replacement for javas interfaces, which also support
inheritance of methods at object-instantiation time.

20.1.1 hello beer
to change a bit from the classic hello world example, bring in some beer. You want
to print the following output on the screen:
hello 2 bottles of beer
hello 3 bottles of beer
hello 4 bottles of beer
hello 5 bottles of beer
hello 6 bottles of beer

imperative-style scala
heres how the code to print this output looks in scala when you use an imperative style:
object beer {
def main(args: array[string]){
var n : int = 2
while( n <= 6){
println(s"hello ${n} bottles of beer")
string
n += 1 interpolation
}
}
}
introduction to scala 487

you can find information about how to run this code on the official scala website (see
https: //docs. Scala-lang. Org/getting-started. Html). This program looks similar to what
youd write in java, and its structure is similar to that of java programs, consisting of
one method called main, which takes an array of strings as argument. (type annota-
tions follow the syntax s : string instead of strings, as in java. ) the main method
doesnt return a value, so its not necessary to declare a return type in scala as youd
have to do in java when you use void.

Note in general, nonrecursive method declarations in scala dont need an
explicit return type, because scala can infer the type for you.

Before we look at the body of the main method, we need to discuss the object declara-
tion. After all, in java you have to declare the method main within a class. The declara-
tion object introduces a singleton object, declaring a class beer and instantiating it at
the same time. Only one instance is created. This example is the first example of a
classical design pattern (the singleton design pattern) implemented as a language fea-
ture, and its free to use out of the box. In addition, you can view methods within an
object declaration as being declared as static, which is why the signature of the main
method isnt explicitly declared as static.
Now look at the body of main. This method also looks similar to a java method, but
statements dont need to end with a semicolon (which is optional). The body consists
of a while loop, which increments a mutable variable, n. For each new value of n, you
print a string on the screen, using the predefined println method. The println line
showcases another feature of scala: string interpolation, which allows you to embed vari-
ables and expressions directly in string literals. In the preceding code, you can use the
variable n directly in the string literal s"hello ${n} bottles of beer". Prepending the
string with the interpolator s provides that magic. Normally in java, you have to do an
explicit concatenation such as "hello " + n + " bottles of beer".
Functional-style scala
but what can scala offer after all our talk about functional-style programming
throughout this book? The preceding code can be written in a more functional-style
form in java as follows:
public class foo {
public static void main(string[] args) {
intstream. Rangeclosed(2, 6)
. Foreach(n -> system. Out. Println("hello " + n +
" bottles of beer"));
}
}

heres how that code looks in scala:
object beer {
def main(args: array[string]){
2 to 6 foreach { n => println(s"hello ${n} bottles of beer") }
488 chapter 20 blending oop and fp: comparing java and scala

}
}

the scala code is similar to the java code but less verbose. First, you can create a range
by using the expression 2 to 6. Heres something cool: 2 is an object of type int. In
scala, everything is an object; theres no concept of primitive types, as in java, which
makes scala a complete object-oriented language. An int object in scala supports a
method named to, which takes as an argument another int and returns a range. You
could have written 2. To(6) instead. But methods that take one argument can be writ-
ten in an infix form. Next, foreach (with a lowercase e) is similar to foreach in java
(with an uppercase e). This method is available on a range (you use the infix notation
again), and it takes a lambda expression as an argument to apply on each element. The
lambda-expression syntax is similar to that in java, but the arrow is => instead of ->.1
the preceding code is functional; youre not mutating a variable as you did in the ear-
lier example using a while loop.

20.1.2 basic data structures: list, set, map, tuple, stream, option
are you feeling good after having a couple of beers to quench your thirst? Most real
programs need to manipulate and store data, so in this section, you manipulate collec-
tions in scala and see how that process compares with java.
Creating collections
creating collections in scala is simple, thanks to scalas emphasis on conciseness. To
exemplify, heres how to create a map:

val authorstoage = map("raoul" -> 23, "mario" -> 40, "alan" -> 53)

several things are new in this line of code. First, its awesome that you can create a map
and associate a key with a value directly, using the syntax ->. Theres no need to add
elements manually, as in java:
map<string, integer> authorstoage = new hashmap<>();
authorstoage. Put("raoul", 23);
authorstoage. Put("mario", 40);
authorstoage. Put("alan", 53);

you learned in chapter 8, however, that java 9 has a couple of factory methods,
inspired by scala, that can help you tidy this type of code:
map<string, integer> authorstoage
= map. Ofentries(entry("raoul", 23),
entry("mario", 40),
entry("alan", 53));

1
note that the scala terms anonymous functions and closures (interchangeable) refer to what java calls lambda
expressions.
Introduction to scala 489

the second new thing is that you can choose not to annotate the type of the variable
authorstoage. You could have explicitly written val authorstoage : map[string,
int], but scala can infer the type of the variable for you. (note that the code is still
checked statically. All variables have a given type at compile time. ) we come back to
this feature in chapter 21. Third, you use the val keyword instead of var. Whats the
difference? The keyword val means that the variable is read-only and cant be reas-
signed to (like final in java). The var keyword means that the variable is read-write.
What about other collections? You can create a list (a singly linked list) or a set
(no duplicates) easily, as follows:
val authors = list("raoul", "mario", "alan")
val numbers = set(1, 1, 2, 3, 5, 8)

the authors variable has three elements, and the numbers variable has five elements.
Immutable vs. Mutable
one important fact to keep in mind is that the collections you created previously are
immutable by default, which means that they cant be changed after theyre created.
Immutability is useful because you know that accessing the collection at any point in
your program always yields a collection with the same elements.
How can you update an immutable collection in scala? To come back to the ter-
minology used in chapter 19, such collections in scala are said to be persistent.
Updating a collection produces a new collection that shares as much as possible
with the previous version, which persists without being affected by changes (as we
show in figures 19.3 and 19.4). As a consequence of this property, your code has
fewer implicit data dependencies: theres less confusion about which location in
your code updates a collection (or any other shared data structure) and at what
point in time.
The following example demonstrates this idea. Add an element to a set:

here, + is a method that adds 8 to the
val numbers = set(2, 5, 3); set, creating a new set object as a result.
Val newnumbers = numbers + 8
println(newnumbers) (2, 5, 3, 8)
println(numbers)
(2, 5, 3)

in this example, the set of numbers isnt modified. Instead, a new set is created with
an additional element.
Note that scala doesnt force you to use immutable collectionsonly makes it easy
to adopt immutability in your code. Also, mutable versions are available in the pack-
age scala. Collection. Mutable.
490 chapter 20 blending oop and fp: comparing java and scala

unmodifiable vs. Immutable
java provides several ways to create unmodifiable collections. In the following code,
the variable newnumbers is a read-only view of the set numbers:
set<integer> numbers = new hashset<>();
set<integer> newnumbers = collections. Unmodifiableset(numbers);

this code means that you wont be able to add new elements through the new-
numbers variable. But an unmodifiable collection is a wrapper over a modifiable col-
lection, so you could still add elements by accessing the numbers variable.
By contrast, immutable collections guarantee that nothing can change the collection,
regardless of how many variables are pointing to it.
We explained in chapter 19 how you could create a persistent data structure: an
immutable data structure that preserves the previous version of itself when modified.
Any modifications always produce a new updated structure.

Working with collections
now that youve seen how to create collections, you need to know what you can do with
them. Collections in scala support operations similar to those in the java stream api. You
may recognize filter and map in the following example and as illustrated in figure 20.1:
val filelines = source. Fromfile("data. Txt"). Getlines. Tolist()
val lineslongupper
= filelines. Filter(l => l. Length() > 10)
. Map(l => l. Touppercase())

l => l. Length() > 10 l => l. Touppercase()

input filter map result

figure 20.1 streamlike operations with scalas list

dont worry about the first line, which transforms a file into a list of strings consisting
of the lines in the file (similar to what files. Readalllines provides in java). The sec-
ond line creates a pipeline of two operations:
 a filter operation that selects only the lines that have a length greater than 10
 a map operation that transforms these long lines to uppercase

this code can be also written as follows:
val lineslongupper
= filelines filter (_. Length() > 10) map(_. Touppercase())
introduction to scala 491

you use the infix notation as well as the underscore (_) character, which is a place-
holder thats positionally matched with any arguments. In this case, you can read
_. Length() as l => l. Length(). In the functions passed to filter and map, the under-
score is bound to the line parameter that is to be processed.
Many more useful operations are available in scalas collection api. We recommend
taking a look at the scala documentation to get an idea (https: //docs. Scala-lang. Org/
overviews/collections/introduction. Html). Note that this api is slightly richer than the
streams api (including support for zipping operations, which let you combine elements
of two lists), so youll definitely gain a few programming idioms by checking it out.
These idioms may also make it into the streams api in future versions of java.
Finally, remember that in java, you can ask for a pipeline to be executed in parallel
by calling parallel on a stream. Scala has a similar trick. You need only use the par
method:

val lineslongupper
= filelines. Par filter (_. Length() > 10) map(_. Touppercase())

tuples
this section looks at another feature thats often painfully verbose in java: tuples. You
may want to use tuples to group people by name and phone number (here, simple
pairs) without declaring an ad hoc new class and instantiating an object for it:
("raoul", "+44 7700 700042"), ("alan", "+44 7700 700314"), and so on.
Unfortunately, java doesnt provide support for tuples, so you have to create your
own data structure. Heres a simple pair class:

public class pair<x, y> {
public final x x;
public final y y;
public pair(x x, y y){
this. X = x;
this. Y = y;
}
}

also, of course you need to instantiate pairs explicitly:

pair<string, string> raoul = new pair<>("raoul", "+44 7700 700042");
pair<string, string> alan = new pair<>("alan", "+44 7700 700314");

okay, but how about triplets and arbitrary-size tuples? Defining a new class for every tuple
size is tedious and ultimately affects the readability and maintainability of your programs.
Scala provides tuple literals, which allow you to create tuples through simple syntac-
tic sugar with the normal mathematical notation, as follows:

val raoul = ("raoul", "+44 7700 700042")
val alan = ("alan", "+44 7700 700314")
492 chapter 20 blending oop and fp: comparing java and scala

scala supports arbitrary-size2 tuples, so the following are possible:
a tuple of type (int,
val book = (2018 "modern java in action", "manning") string, string)
val numbers = (42, 1337, 0, 3, 14)
a tuple of type (int,
int, int, int, int)
you can access the elements of the tuples by their positions by using the accessors _1,
_2 (starting at 1), as in this example:

println(book. _1) prints 2018
println(numbers. _4) prints 3

isnt that example much nicer than what youd have to write in java? The good news is
that there are discussions about introducing tuple literals in future versions of java.
(see chapter 21 for more discussion of possible new features in java. )
stream
the collections that weve described so farlist, set, map, and tupleare all evalu-
ated eagerly (that is, immediately). By now, you know that streams in java are evaluated
on demand (that is, lazily). You saw in chapter 5 that because of this property, streams
can represent an infinite sequence without overflowing memory.
Scala also provides a corresponding lazily evaluated data structure called stream.
But streams in scala provide more features than those in java. Streams in scala
remember values that were computed so that previous elements can be accessed. In
addition, streams are indexed so that elements can be accessed by an index, like a list.
Note that the trade-off for these additional properties is the fact that streams are less
memory-efficient compared with javas streams, because being able to refer to previ-
ous elements means that the elements need to be remembered (cached).
Option
another data structure that youll be familiar with is optionscalas version of javas
optional, which we discussed in chapter 11. We argued that you should use optional
when possible to design better apis, in which by reading the signature of a method,
users can tell whether they can expect an optional value. You should use this data
structure instead of null whenever possible to prevent null-pointer exceptions.
You saw in chapter 11 that you can use optional to return the name of a persons
insurance if the persons age is greater than some minimum age, as follows:
public string getcarinsurancename(optional<person> person, int minage) {
return person. Filter(p -> p. Getage() >= minage)
. Flatmap(person: : getcar)
. Flatmap(car: : getinsurance)
. Map(insurance: : getname)
. Orelse("unknown");
}

2
tuples have a limit of 22 elements.
Functions 493

in scala, you can use option in a way similar to optional:

def getcarinsurancename(person: option[person], minage: int) =
person. Filter(_. Age >= minage)
. Flatmap(_. Car)
. Flatmap(_. Insurance)
. Map(_. Name)
. Getorelse("unknown")

you can recognize the same structure and method names apart from getorelse,
which is the equivalent of orelse in java. You see, throughout this book, youve
learned new concepts that you can apply directly to other programming languages!
Unfortunately, null also exists in scala for java compatibility reasons, but its use is
highly discouraged.

20.2 functions
scala functions can be viewed as being sequences of instructions that are grouped to
perform a task. These functions are useful for abstracting behavior and are the cor-
nerstone of functional programming.
In java, youre familiar with methods: functions associated with a class. Youve also
seen lambda expressions, which can be considered to be anonymous functions. Scala
offers a richer set of features to support functions than java does, and we look at those
features in this section. Scala provides the following:
 function typessyntactic sugar that represents the idea of java function descrip-
tors (that is, notations that represent the signature of the abstract method
declared in a functional interface), which we described in chapter 3
 anonymous functions that dont have the no-writing restriction on nonlocal
variables that javas lambda expressions have
 support for currying, which means breaking a function that takes multiple argu-
ments into a series of functions, each of which takes some of the arguments

20.2.1 first-class functions in scala
functions in scala are first-class values, which means that they can be passed around as
parameters, returned as a result, and stored in variables, like values such as integer
and string. As weve shown you in earlier chapters, method references and lambda
expressions in java can also be seen as first-class functions.
Heres an example of how first-class functions work in scala. Suppose that you have a
list of strings representing tweets youve received. Youd like to filter this list with different
criteria, such as tweets that mention the word java or tweets of a certain short length. You
can represent these two criteria as predicates (functions that return a boolean):

def isjavamentioned(tweet: string) : boolean = tweet. Contains("java")
def isshorttweet(tweet: string) : boolean = tweet. Length() < 20
494 chapter 20 blending oop and fp: comparing java and scala

in scala, you can pass these methods directly to the built-in filter as follows (as you
can pass them by using method references in java):

val tweets = list(
"i love the new features in java",
"how's it going? ",
"an sql query walks into a bar, sees two tables and says 'can i join
you? '"
)
tweets. Filter(isjavamentioned). Foreach(println)
tweets. Filter(isshorttweet). Foreach(println)

now inspect the signature of the built-in method filter:

def filter[t](p: (t) => boolean): list[t]

you may wonder what the type of the parameter p means (here, (t) => boolean),
because in java, youd expect a functional interface. This scala syntax isnt (yet) avail-
able in java, but it describes a function type. Here, the type represents a function that
takes an object of type t and returns a boolean. In java, this type is expressed as a
predicate<t> or function<t, boolean>, which has the same signature as the isjava-
mentioned and isshorttweet methods, so you can pass them as arguments to filter.
The designers of the java language decided not to introduce similar syntax for func-
tion types to keep the language consistent with previous versions. (introducing too
much new syntax in a new version of the language is viewed as adding too much cogni-
tive overhead. )

20.2.2 anonymous functions and closures
scala also supports anonymous functions, which have syntax similar to that of lambda
expressions. In the following example, you can assign to a variable named islong-
tweet an anonymous function that checks whether a given tweet is long:

a variable of function
val islongtweet : string => boolean type string to boolean
= (tweet : string) => tweet. Length() > 60
an anonymous function

in java, a lambda expression lets you create an instance of a functional interface. Scala
has a similar mechanism. The preceding code is syntactic sugar for declaring an anon-
ymous class of type scala. Function1 (a function of one parameter), which provides
the implementation of the method apply:

val islongtweet : string => boolean
= new function1[string, boolean] {
def apply(tweet: string): boolean = tweet. Length() > 60
}
functions 495

because the variable islongtweet holds an object of type function1, you can call the
method apply, which can be seen as calling the function:

islongtweet. Apply("a very short tweet") returns false

in java, you could do the following:
function<string, boolean> islongtweet = (string s) -> s. Length() > 60;
boolean long = islongtweet. Apply("a very short tweet");

to allow you to use lambda expressions, java provides several built-in functional inter-
faces such as predicate, function, and consumer. Scala provides traits (you can think
of traits as being interfaces for now) to achieve the same thing: function0 (a function
with 0 parameters and a return result) up to function22 (a function with 22 parame-
ters), all of which define the method apply.
Another cool trick in scala allows you to implicitly call the apply method by using
syntactic sugar that looks more like a function call:

islongtweet("a very short tweet") returns false

the compiler automatically converts a call f(a) to f. Apply(a) and, more generally, a
call f(a1, ... , an) to f. Apply(a1, ... , an), if f is an object that supports the apply
method. (note that apply can have any number of arguments. )
closures
in chapter 3, we commented on whether lambda expressions in java constitute clo-
sures. A closure is an instance of a function that can reference nonlocal variables of
that function with no restrictions. But lambda expressions in java have a restriction:
they cant modify the content of local variables of a method in which the lambda is
defined. Those variables have to be implicitly final. It helps to think that lambdas
close over values, rather than variables.
By contrast, anonymous functions in scala can capture variables themselves, not
the values to which the variables currently refer. The following is possible in scala:
def main(args: array[string]) {
var count = 0 a closure capturing and
val inc = () => count+=1 incrementing count
inc()
println(count) prints 1
inc()
println(count) prints 2
}

but in java, the following results in a compiler error because count is implicitly forced
to be final:
public static void main(string[] args) {
int count = 0; error: count must be
runnable inc = () -> count+=1; final or effectively final.
496 chapter 20 blending oop and fp: comparing java and scala

inc. Run();
system. Out. Println(count);
inc. Run();
}

we argued in chapters 7, 18, and 19 that you should avoid mutation whenever possi-
ble to make your programs easier to maintain and parallelizable, so use this feature
only when strictly necessary.

20.2.3 currying
in chapter 19, we described a technique called currying, in which a function f of two
arguments (x and y, say) is seen instead as a function g of one argument, which
returns a function also of one argument. This definition can be generalized to func-
tions with multiple arguments, producing multiple functions of one argument. In
other words, you can break down a function that takes multiple arguments into a
series of functions, each of which takes a subset of the arguments. Scala provides a
construct that makes it easy to curry an existing function.
To understand what scala brings to the table, first revisit an example in java. You
can define a simple method to multiply two integers:
static int multiply(int x, int y) {
return x * y;
}
int r = multiply(2, 10);

but this definition requires all the arguments to be passed to it. You can break down
the multiply method manually by making it return another function:
static function<integer, integer> multiplycurry(int x) {
return (integer y) -> x * y;
}

the function returned by multiplycurry captures the value of x and multiplies it by
its argument y, returning an integer. As a result, you can use multiplycurry as fol-
lows in a map to multiply each element by 2:
stream. Of(1, 3, 5, 7)
. Map(multiplycurry(2))
. Foreach(system. Out: : println);

this code produces the result 2, 6, 10, 14. This code works because map expects a
function as argument and multiplycurry returns a function.
Its a bit tedious in java to split up a function manually to create a curried form,
especially if the function has multiple arguments. Scala has a special syntax that per-
forms this automatically. You can define the normal multiply method as follows:
def multiply(x : int, y: int) = x * y
val r = multiply(2, 10)
classes and traits 497

and here is the curried form:
defining a
curried function
def multiplycurry(x : int)(y : int) = x * y
val r = multiplycurry(2)(10)
invoking a curried
function

when you use the (x: int)(y: int) syntax, the multiplycurry method takes two argu-
ment lists of one int parameter. By contrast, multiply takes one list of two int parame-
ters. What happens when you call multiplycurry? The first invocation of multiplycurry
with a single int (the parameter x), multiplycurry(2), returns another function that
takes a parameter y and multiplies it by the captured value of x (here, the value 2). We
say that this function is partially applied as explained in chapter 19, because not all argu-
ments are provided. The second invocation multiplies x and y. You can store the first
invocation to multiplycurry inside a variable and reuse it, as follows:

val multiplybytwo : int => int = multiplycurry(2)
val r = multiplybytwo(10) 20

by comparison with java, in scala you dont need to provide the curried form of a
function manually, as in the preceding example. Scala provides a convenient function-
definition syntax to indicate that a function has multiple curried argument lists.

20.3 classes and traits
in this section, we look at how classes and interfaces in java compare with those in
scala. These two constructs are paramount to design applications. Youll see that
scalas classes and interfaces can provide more flexibility than those in java.

20.3.1 less verbosity with scala classes
because scala is a full object-oriented language, you can create classes and instantiate
them to produce objects. In its most basic form, the syntax to declare and instantiate
classes is similar to that of java. Heres how to declare a hello class:
class hello {
def saythankyou(){
println("thanks for reading our book")
}
}
val h = new hello()
h. Saythankyou()

getters and setters
scala becomes more interesting when you have a class with fields. Have you ever
come across a java class that purely defines a list of fields and had to declare a long
list of getters, setters, and an appropriate constructor? What a pain! In addition,
you often see tests for the implementation of each method. A large amount of code
498 chapter 20 blending oop and fp: comparing java and scala

typically is devoted to such classes in enterprise java applications. Consider this
simple student class:
public class student {
private string name;
private int id;
public student(string name) {
this. Name = name;
}
public string getname() {
return name;
}
public void setname(string name) {
this. Name = name;
}
public int getid() {
return id;
}
public void setid(int id) {
this. Id = id;
}
}

you have to manually define the constructor that initializes all fields, two getters, and
two setters. A simple class now has more than 20 lines of code. Several ides (inte-
grated development environments) and tools can help you generate this code, but
your code base still has to deal with a large amount of additional code thats not too
useful compared with real business logic.
In scala, constructors, getters, and setters can be generated implicitly, which
results in code with less verbosity:
initialize a
class student(var name: string, var id: int) student object.
Val s = new student("raoul", 1)
println(s. Name)
get the name and
s. Id = 1337 print raoul.
Println(s. Id) set the id.
Print 1337.

In java, you could get similar behavior by defining public fields, but youd still have to
define the constructor explicitly. Scala classes save you boilerplate code.

20.3.2 scala traits vs. Java interfaces
scala has another useful feature for abstraction, called traits, which are scalas
replacement for javas interfaces. A trait can define both abstract methods and meth-
ods with a default implementation. Traits can also be multiply inherited like interfaces
in java, so you can see them as being similar to java interfaces that support default
methods. Traits can also contain fields such as abstract classes, which java interfaces
dont support. Are traits like abstract classes? No, because unlike abstract classes, traits
can be multiply inherited. Java has always had multiple inheritance of types because a
summary 499

class can implement multiple interfaces. Java 8, through default methods, introduced
multiple inheritance of behaviors but still doesnt allow multiple inheritance of state
something permitted by scala traits.
To see what a trait looks like in scala, define a trait called sized that contains one
mutable field called size and one method called isempty with a default implementation:
trait sized {
a field called size
var size : int = 0
def isempty() = size == 0
a method called isempty with
} a default implementation

you can compose this code at declaration time with a class, such as an empty class that
always has size 0:
a class inheriting from
the trait sized
class empty extends sized
println(new empty(). Isempty()) prints true

interestingly, compared with java interfaces, traits can be composed at object instanti-
ation time (but this operation is still a compile-time operation). You can create a box
class and decide that one specific instance should support the operations defined by
the trait sized, as follows:
composing the trait at
class box object instantiation time
val b1 = new box() with sized
println(b1. Isempty()) prints true
val b2 = new box()
b2. Isempty()
compile error: the box class declaration
doesnt inherit from sized.

What happens if multiple traits are inherited, declaring methods with the same signa-
tures or fields with the same names? Scala provides restriction rules similar to those
that apply to default methods (chapter 13).

Summary
 java and scala combine object-oriented and functional-programming features
into one programming language; both run on the jvm and to a large extent
can interoperate.
 Scala supports collection abstractions similar to those in javalist, set, map,
stream, optionbut also supports tuples.
 Scala provides richer features that support more functions than java does.
These features include function types, closures that have no restrictions on
accessing local variables, and built-in currying forms.
 Classes in scala can provide implicit constructors, getters, and setters.
 Scala supports traits, which are interfaces that can include fields and default
methods.
Conclusions and
where next for java

this chapter covers
 the new java 8 features and their evolutionary
effect on programming style
 the new java 9 module system
 the new six-monthly java incremental-release
life cycle
 the first incremental release forming java 10
 a few ideas that youll likely see implemented
in some future version of java

weve covered a lot of material in this book, and we hope that you feel that youre
ready to start using the new java 8 and 9 features in your own code, perhaps build-
ing on our examples and quizzes. In this chapter, we review the journey of learning
about java 8 and the gentle push toward functional-style programming, as well as
the advantages of the new modularization capability and other minor improve-
ments introduced with java 9. You also learn about what is included in java 10. In
addition, we speculate on what future enhancements and great new features may
be in javas pipeline beyond java 9, 10, 11, and 12.

500
review of java 8 features 501

21.1 review of java 8 features
a good way to help you understand java 8 as a practical, useful language is to revisit
the features in turn. Instead of simply listing them, wed like to present them as being
interlinked to help you understand them not only as a set of features, but also as a
high-level overview of the coherent language design that is java 8. Our other aim in
this review chapter is to emphasize how most of the new features in java 8 are facilitat-
ing functional-style programming in java. Remember, supporting function program-
ming wasnt a capricious design choice but a conscious design strategy centered on
two trends, which we regard as climate change in the model from chapter 1:
 the increasing need to exploit the power of multicore processors now that, for
silicon technology reasons, the additional transistors annually provided by moores
law no longer translate into higher clock speeds of individual cpu cores. Put
simply, making your code run faster requires parallel code.
 The increasing tendency to concisely manipulate collections of data with a
declarative style for processing data, such as taking some data source, extracting
all data that matches a given criterion, and applying some operation to the
result (summarizing it or making a collection of the result for further process-
ing later). This style is associated with the use of immutable objects and collec-
tions, which are then processed to produce further immutable values.

Neither motivation is effectively supported by the traditional, object-oriented, impera-
tive approach, which centers on mutating fields and applying iterators. Mutating data
on one core and reading it from another is surprisingly expensive, not to mention
that it introduces the need for error-prone locking. Similarly, when youre focused on
iterating over and mutating existing objects, the stream-like programming idiom can
feel alien. But these two trends are supported by ideas from functional programming,
which explains why the java 8 center of gravity has moved a bit from what youve come
to expect from java.
This chapter reviews, in a big-picture unifying view, what youve learned from this
book and shows you how everything fits together in the new climate.

21.1.1 behavior parameterization (lambdas and method references)
to write a reusable method such as filter, you need to specify as its argument a
description of the filtering criterion. Although java experts could achieve this task in
earlier versions of java by wrapping the filtering criterion as a method inside a class
and passing an instance of that class, this solution was unsuitable for general use
because it was too cumbersome to write and maintain.
As you discovered in chapters 2 and 3, java 8 provides a way, borrowed from func-
tional programming, to pass a piece of code to a method. Java conveniently provides
two variants:
502 chapter 21 conclusions and where next for java

 passing a lambda (a one-off piece of code) such as

apple -> apple. Getweight() > 150

 passing a method reference to an existing method, such as apple: : isheavy

these values have types such as function<t, r>, predicate<t>, and bifunction<t,
u, r>, and the recipient can execute them by using the methods apply, test, and so
on. These types are called functional interfaces and have a single abstract method, as
you learned in chapter 3. Of themselves, lambdas can seem to be rather a niche con-
cept, but the way that java 8 uses them in much of the new streams api propels them
to the center of java.

21.1.2 streams
the collection classes in java, along with iterators and the for-each construct, have
served programmers honorably for a long time. It would have been easy for the java 8
designers to add methods such as filter and map to collections, exploiting lambdas
to express database-like queries. Instead, the designers they added a new streams api,
which is the subject of chapters 47, and its worth pausing to consider why.
Whats wrong with collections that requires them to be replaced or augmented
by a similar notion of streams? Well summarize this way: if you have a large collection
and apply three operations to it (perhaps mapping the objects in the collection to
sum two of their fields, filtering the sums that satisfy some criterion, and sorting the
result), you make three separate traversals of the collection. Instead, the streams api
lazily forms these operations into a pipeline and does a single stream traversal per-
forming all the operations together. This process is much more efficient for large
datasets, and for reasons such as memory caches, the larger the dataset, the more
important it is to minimize the number of traversals.
The other, no less important, reason concerns processing elements in parallel,
which is vital to the efficient exploitation of multicore cpus. Streams, and in particu-
lar the parallel method, allow a stream to be marked as suitable for parallel process-
ing. Recall that parallelism and mutable state fit badly together, so core functional
concepts (side-effect-free operations and methods parameterized with lambdas and
method references that permit internal iteration instead of external iteration, as dis-
cussed in chapter 4) are central to exploiting streams in parallel by using map, filter,
and the like.
In the following section, we look at how these ideas, which we introduced in terms
of streams, have a direct analog in the design of completablefuture.

21.1.3 completablefuture
java has provided the future interface since java 5. Futures are useful for exploiting
multicore because they allow a task to be spawned onto another thread or core and
allow the spawning task to continue executing along with the spawned task. When the
review of java 8 features 503

spawning task needs the result, it can use the get method to wait for the future to
complete (produce its value).
Chapter 16 explains the java 8 completablefuture implementation of future,
which again exploits lambdas. A useful, if slightly imprecise, motto is completable-
future is to future as stream is to collection. To compare:
 stream lets you pipeline operations and provides behavior parameterization
with map, filter, and the like, eliminating the boilerplate code that you typi-
cally have to write when you use iterators.
 Completablefuture provides operations such as thencompose, thencombine,
and allof, which provide functional-programming-style concise encodings of
common design patterns involving futures and let you avoid similar imperative-
style boilerplate code.
This style of operations, albeit in a simpler scenario, also applies to the java 8 opera-
tions on optional, which we revisit in the next section.

21.1.4 optional
the java 8 library provides the class optional<t>, which allows your code to specify
that a value is a proper value of type t or a missing value returned by the static method
optional. Empty. This feature is great for program comprehension and documenta-
tion. It provides a data type with an explicit missing value instead of the previous
error-prone use of the null pointer to indicate missing values, which programmers
could never be sure was a planned missing value or an accidental null resulting from
an earlier erroneous computation.
As discussed in chapter 11, if optional<t> is used consistently, programs should
never produce nullpointerexceptions. Again, you could see this situation as a one-
off, unrelated to the rest of java 8, and ask, how does changing from one form of
missing value to another help me write programs? Closer inspection shows that the
optional<t> class provides map, filter, and ifpresent. These methods have behav-
ior similar to that of corresponding methods in the streams class and can be used to
chain computations, again in functional style, with the tests for missing value done by
the library instead of user code. The choice of internal versus external testing in
optional<t> is directly analogous to how the streams library does internal versus
external iteration in user code. Java 9 added various new methods to the optional
api, including stream(), or(), and ifpresentorelse().

21.1.5 flow api
java 9 standardized reactive streams and the reactive-pull-based backpressure proto-
col, a mechanism designed to prevent a slow consumer from being overwhelmed by
one or more faster producers. The flow api includes four core interfaces that library
implementations can support to provide wider compatibility: publisher, subscriber,
subscription, and processor.
504 chapter 21 conclusions and where next for java

our final topic in this section concerns not functional-style programming, but java 8
support for upward-compatible library extensions driven by software-engineering desires.

21.1.6 default methods
java 8 has other additions, none of which particularly affects the expressiveness of any
program. But one thing thats helpful to library designers allows default methods to
be added to an interface. Before java 8, interfaces defined method signatures; now
they can also provide default implementations for methods that the interface
designer suspects not all clients will want to provide explicitly.
This tool is a great new tool for library designers because it gives them the ability to
augment an interface with a new operation without having to require all clients
(classes implementing this interface) to add code to define this method. Therefore,
default methods are also relevant to users of libraries because they shield the users
from future interface changes (see chapter 13).

21.2 the java 9 module system
java 8 added a lot, both in terms of new features (lambdas and default methods on
interfaces, for example) and new useful classes in the native api, such as stream and
completablefuture. Java 9 didnt introduce any new language features but mostly
polished the work started in java 8, completing the classes introduced there with some
useful methods such as takewhile and dropwhile on a stream and completeon-
timeout on a completablefuture. In fact, the main focus of java 9 was the introduc-
tion of the new module system. This new system doesnt affect the language except for
the new module-info. Java file, but nevertheless improves the way in which you
design and write applications from an architectural point of view, clearly marking the
boundaries of subparts and defining how they interact.
Java 9, unfortunately, harmed the backward compatibility of java more than any
other release (try compiling a large java 8 code base with java 9). But this cost is worth
paying for the benefits of proper modularization. One reason is to ensure better and
stronger encapsulation across packages. In fact, java visibility modifiers are designed to
define encapsulation among methods and classes, but across packages, only one visibil-
ity is possible: public. This lack makes it hard to modularize a system properly, in partic-
ular to specify which parts of a module are designed for public use and which parts are
implementation details that should be hidden from other modules and applications.
The second reason, which is an immediate consequence of the weak encapsulation
across packages, is that without a proper module system, its impossible to avoid
exposing functionalities that are relevant for security of all the code running in the
same environment. Malicious code may access critical parts of your module, thus
bypassing all security measures encoded in them.
Finally, the new java module system enables the java runtime to be split into
smaller parts, so you can use only the parts that are necessary for your application. It
would be surprising if corba was a requirement for your new java project, for example,
java 10 local variable type inference 505

but its likely to be included in all your java applications. Although this act may be of
limited relevance for traditional-size computing devices, its important for embedded
appliances and for the increasingly frequent situation in which your java applications
run in a containerized environment. In other words, the java module system is an
enabler that allows the use of the java runtime in internet of things (iot) applications
and in the cloud.
As discussed in chapter 14, the java module system solves these problems by intro-
ducing a language-level mechanism to modularize your large systems and the java
runtime itself. The advantages of the java module system include the following:
 reliable configurationexplicitly declaring module requirements allows early
detection of errors at build time rather than at runtime in the case of missing,
conflicting, or circular dependencies.
 Strong encapsulationthe java module system enables modules to export only
specific packages and then separate the public and accessible boundaries of
each module with internal implementation.
 Improved securitynot allowing users to invoke specific parts of your module
makes it much harder for an attacker to evade the security controls imple-
mented in them.
 Better performancemany optimization techniques can be more effective when a
class can refer to few components rather than to any other classes loaded by the
runtime.
 Scalabilitythe java module system allows the java se platform to be decom-
posed into smaller parts containing only the features required by the running
application.

In general, modularization is a hard topic, and its unlikely to be a driver for quick
adoption of java 9 as lambdas were for java 8. We believe, however, that in the long
run, the effort you invest in modularizing your application will be repaid in terms of
easier maintainability.
So far, weve summarized the concepts of java 8 and 9 covered in this book. In the
next section, we turn to the thornier subject of future enhancements and great fea-
tures that may be in javas pipeline beyond java 9.

21.3 java 10 local variable type inference
originally in java, whenever you introduced a variable or method, you gave its type at
the same time. The example

double convertusdtogbp(double money) { exchangerate e = ... ; }

contains three types, which give the result the type of convertusdtogbp, the type of its
argument money, and the type of its local variable e. Over time, this requirement has
506 chapter 21 conclusions and where next for java

been relaxed in two ways. First, you may omit type parameters of generics in an
expression when the context determines them. This example

map<string, list<string>> mymap = new hashmap<string, list<string>>();

can be abbreviated to the following since java 7:

map<string, list<string>> mymap = new hashmap<>();

second, to use the same idea of propagating the type determined by context into an
expression, a lambda expression such as

function<integer, boolean> p = (integer x) -> booleanexpression;

can be shortened to

function<integer, boolean> p = x -> booleanexpression;

by omitting types. In both cases, the compiler infers the omitted types.
Type inference has a few advantages when a type consists of a single identifier, the
main one being reduced editing work when replacing one type with another. But as
types grow in size, generics parameterized by further generic types, type inference can
aid readability.1 the scala and c# languages permit a type in a local-variable-initialized
declaration to be replaced by the (restricted) keyword var; the compiler fills in the
appropriate type from the right side. The declaration of mymap shown earlier in java
syntax could be rendered like this:

var mymap = new hashmap<string, list<string>>();

this idea is called local variable type inference and is included in java 10.
Theres some small cause for concern, however. Given a class car that subclasses a
class vehicle, does the declaration

var x = new car();

implicitly declare x to have type car or vehicle (or even object)? In this case, a sim-
ple explanation that the missing type is the type of the initializer (here, car) is per-
fectly clear. Java 10 formalizes this fact, also stating that var cant be used when theres
no initializer.

1
its important that type inference be done sensibly, of course. Type inference works best when you have only
one way, or one easily documentable way, to re-create the type that the user omitted. Problems occur if the
system infers a different type from the one that the user was thinking of. So a good design of type inference
produces a fault when two incomparable types could be inferred; heuristics can give the appearance of pick-
ing the wrong one seemingly at random.
Whats ahead for java? 507

21.4 whats ahead for java?
Some of the points we cover in this section are discussed in more detail on the jdk
enhancement proposal website at http: //openjdk. Java. Net/jeps/0. Here, we take care
to explain why seemingly sensible ideas have subtle difficulties or interactions with
existing features that inhibit their direct incorporation into java.

21.4.1 declaration-site variance
java supports wildcards as flexible mechanisms that allow subtyping for generics (gener-
ally referred to as use-site variance). This support makes the following assignment valid:

list<? Extends number> numbers = new arraylist<integer>();

but the following assignment, omitting the "? Extends", produces a compile-time error:

list<number> numbers = new arraylist<integer>(); incompatible
types

many programming languages, such as c# and scala, support a different variance
mechanism called declaration-site variance. These languages allow programmers to
specify variance when defining a generic class. This feature is useful for classes that are
inherently variant. Iterator, for example, is inherently covariant, and comparator is
inherently contravariant, and you shouldnt need to think in terms of ? Extends or ?
Super when you use them. Adding declaration-site variance to java would be useful,
because these specifications appear instead at the declaration of classes. As a result,
this addition would reduce some cognitive overhead for programmers. Note that at
the writing (2018), a jdk enhancement proposal would allow default declaration-site
variance in upcoming versions of java (http: //openjdk. Java. Net/jeps/300).

21.4.2 pattern matching
as we discussed in chapter 19, functional-style languages typically provide some form
of pattern matchingan enhanced form of switchin which you can ask, is this
value an instance of a given class? And (optionally) recursively ask whether its fields
have certain values. In java a simple case test looks like this:
if (op instanceof binop){
expr e = ((binop) op). Getleft();
}

note that you have to repeat the type binop within the cast expression, even though
its clear that the object referenced by op is of that type.
You may have a complicated hierarchy of expressions to process, of course, and the
approach of chaining multiple if conditions will make your code more verbose. Its
worth reminding you that traditional object-oriented design discourages the use of
switch and encourages patterns such as the visitor pattern, in which data-type-
dependent control flow is done by method dispatch instead of by switch. At the other
508 chapter 21 conclusions and where next for java

end of the programming language spectrum, in functional-style programming, pat-
tern matching over values of data types is often the most convenient way to design
a program.
Adding scala-style pattern matching in full generality to java seems to be a big job,
but following the recent generalization to switch to allow strings, you can imagine a
more modest syntax extension that allows switch to operate on objects by using the
instanceof syntax. In fact, a jdk enhanced proposal explores pattern matching as a
language feature for java (http: //openjdk. Java. Net/jeps/305). The following example
revisits our example from chapter 19 and assumes a class expr, which is subclassed
into binop and number:
switch (someexpr) {
case (op instanceof binop):
dosomething(op. Getopname(), op. Getleft(), op. Getright());
case (n instanceof number):
dealwithleafnode(n. Getvalue());
default:
defaultaction(someexpr);
}

notice a couple of things. First, this code steals from pattern matching the idea that
in case (op instanceof binop): , op is a new local variable (of type binop), which
becomes bound to the same value as someexpr. Similarly, in the number case, n
becomes a variable of type number. In the default case, no variable is bound. This pro-
posal eliminates much boilerplate code compared with using chains of if-then-else
and casting to subtype. A traditional object-oriented designer probably would argue
that such data-type dispatch code would better be expressed with visitor-style methods
overridden in subtypes, but to functional-programming eyes, this solution results in
related code being scattered over several class definitions. This classical design dichot-
omy is discussed in the literature as the expression problem.2

21.4.3 richer forms of generics
this section discusses two limitations of java generics and looks at a possible evolution
to mitigate them.
Reified generics
when generics were introduced in java 5, they had to be backward-compatible with the
existing jvm. To this end, the runtime representations of arraylist<string> and
arraylist<integer> are identical. This model is called the erasure model of generic polymor-
phism. Certain small runtime costs are associated with this choice, but the most significant
effect for programmers is that parameters of generic types can be only objects and not
primitive types. Suppose that java allowed, say, arraylist<int>. Then you could allocate
an arraylist object on the heap containing a primitive value such as int 42, but the

2
for a more complete explanation, see http: //en. Wikipedia. Org/wiki/expression_problem.
Whats ahead for java? 509

arraylist container wouldnt contain any indicator of whether it contained an object
value such as a string or a primitive int value such as 42.
At some level, this situation seems to be harmless. If you get a primitive 42 from
an arraylist<int> and a string object "abc" from an arraylist<string>, why
should you worry that the arraylist containers are indistinguishable? Unfortu-
nately, the answer is garbage collection, because the absence of runtime type infor-
mation about the contents of the arraylist would leave the jvm unable to
determine whether element 13 of your arraylist was a string reference (to be fol-
lowed and marked as in use by garbage collection) or an int primitive value (most
definitely not to be followed).
In the c# language, the runtime representations of arraylist<string>, array-
list<integer>, and arraylist<int> are in principle different. But even if these
representations were the same, sufficient type information is kept at runtime to
allow, for example, garbage collection to determine whether a field is a reference or
a primitive. This model is called the reified model of generic polymorphism or, more sim-
ply, reified generics. The word reification means making explicit something that other-
wise would be implicit.
Reified generics are clearly desirable because they enable a more full unification
of primitive types and their corresponding object typessomething that youll see as
problematic in the following sections. The main difficulty for java is backward compat-
ibility, both in the jvm and in existing programs that use reflection and expect gener-
ics to be erased.
Additional syntactic flexibility in generics for function types
generics proved to be a wonderful feature when they were added to java 5. Theyre
also fine for expressing the type of many java 8 lambdas and method references. You
can express a one-argument function this way:

function<integer, integer> square = x -> x * x;

if you have a two-argument function, you use the type bifunction<t, u, r>, where t
is the type of the first parameter, u the second, and r the result. But theres no tri-
function unless you declare it yourself.
Similarly, you cant use function<t, r> for references to methods that take zero
arguments and return result type r; you have to use supplier<r> instead.
In essence, java 8 lambdas have enriched what you can write, but the type system
hasnt kept up with the flexibility of the code. In many functional languages, you can
write, for example, the type (integer, double) => string, to represent what java 8
calls bifunction<integer, double, string>, along with integer => string to represent
function<integer, string>, and even () => string to represent supplier<string>.
You can understand => as an infix version of function, bifunction, supplier, and
the like. A simple extension to java syntax for types to allow infix => would result in
more readable types analogous to what scala provides, as discussed in chapter 20.
510 chapter 21 conclusions and where next for java

primitive specializations and generics
in java, all primitive types (int, for example) have a corresponding object type (here,
java. Lang. Integer). Often, programmers refer to these types as unboxed and boxed.
Although this distinction has the laudable aim of increasing runtime efficiency, the
types can become confusing. Why, for example, do you write predicate<apple> instead
of function<apple, boolean> in java 8? An object of type predicate<apple>, when
called by the test method, returns a primitive boolean.
By contrast, like all java generics, a function can be parameterized only by object
types. In the case of function<apple, boolean>, this is the object type boolean, not
the primitive type boolean. Predicate<apple> is more efficient because it avoids box-
ing the boolean to make a boolean. This issue has led to the creation of multiple sim-
ilar interfaces such as longtointfunction and booleansupplier, which add further
conceptual overload.
Another example concerns the differences between void, which can qualify only
method return types and has no values, and the object type void, which has null as its
only value (a question that regularly appears in forums). The special cases of function
such as supplier<t>, which could be written () => t in the new notation proposed in
the previous section, further attest to the ramifications caused by the distinction
between primitive and object types. We discussed earlier how reified generics could
address many of these issues.

21.4.4 deeper support for immutability
some expert readers may have been a little upset when we said that java 8 has three
forms of values:
 primitive values
 (references to) objects
 (references to) functions

at one level, were going to stick to our guns and say, but these are the values that a
method may now take as arguments and return as results. But we also want to con-
cede that this explanation is a little problematic. To what extent do you return a
(mathematical) value when you return a reference to a mutable array? A string or an
immutable array clearly is a value, but the case is far less clear-cut for a mutable object
or array. Your method may return an array with its elements in ascending order, but
some other code may change one of its elements later.
If youre interested in functional-style programming in java, you need linguistic
support for saying immutable value. As noted in chapter 18, the keyword final
doesnt achieve this purpose; it only stops the field that it qualifies from being
updated. Consider this example:

final int[] arr = {1, 2, 3};
final list<t> list = new arraylist<>();
whats ahead for java? 511

the first line forbids another assignment arr = ... But doesnt forbid arr[1] = 2; the
second line forbids assignments to list but doesnt forbid other methods from
changing the number of elements in list. The keyword final works well for primi-
tive values, but for references to objects, it often produces a false sense of security.
What were leading up to is this: given that functional-style programming puts
strong emphasis on not mutating existing structure, a strong argument exists for a
keyword such as transitively_final, which can qualify fields of reference type and
ensure that no modification can take place in the field or any object directly or indi-
rectly accessible via that field.
Such types represent one intuition about values: values are immutable, and only
variables (which contain values) may be mutated to contain a different immutable
value. As we remarked at the beginning of this section, java authors (including us)
sometimes inconsistently talk about the possibility of a java values being a mutable
array. In the next section, we return to proper intuition and discuss the idea of value
types, which can contain only immutable values even if variables of value types can still
be updated unless theyre qualified with final.

21.4.5 value types
in this section, we discuss the difference between primitive types and object types, fol-
lowing up on the discussion of the desire for value types, which help you write pro-
grams functionally, as object types are necessary for object-oriented programming.
Many of the issues we discuss are related, so theres no easy way to explain one prob-
lem in isolation. Instead, we identify the problem by its various facets.
Cant the compiler treat integer and int identically?
Given all the implicit boxing and unboxing that java has slowly acquired since
java 1.1, you might ask whether its time for java to treat, for example, integer and
int identically and to rely on the java compiler to optimize into the best form for
the jvm.
This idea is wonderful in principle, but consider the problems surrounding adding
the type complex to java to see why boxing is problematic. The type complex, which
models so-called complex numbers with real and imaginary parts, is naturally intro-
duced as follows:
class complex {
public final double re;
public final double im;
public complex(double re, double im) {
this. Re = re;
this. Im = im;
}
public static complex add(complex a, complex b) {
return new complex(a. Re+b. Re, a. Im+b. Im);
}
}
512 chapter 21 conclusions and where next for java

but values of type complex are reference types, and every operation on complex needs
to do an object allocation, dwarfing the cost of the two additions in add. Programmers
need a primitive-type analog of complex, perhaps called complex.
The issue is that programmers want an unboxed object, for which neither java nor
the jvm offers any real support. You can return to the lament oh, but surely the com-
piler can optimize this. Sadly, this process is much harder than it appears; although a
compiler optimization based on so-called escape analysis can sometimes determine
that unboxing is okay, its applicability is limited by javas assumptions of objects,
which have been present since java 1.1. Consider the following puzzler:
double d1 = 3.14;
double d2 = d1;
double o1 = d1;
double o2 = d2;
double ox = o1;
system. Out. Println(d1 == d2 ? "yes" : "no");
system. Out. Println(o1 == o2 ? "yes" : "no");
system. Out. Println(o1 == ox ? "yes" : "no");

the result is yes, no, yes. An expert java programmer probably would say, what
silly code. Everyone knows you should use equals on the last two lines instead of ==.
But well persist. Even though all these primitives and objects contain the immutable
value 3.14 and should be indistinguishable, the definitions of o1 and o2 create new
objects, and the == operator (identity comparison) can tell them apart. Note that on
primitives, the identity comparison does bitwise comparison, but on objects, it does
reference equality. Often, you accidentally create a new distinct double object, which
the compiler needs to respect because the semantics of object, from which double
inherits, require this. Youve seen this discussion before, both in the discussion of
value types in this chapter and in chapter 19, where we discussed referential transpar-
ency of methods that functionally update persistent data structures.
Value types: not everything is a primitive or an object
we suggest that the resolution of this problem is to rework the java assumptions that
(1) everything that isnt a primitive is an object and hence inherits object and (2) all
references are references to objects.
The development starts this way. Values take two forms:
 object types that have mutable fields unless forbidden with final and that also
have identity, which may be tested with ==.
 Value types, which are immutable and dont have reference identity. Primitive
types are a subset of this wider notion.
You could allow user-defined value types (perhaps starting with a lowercase letter to
emphasize their similarity to primitive types such as int and boolean). On value types,
== would, by default, perform an element-by-element comparison in the same way
that hardware comparison on int performs a bit-by-bit comparison. We need to be
careful for floating-point members as comparison is a somewhat more sophisticated
whats ahead for java? 513

operation. The type complex would be a perfect example of a nonprimitive value type;
such types resemble c# structs.
In addition, value types can reduce storage requirements because they dont
have reference identity. Figure 21.1 illustrates an array of size three, whose elements
0, 1, and 2 are light gray, white, and dark gray, respectively. The left diagram shows a
typical storage requirement when pair and complex are objects, and the right

objects value types

complex[] complex[]

2.4 2.4

-0.1 -0.1

5.2
5.2
1.1
1.1
-1.8
-1.8 0.7
0.7

pair<complex, complex>[] pair<complex, complex>[]

2.4 2.4

-0.1 -0.1

1.4
-1.8
-0.3
0.7
5.2
1.4 1.1
-0.3 8.4

8.4 2.0

2.0 -1.8

0.7
5.4
5.4
3.0
3.0

5.2

1.1

figure 21.1 objects versus value types
514 chapter 21 conclusions and where next for java

diagram shows the better layout when pair and complex are value types. Note that
we call them pair and complex in lowercase in the diagram to emphasize their simi-
larity to primitive types. Note also that value types are likely to produce better per-
formance, not only for data access (multiple levels of pointer indirection replaced
by a single indexed-addressing instruction), but also for hardware cache use (due to
data contiguity).
Note that because value types dont have reference identity, the compiler can box
and unbox them at its choice. If you pass a complex as argument from one function to
another, the compiler can naturally pass it as two separate doubles. (returning it with-
out boxing is trickier in the jvm, of course, because the jvm provides only method-
return instructions for passing values representable in a 64-bit machine register. ) but
if you pass a larger value type as an argument (perhaps a large immutable array), the
compiler can instead, transparently to the user, pass it as a reference when it has been
boxed. Similar technology already exists in c#. Microsoft says (https: //docs. Microsoft
. Com/en-us/dotnet/csharp/language-reference/keywords/value-types):
variables that are based on value types directly contain values. Assigning one value type
variable to another copies the contained value. This differs from the assignment of
reference type variables, which copies a reference to the object but not the object itself.

At the time of writing (2018), a jdk enhancement proposal is pending for value types
in java (http: //openjdk. Java. Net/jeps/169).
Boxing, generics, value types: the interdependency problem
wed like to have value types in java because functional-style programs deal with
immutable values that dont have identity. Wed like to see primitive types as a special
case of value types, but the erasure model of generics, which java currently has, means
that value types cant be used with generics without boxing. Object (boxed) versions
(such as integer) of primitive types (such as int) continue to be vital for collections
and java generics because of their erasure model, but now their inheriting object
(and, hence, reference equality) is seen as a drawback. Addressing any of these prob-
lems means addressing them all.

21.5 moving java forward faster
there have been ten major releases of java in 22 yearsan average of more than two
years between releases. In some cases, the wait was five years. The java architects real-
ized that this situation is no longer sustainable because it doesnt evolve the language
fast enough and is the main reason why emerging languages on the jvm (such as
scala and kotlin) are creating a huge feature gap for java. Such a long release cycle is
arguably reasonable for huge and revolutionary features such as lambdas and the java
module system, but it also implies that minor improvements have to wait, for no valid
reason, for the complete implementation of one of those big changes before being
incorporated into the language. The collection factory methods discussed in chapter
8, for example, were ready to ship long before the java 9 module system was finalized.
The final word 515

for these reasons, it has been decided that from now on, java will have a six-month
development cycle. In other words, a new major version of java and the jvm will
appear every six months, with java 10 released in march 2018 and java 11 due in sep-
tember 2018. The java architects also realized that although this faster development
cycle is beneficial for the language itself, and also for agile companies and developers
who are used to constantly experimenting with new technologies, it could be prob-
lematic for more conservative organizations, which generally update their software at
a slower pace. For that reason, the java architects also decided that every three years,
therell be a long-term support (lts) release that will be supported for the subse-
quent three years. Java 9 isnt an lts release, so its considered to be at the end of its
life now java 10 is out. The same thing will happen with java 10. Java 11, by contrast,
will be an lts version, with release planned for september 2018 and supported until
september 2021. Figure 21.2 shows the life cycle of the java versions that are planned
for released in the next few years.

9

10

11

12

13

14

15

16

17

2017 2018 2019 2020 2021 2022 2023 2024 2025

figure 21.2 the life cycle of future java releases

we strongly empathize with the decision to give java a shorter development cycle, espe-
cially nowadays, when all software systems and languages are meant to improve as
quickly as possible. A shorter development cycle enables java to evolve at the right
speed and allows the language to remain relevant and appropriate in the coming years.

21.6 the final word
this book explored the main new features added by java 8 and 9. Java 8 represents
perhaps the biggest evolution step ever taken by java. The only comparably large evo-
lution step was the introduction, a decade previously (in 2005), of generics in java 5.
The most characteristic feature of java 9 is the introduction of the long-awaited module
516 chapter 21 conclusions and where next for java

system, which is likely to be more interesting for software architects than to develop-
ers. Java 9 also embraced reactive streams by standardizing its protocol through the
flow api. Java 10 introduces local-variable type inference, which is a popular feature
in other programming languages to help productivity. Java 11 allows the var syntax of
local-variable type inference to be used in the list of parameters of an implicitly typed
lambda expression. Perhaps more importantly, java 11 embraces the concurrency and
reactive programming ideas discussed in this book and brings a new asynchronous
http client library that fully adopts completablefutures. Finally, at the time of writ-
ing, java 12 was announced to support an enhanced switch construct that can be used
as an expression instead of just a statementa key feature of functional programming
languages. In fact, switch expressions pave the way for the introduction of pattern
matching in java, which we discussed in section 21.4.2. All these language updates
show that functional programming ideas and influence will continue to make their
way into java in the future!
In this chapter, we looked at pressures for further java evolution. In conclusion, we
propose the following statement:
java 8, 9, 10, and 11 are excellent places to pause but not to stop!

We hope that youve enjoyed this learning adventure with us and that weve sparked
your interest in exploring the further evolution of java.
Appendix a
miscellaneous
language updates
in this appendix, we discuss three other language updates in java 8: repeated anno-
tations, type annotations, and generalized target-type inference. Appendix b dis-
cusses library updates in java 8. We dont discuss jdk 8 updates such as nashorn
and compact profiles because theyre new jvm features. This book focuses on
library and language updates. We invite you to read the following links if youre
interested in nashorn and compact profiles: http: //openjdk. Java. Net/projects/
nashorn/ and http: //openjdk. Java. Net/jeps/161.

A.1 annotations
the annotation mechanism in java 8 has been enhanced in two ways:
 you can repeat annotations.
 You can annotate any type uses.
Before we explain these updates, its worth quickly refreshing what you can do with
annotations before java 8.
Annotations in java are a mechanism that lets you decorate program elements
with additional information (note that prior to java 8 only declarations can be
annotated). In other words, its a form of syntactic metadata. For example, annota-
tions are popular with the junit framework. In the following code, the method
setup is annotated with the annotation @before, and the method testalgorithm is
annotated with @test:
@before
public void setup(){
this. List = new arraylist<>();
}

517
518 appendix a miscellaneous language updates

@test
public void testalgorithm(){
...
Assertequals(5, list. Size());
}

annotations are suitable for several use cases:
 in the context of junit, annotations can differentiate methods that should be
run as a unit test and methods that are used for setup work.
 Annotations can be used for documentation. For instance, the @deprecated
annotation is used to indicate that a method should no longer be used.
 The java compiler can also process annotations in order to detect errors, sup-
press warnings, or generate code.
 Annotations are popular in java ee, where theyre used to configure enterprise
applications.

A.1.1 repeated annotations
previous versions of java forbid more than one annotation of a given annotation type
to be specified on a declaration. For this reason, the following code is invalid:
@interface author { string name(); } error:
@author(name="raoul") @author(name="mario") @author(name="alan") duplicate
class book{ } annotation
java ee programmers often make use of an idiom to circumvent this restriction. You
declare a new annotation, which contains an array of the annotation you want to
repeat. It looks like this:
@interface author { string name(); }
@interface authors {
author[] value();
}
@authors(
{ @author(name="raoul"), @author(name="mario") , @author(name="alan")}
)
class book{}

the nested annotation on the book class is pretty ugly. This is why java 8 essentially
removes this restriction, which tidies things a bit. Youre now allowed to specify multi-
ple annotations of the same annotation type on a declaration, provided they stipulate
that the annotation is repeatable. Its not the default behavior; you have to explicitly
ask for an annotation to be repeatable.
Making an annotation repeatable
if an annotation has been designed to be repeatable, you can just use it. But if youre
providing annotations for your users, then setup is required to specify that an annota-
tion can be repeated. There are two steps:
1 mark the annotation as @repeatable.
2 provide a container annotation.
Annotations 519

heres how you can make the @author annotation repeatable:
@repeatable(authors. Class)
@interface author { string name(); }
@interface authors {
author[] value();
}

as a result, the book class can be annotated with multiple @author annotations:
@author(name="raoul") @author(name="mario") @author(name="alan")
class book{ }

at compile time book is considered to be annotated by @authors({ @author(name=
"raoul"), @author(name="mario"), @author(name="alan")}), so you can view this
new mechanism as syntactic sugar around the previous idiom used by java program-
mers. Annotations are still wrapped in a container to ensure behavioral compatibility
with legacy reflection methods. The method getannotation(class<t> annotation-
class) in the java api returns the annotation of type t for an annotated element.
Which annotation should this method return if there are several annotations of type t?
Without diving into too much detail, the class class supports a new get-
annotationsbytype method that facilitates working with repeatable annotations.
For example, you can use it as follows to print all the author annotations on the
book class:
retrieve an array consisting of the
repeatable author annotations
public static void main(string[] args) {
author[] authors = book. Class. Getannotationsbytype(author. Class);
arrays. Aslist(authors). Foreach(a -> { system. Out. Println(a. Name()); });
}

for this to work, both the repeatable annotation and its container must have a runtime
retention policy. More information about compatibility with legacy reflection meth-
ods can be found here: http: //cr. Openjdk. Java. Net/~abuckley/8misc. Pdf.

A.1.2 type annotations
as of java 8, annotations can be also applied to any type uses. This includes the new
operator, type casts, instanceof checks, generic type arguments, and implements and
throws clauses. Here we indicate that the variable name of type string cant be null
using a @nonnull annotation:
@nonnull string name = person. Getname();

similarly, you can annotate the type of the elements in a list:
list<@nonnull car> cars = new arraylist<>();

why is this interesting? Annotations on types can be useful to perform program analy-
sis. In these two examples, a tool could ensure that getname doesnt return null and
that the elements of the list of cars are always non-null. This can help reduce unex-
pected errors in your code.
520 appendix a miscellaneous language updates

java 8 doesnt provide official annotations or a tool to use them out of the box. It
provides only the ability to use annotations on types. Luckily, a tool called the checker
framework exists, which defines several type annotations and lets you enhance type
checking using them. If youre curious, we invite you to take a look at its tutorial:
http: //www. Checkerframework. Org. More information about where you can use anno-
tations in your code can be found here: http: //docs. Oracle. Com/javase/specs/jls/se8/
html/jls-9. Html#jls-9.7.4.

A.2 generalized target-type inference
java 8 enhances the inference of generic arguments. Youre already familiar with type
inference using context information before java 8. For example, the method empty-
list in java is defined as follows:
static <t> list<t> emptylist();

the method emptylist is parameterized with the type parameter t. You can call it as
follows to provide an explicit type to the type parameter:
list<car> cars = collections. <car>emptylist();

but java is capable of inferring the generic argument. The following is equivalent:
list<car> cars = collections. Emptylist();

before java 8, this inference mechanism based on the context (that is, target typing)
was limited. For example, the following wasnt possible:
static void cleancars(list<car> cars) {

}
cleancars(collections. Emptylist());

youd get the following error:
cleancars (java. Util. List<car>)cannot be applied to
(java. Util. List<java. Lang. Object>)

to fix it youd have to provide an explicit type argument like the one we showed
previously.
In java 8 the target type includes arguments to a method, so you dont need to pro-
vide an explicit generic argument:
list<car> cleancars = dirtycars. Stream()
. Filter(car: : isclean)
. Collect(collectors. Tolist());

in this code, its exactly this enhancement that lets you write collectors. Tolist()
instead of collectors. <car>tolist().
Appendix b
miscellaneous
library updates
this appendix reviews the main additions to the java 8 library.

B.1 collections
the biggest update to the collections api is the introduction of streams, which we
discussed in chapters 4, 5, and 6. There are also other updates discussed in chap-
ter 9 and additional minor additions summarized in this appendix.

B.1.1 additional methods
the java api designers made the most out of default methods and added several new
methods to collection interfaces and classes. The new methods are listed in table b.1.
Table b.1 new methods added to collection classes and interfaces

class/interface new methods

map getordefault, foreach, compute, computeifabsent,
computeifpresent, merge, putifabsent, remove(key, value),
replace, replaceall, of, ofentries

iterable foreach, spliterator

iterator foreachremaining

collection removeif, stream, parallelstream

list replaceall, sort, of

bitset stream

set of

521
522 appendix b miscellaneous library updates

map
the map interface is the most updated interface, with support for several new conve-
nient methods. For example, the method getordefault can be used to replace an
existing idiom that checks whether a map contains a mapping for a given key. If not,
you can provide a default value to return instead. Previously you would do this:
map<string, integer> carinventory = new hashmap<>();
integer count = 0;
if(map. Containskey("aston martin")){
count = map. Get("aston martin");
}

you can now more simply do the following:

integer count = map. Getordefault("aston martin", 0);

note that this works only if theres no mapping. For example, if the key is explicitly
mapped to the value null, then no default value will be returned.
Another particularly useful method is computeifabsent, which we briefly men-
tioned in chapter 19 when explaining memoization. It lets you conveniently use the
caching pattern. Lets say that you need to fetch and process data from a different
website. In such a scenario, its useful to cache the data, so you dont have to execute
the (expensive) fetching operation multiple times:
public string getdata(string url){
string data = cache. Get(url); check if the data is
if(data == null){ already cached.
Data = getdata(url);
cache. Put(url, data);
if not, fetch the data and
} then cache it in the map
return data; for future use.
}

you can now write this code more concisely by using computeifabsent as follows:
public string getdata(string url){
return cache. Computeifabsent(url, this: : getdata);
}

a description of all other methods can be found in the official java api documenta-
tion (http: //docs. Oracle. Com/javase/8/docs/api/java/util/map. Html). Note that
concurrenthashmap was also updated with additional methods. We discuss them in
section b.2.
Collection
the removeif method can be used to remove all elements in a collection that match a
predicate. Note that this is different than the filter method included in the streams
api. The filter method in the streams api produces a new stream; it doesnt mutate
the current stream or source.
Collections 523

list
the replaceall method replaces each element in a list with the result of applying a
given operator to it. Its similar to the map method in a stream, but it mutates the ele-
ments of the list. In contrast, the map method produces new elements.
For example, the following code will print [2, 4, 6, 8, 10] because the list is mod-
ified in place:
list<integer> numbers = arrays. Aslist(1, 2, 3, 4, 5);
numbers. Replaceall(x -> x * 2); prints
system. Out. Println(numbers); [2, 4, 6, 8, 10]

b.1.2 the collections class
the collections class has been around for a long time to operate on or return col-
lections. It now includes additional methods to return unmodifiable, synchronized,
checked, and empty navigablemap and navigableset. In addition, it includes the
method checkedqueue, which returns a view of queue thats extended with dynamic
type checking.

B.1.3 comparator
the comparator interface now includes default and static methods. You used the
comparator. Comparing static method in chapter 3 to return a comparator object
given a function that extracts the sorting key.
New instance methods include the following:
 reversedreturns a comparator with the reverse ordering of the current
comparator.
 Thencomparingreturns a comparator that uses another comparator when
two objects are equal.
 Thencomparingint, thencomparingdouble, thencomparinglongwork like the
thencomparing method, but take a function specialized for primitive types
(respectively, tointfunction, todoublefunction, and tolongfunction).
New static methods include these:
 comparingint, comparingdouble, comparinglongwork like the comparing
method, but take a function specialized for primitive types (respectively toint-
function, todoublefunction, and tolongfunction).
 Naturalorderreturns a comparator object that imposes a natural order on
comparable objects.
 Nullsfirst, nullslastreturn a comparator object that considers null to be
less than non-null or greater than non-null.
 Reverseorderequivalent to naturalorder(). Reverse().
524 appendix b miscellaneous library updates

b.2 concurrency
java 8 brings several updates related to concurrency. The first is, of course, the intro-
duction of parallel streams, which we explore in chapter 7. Theres also the introduc-
tion of the completablefuture class, which you can learn about in chapter 16.
There are other noticeable updates. For example, the arrays class now supports
parallel operations. We discuss these operations in section b.3.
In this section, we look at updates in the java. Util. Concurrent. Atomic package,
which deals with atomic variables. In addition, we discuss updates to the concurrent-
hashmap class, which supports several new methods.

B.2.1 atomic
the java. Util. Concurrent. Atomic package offers several numeric classes, such as
atomicinteger and atomiclong that support atomic operation on single variables.
They were updated to support new methods:
 getandupdateatomically updates the current value with the results of apply-
ing the given function, returning the previous value.
 Updateandgetatomically updates the current value with the results of apply-
ing the given function, returning the updated value.
 Getandaccumulateatomically updates the current value with the results of
applying the given function to the current and given values, returning the pre-
vious value.
 Accumulateandgetatomically updates the current value with the results of
applying the given function to the current and given values, returning the
updated value.
Heres how to atomically set the minimum between an observed value of 10 and an
existing atomic integer:
int min = atomicinteger. Accumulateandget(10, integer: : min);

adders and accumulators
the java api recommends using the new classes longadder, longaccumulator, double-
adder, and doubleaccumulator instead of the atomic classes equivalent when multiple
threads update frequently but read less frequently (for example, in the context of statis-
tics). These classes are designed to grow dynamically to reduce thread contention.
The classes longadder and doubleadder support operations for additions, whereas
longaccumulator and doubleaccumulator are given a function to combine values. For
example, to calculate the sum of several values, you can use a longadder as follows.

Listing b.1 longadder to calculate the sum of values
get the longadder adder = new longadder();
sum at the initial sum value
adder. Add(10); is set to 0 with the
some // ... Do some addition in
point. Default constructor.
Long sum = adder. Sum(); several different threads.
Concurrency 525

or you can use a longaccumulator as follows.

Listing b.2 longaccumulator to calculate the sum of values

longaccumulator acc = new longaccumulator(long: : sum, 0);
accumulate values in
acc. Accumulate(10);
several different threads.
// ...
Get the result
long result = acc. Get();
at some point.

B.2.2 concurrenthashmap
the concurrenthashmap class was introduced to provide a more modern hashmap, which
is concurrent friendly. Concurrenthashmap allows concurrent add and updates that lock
only certain parts of the internal data structure. Thus, read and write operations have
improved performance compared to the synchronized hashtable alternative.
Performance
concurrenthashmaps internal structure was updated to improve performance. Entries
of a map are typically stored in buckets accessed by the generated hashcode of the key.
But if many keys return the same hashcode, performance will deteriorate because
buckets are implemented as lists with o(n) retrieval. In java 8, when the buckets
become too big, theyre dynamically replaced with sorted trees, which have o(log(n))
retrieval. Note that this is possible only when the keys are comparable (for example,
string or number classes).
Stream-like operations
concurrenthashmap supports three new kinds of operations reminiscent of what you
saw with streams:
 foreachperforms a given action for each (key, value)
 reducecombines all (key, value) given a reduction function into a result
 searchapplies a function on each (key, value) until the function produces a
non-null result
each kind of operation supports four forms, accepting functions with keys, values,
map. Entry, and (key, value) arguments:
 operates with keys and values (foreach, reduce, search)
 operates with keys (foreachkey, reducekey, searchkey)
 operates with values (foreachvalue, reducevalue, searchvalues)
 operates with map. Entry objects (foreachentry, reduceentries, searchentries)
note that these operations dont lock the state of the concurrenthashmap. They oper-
ate on the elements as they go along. The functions supplied to these operations
shouldnt depend on any ordering or on any other objects or values that may change
while computation is in progress.
In addition, you need to specify a parallelism threshold for all these operations.
The operations will execute sequentially if the current map size is estimated to be less
526 appendix b miscellaneous library updates

than the given threshold. Using a value of 1 enables maximal parallelism using the
common thread pool. Using a value of long. Max_value runs the operation on a sin-
gle thread.
In this example we use the method reducevalues to find the maximum value in
the map:
concurrenthashmap<string, integer> map = new concurrenthashmap<>();
optional<integer> maxvalue =
optional. Of(map. Reducevalues(1, integer: : max));

note that there are primitive specializations for int, long, and double for each reduce
operation (for example, reducevaluestoint, reducekeystolong, and so on).
Counting
the concurrenthashmap class provides a new method called mappingcount, which
returns the number of mappings in the map as a long. It should be used instead of the
method size, which returns an int. This is because the number of mappings may not
fit in an int.
Set views
the concurrenthashmap class provides a new method called keyset that returns a
view of the concurrenthashmap as a set (changes to the map are reflected in the set
and vice versa). You can also create a set backed by a concurrenthashmap using the
new static method newkeyset.

B.3 arrays
the arrays class provides various static methods to manipulate arrays. It now includes
four new methods (which have primitive specialized overloaded variants).

B.3.1 using parallelsort
the parallelsort method sorts the specified array in parallel, using a natural order,
or using an extra comparator for an array of objects.

B.3.2 using setall and parallelsetall
the setall and parallelsetall methods set all elements of the specified array,
respectively sequentially or in parallel, using the provided function to compute each
element. The function receives the element index and returns a value for that index.
Because parallelsetall is executed in parallel, the function must be side-effect free,
as explained in chapters 7 and 18.
As an example, you can use the method setall to produce an array with the values
0, 2, 4, 6, ... :
int[] evennumbers = new int[10];
arrays. Setall(evennumbers, i -> i * 2);
number and math 527

b.3.3 using parallelprefix
the parallelprefix method cumulates, in parallel, each element of the given array,
using the supplied binary operator. In the next listing you produce the values 1, 2, 3,
4, 5, 6, 7, ....

Listing b.3 parallelprefix cumulates in parallel elements of an array

int[] ones = new int[10];
arrays. Fill(ones, 1); ones is now
arrays. Parallelprefix(ones, (a, b) -> a + b); [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

b.4 number and math
the java 8 api enhances the number and math classes with new methods.

B.4.1 number
the new methods of the number class are as follows:
 the short, integer, long, float, and double classes include the sum, min, and
max static methods. You saw these methods in conjunction with the reduce
operation in chapter 5.
 The integer and long classes include the methods compareunsigned, divide-
unsigned, remainderunsigned, and tounsignedstring to work with unsigned
values.
 The integer and long classes also respectively include the static methods
parseunsignedint and parseunsignedlong, to parse strings as an unsigned int
or long.
 The byte and short classes include the methods tounsignedint and tounsigned-
long, to convert the argument to an int or long by an unsigned conversion. Sim-
ilarly, the integer class now includes the static method tounsignedlong.
 The double and float classes include the static method isfinite, to check
whether the argument is a finite floating-point value.
 The boolean class now includes the static methods logicaland, logicalor, and
logicalxor, to apply the and, or, and xor operations between two booleans.
 The biginteger class includes the methods bytevalueexact, shortvalueexact,
intvalueexact, and longvalueexact, to convert this biginteger to the respec-
tive primitive type. But it throws an arithmetic exception if theres a loss of
information during the conversion.

B.4.2 math
the math class includes new methods that throw an arithmetic exception if the result
of the operation overflows. These methods consist of addexact, subtractexact,
multiplyexact, incrementexact, decrementexact, and negateexact with int and long
arguments. In addition, theres a static tointexact method to convert a long value to
an int. Other additions include the static methods floormod, floordiv, and nextdown.
528 appendix b miscellaneous library updates

b.5 files
noticeable additions to the files class let you produce a stream from files. We men-
tion the new static method files. Lines in chapter 5; it lets you read a file lazily as a
stream. Other useful static methods that return a stream include the following:
 files. Listproduces a stream<path> consisting of entries in a given direc-
tory. The listing isnt recursive. Because the stream is consumed lazily, its a use-
ful method for processing potentially very large directories.
 Files. Walkjust like files. List, it produces a stream<path> consisting of
entries in a given directory. But the listing is recursive and the depth level can
be configured. Note that the traversal is performed depth-first.
 Files. Findproduces a stream<path> from recursively traversing a directory
to find entries that match a given predicate.

B.6 reflection
we discussed several changes to the annotation mechanism in java 8 in appendix a.
The reflection api was updated to support these changes.
Another addition to the reflection api is that information about parameters of
methods such as names and modifiers can now be accessed with the help of the new
java. Lang. Reflect. Parameter class, which is referenced in the new java. Lang
. Reflect. Executable class that serves as a shared superclass for the common func-
tionality of method and constructor.

B.7 string
the string class now includes a convenient static method called join toas you may
guessjoin strings with a delimiter! You can use it as follows:

string authors = string. Join(", ", "raoul", "mario", "alan"); raoul, mario,
system. Out. Println(authors); alan
appendix c
performing multiple
operations in parallel
on a stream
one of the biggest limitations of a java 8 stream is that you can operate on it only
once and get only one result while processing it. Indeed, if you try to traverse a
stream for a second time, the only thing you can achieve is an exception like this:
java. Lang. Illegalstateexception: stream has already been operated upon or closed

despite this, there are situations where youd like to get several results when process-
ing a single stream. For instance, you may want to parse a log file in a stream, as we
did in section 5.7.3, but gather multiple statistics in a single step. Or, keeping with the
menu data model used to explain streams features in chapters 4, 5, and 6, you may
want to retrieve different information while traversing the stream of dishes.
In other words, youd like to push a stream through more than one lambda on a
single pass, and to do this you need a type of fork method and to apply different
functions to each forked stream. Even better, it would be great if you could per-
form those operations in parallel, using different threads to calculate the different
required results.
Unfortunately, these features arent currently available on the stream implementa-
tion provided in java 8, but in this appendix well show you a way to use a spliterator
and in particular its late-binding capacity, together with blockingqueues and futures,
to implement this useful feature and make it available with a convenient api.1

1
the implementation presented in the rest of this appendix is based on the solution posted by paul sandoz
in the email he sent to the lambda-dev mailing list: http: //mail. Openjdk. Java. Net/pipermail/lambda-dev/
2013-november/011516. Html.

529
530 appendix c performing multiple operations in parallel on a stream

c.1 forking a stream
the first thing necessary to execute multiple operations in parallel on a stream is to
create a streamforker that wraps the original stream, on which you can define the dif-
ferent operations you want to perform. Take a look at the following listing.

Listing c.1 defining a streamforker to execute multiple operations on a stream

public class streamforker<t> {

private final stream<t> stream;
private final map<object, function<stream<t>, ? >> forks =
new hashmap<>();

public streamforker(stream<t> stream) {
this. Stream = stream;
}

public streamforker<t> fork(object key, function<stream<t>, ? > f) {
forks. Put(key, f);
return this; index the function
return this to fluently to be applied on the
} invoke the fork method stream with a key.
Public results getresults() { multiple times.
// to be implemented
}
}

here the fork method accepts two arguments:
 a function, which transforms the stream into a result of any type representing
one of these operations
 a key, which will allow you to retrieve the result of that operation and accumu-
lates these key/function pairs in an internal map
the fork method returns the streamforker itself; therefore, you can build a pipeline by
forking several operations. Figure c.1 shows the main ideas behind the streamforker.
Here the user defines three operations to be performed on a stream indexed by
three keys. The streamforker then traverses the original stream and forks it into
three other streams. At this point the three operations can be applied in parallel on
the forked streams, and the results of these function applications, indexed with their
corresponding keys, are used to populate the resulting map.
The execution of all the operations added through the fork method is triggered
by the invocation of the method getresults, which returns an implementation of the
results interface defined as follows:
public static interface results {
public <r> r get(object key);
}

this interface has only one method to which you can pass one of the key objects used
in one of the fork methods, and that method returns the result of the operation cor-
responding to that key.
Forking a stream 531

fork(k1, 1) fork(k2, 2) fork(k3, 3)

streamforker
stream fork

fork1 fork2 fork3

parallel
computation 1 apply 2 apply 3 apply

result1 result2 result3

k1 result1

results k2 result2
k3 result3

figure c.1 the streamforker in action

c.1.1 implementing the results interface with the
forkingstreamconsumer
the getresults method can be implemented as follows:

public results getresults() {
forkingstreamconsumer<t> consumer = build();
try {
stream. Sequential(). Foreach(consumer);
} finally {
consumer. Finish();
}
return consumer;
}

the forkingstreamconsumer implements both the results interface defined previ-
ously and the consumer interface. As youll see when we analyze its implementation in
more detail, its main task is to consume all the elements in the stream and multiplex
them to a number of blockingqueues equal to the number of operations submitted
via the fork method. Note that it is ensured that the stream is sequential, because if
the method foreach were performed on a parallel stream, its elements could be
pushed to the queues out of order. The finish method adds special elements to those
queues to signal that there are no more items to be processed. The build method
used to create the forkingstreamconsumer is shown in the next listing.
532 appendix c performing multiple operations in parallel on a stream

listing c.2 the build method used to create forkingstreamconsumer

private forkingstreamconsumer<t> build() {
create a list of queues, with
list<blockingqueue<t>> queues = new arraylist<>();
a queue for each operation.
Map<object, future<? >> actions =
map the futures that will
forks. Entryset(). Stream(). Reduce( contain the results of the
new hashmap<object, future<? >>(), operations with the keys used
(map, e) -> { to identify those operations.
Map. Put(e. Getkey(),
getoperationresult(queues, e. Getvalue()));
return map;
},
(m1, m2) -> {
m1. Putall(m2);
return m1;
});

return new forkingstreamconsumer<>(queues, actions);
}

in listing c.2, you first create the list of blockingqueues mentioned previously. Then
you create a map, having as keys the same keys used to identify the different operations
to be executed on the stream, and having as values the futures that will contain the
corresponding results of these operations. The list of blockingqueues and the map
of futures are then passed to the constructor of the forkingstreamconsumer. Each
future is created with this getoperationresult method, as shown in the next listing.

Listing c.3 futures created with the getoperationresult method

private future<? > getoperationresult(list<blockingqueue<t>> queues, create a
function<stream<t>, ? > f) { queue and
create a blockingqueue<t> queue = new linkedblockingqueue<>(); add it to the
spliterator queues. Add(queue); list of queues.
Traversing the spliterator<t> spliterator = new blockingqueuespliterator<>(queue);
elements in stream<t> source = streamsupport. Stream(spliterator, false);
that queue. Create a stream
return completablefuture. Supplyasync( () -> f. Apply(source) ); having that
} spliterator as
create a future calculating asynchronously the
application of the given function on that stream. Source.

The method getoperationresult creates a new blockingqueue and adds it to the
list of queues. This queue is passed to a new blockingqueuespliterator, which is a
late-binding spliterator, reading the item to be traversed from the queue; well
examine how its made shortly.
You then create a sequential stream traversing this spliterator, and finally you
create a future to calculate the result of applying the function representing one of
the operations you want to perform on this stream. This future is created using a
static factory method of the completablefuture class that implements the future
interface. This is another new class introduced in java 8, and we investigated it in
detail in chapter 16.
Forking a stream 533

c.1.2 developing the forkingstreamconsumer and the
blockingqueuespliterator
the last two outstanding parts you need to develop are the forkingstreamconsumer
and blockingqueuespliterator classes we introduced previously. The first one can
be implemented as follows.

Listing c.4 a forkingstreamconsumer to add stream elements to multiple queues

static class forkingstreamconsumer<t> implements consumer<t>, results {
static final object end_of_stream = new object();

private final list<blockingqueue<t>> queues;
private final map<object, future<? >> actions;

forkingstreamconsumer(list<blockingqueue<t>> queues,
map<object, future<? >> actions) {
this. Queues = queues;
this. Actions = actions;
}
propagates the traversed
@override element of the stream to
public void accept(t t) { all the queues
queues. Foreach(q -> q. Add(t));
}
adds one last element to
void finish() { the queue to signal that
accept((t) end_of_stream); the stream is finished
}

@override returns the result
public <r> r get(object key) { of the operation
try { indexed by the
return ((future<r>) actions. Get(key)). Get(); given key and waits
} catch (exception e) { for the completion
throw new runtimeexception(e); of the future
} calculating it
}
}

this class implements both the consumer and results interfaces and holds a refer-
ence to the list of blockingqueues and to the map of futures executing the different
operations on the stream.
The consumer interface requires an implementation for the method accept. Here,
every time forkingstreamconsumer accepts an element of the stream, it adds that ele-
ment to all the blockingqueues. Also, after all the elements of the original stream
have been added to all queues, the finish method causes one last item to be added to
all of them. This item, when met by blockingqueuespliterators, will make the
queues understand that there are no more elements to be processed.
The results interface requires an implementation for the get method. Here, it
retrieves the future thats indexed in the map with the argument key and unwraps its
result or waits until a result is available.
534 appendix c performing multiple operations in parallel on a stream

finally, there will be a blockingqueuespliterator for each operation to be per-
formed on the stream. Each blockingqueuespliterator will have a reference to one
of the blockingqueues populated by the forkingstreamconsumer, and it can be
implemented as shown in the following listing.

Listing c.5 a spliterator reading the elements it traverses from a blockingqueue

class blockingqueuespliterator<t> implements spliterator<t> {
private final blockingqueue<t> q;

blockingqueuespliterator(blockingqueue<t> q) {
this. Q = q;
}

@override
public boolean tryadvance(consumer<? Super t> action) {
t t;
while (true) {
try {
t = q. Take();
break;
} catch (interruptedexception e) { }
}

if (t ! = forkingstreamconsumer. End_of_stream) {
action. Accept(t);
return true;
}

return false;
}

@override
public spliterator<t> trysplit() {
return null;
}

@override
public long estimatesize() {
return 0;
}

@override
public int characteristics() {
return 0;
}
}

in this listing a spliterator is implemented, not to define the policy of how to split a
stream but only to use its late-binding capability. For this reason the trysplit method
is unimplemented.
Also, its impossible to return any meaningful value from the estimatedsize
method because you cant foresee how many elements can be still taken from the
queue. Further, because youre not attempting any split, this estimation will be useless.
Forking a stream 535

this implementation doesnt have any of the spliterator characteristics we listed in
table 7.2, so the characteristic method returns 0.
The only method implemented here is tryadvance, which waits to take from its
blockingqueue the elements of the original stream added to it by the forkingstream-
consumer. It sends those elements to a consumer that (based on how this spliterator
was created in the getoperationresult method) is the source of a further stream (on
which the corresponding function, passed to one of the fork method invocations, has
to be applied). The tryadvance method returns true, to notify its invoker that there
are other elements to be consumed, until it finds on the queue the special object
added by forkingstreamconsumer to signal that there are no more elements to be
taken from the queue. Figure c.2 shows an overview of the streamforker and its
building blocks.

Streamforker forkstreamconsumer
q3
forks
accept q2
key3 f3 stream m
q1
key2 f2
key1 f1

q1 q2 q3

take take take
apply apply apply
f1 s1 f2 s2 f3 s3

blockingqueuespliterator1 blockingqueuespliterator2 blockingqueuespliterator3

figure c.2 the streamforker building blocks

in the figure, the streamforker in the upper left has a map, where each operation to
be performed on the stream, defined by a function, is indexed by a key. The forking-
streamconsumer on the right holds a queue for each of these operations and con-
sumes all the elements in the original stream, multiplexing them to all the queues.
At the bottom of the figure, each queue has a blockingqueuespliterator pulling
its items and acting as a source for a different stream. Finally, each of these streams,
forked by the original one, is passed as argument to one of the functions, thus execut-
ing one of the operations to be performed. You now have all the components of your
streamforker, so its ready to use.
536 appendix c performing multiple operations in parallel on a stream

c.1.3 putting the streamforker to work
lets put the streamforker to work on the menu data model that we defined in chap-
ter 4, by forking the original stream of dishes to perform four different operations in
parallel on it, as shown in the next listing. In particular, you want to generate a
comma-separated list of the names of all available dishes, calculate the total calories of
the menu, find the dish with the most calories, and group all dishes by their type.

Listing c.6 putting the streamforker to work

stream<dish> menustream = menu. Stream();

streamforker. Results results = new streamforker<dish>(menustream)
. Fork("shortmenu", s -> s. Map(dish: : getname)
. Collect(joining(", ")))
. Fork("totalcalories", s -> s. Maptoint(dish: : getcalories). Sum())
. Fork("mostcaloricdish", s -> s. Collect(reducing(
(d1, d2) -> d1. Getcalories() > d2. Getcalories() ? D1 : d2))
. Get())
. Fork("dishesbytype", s -> s. Collect(groupingby(dish: : gettype)))
. Getresults();

string shortmenu = results. Get("shortmenu");
int totalcalories = results. Get("totalcalories");
dish mostcaloricdish = results. Get("mostcaloricdish");
map<dish. Type, list<dish>> dishesbytype = results. Get("dishesbytype");

system. Out. Println("short menu: " + shortmenu);
system. Out. Println("total calories: " + totalcalories);
system. Out. Println("most caloric dish: " + mostcaloricdish);
system. Out. Println("dishes by type: " + dishesbytype);

the streamforker provides a convenient, fluent api to fork a stream and assign a dif-
ferent operation to each forked stream. These operations are expressed in terms of
functions applied on the stream and can be identified by any arbitrary object; in this
case weve chosen to use strings. When you have no more forks to add, you can
invoke getresults on the streamforker to trigger the execution of all the defined
operations and obtain streamforker. Results. Because these operations are inter-
nally performed asynchronously, the getresults method returns immediately, with-
out waiting for all the results to be available.
You can obtain the result of a specific operation by passing the key used to identify
it to the streamforker. Results interface. If in the meantime the computation of that
operation completes, the get method will return the corresponding result; otherwise,
it will block until such a result isnt available.
As expected, this piece of code generates the following output:
short menu: pork, beef, chicken, french fries, rice, season fruit, pizza,
prawns, salmon
total calories: 4300
most caloric dish: pork
dishes by type: {other=[french fries, rice, season fruit, pizza], meat=[pork,
beef, chicken], fish=[prawns, salmon]}
performance considerations 537

c.2 performance considerations
for performance reasons you shouldnt take for granted that this approach is more
efficient than traversing the stream several times. The overhead caused by the use of
the blocking queues can easily outweigh the advantages of executing the different
operations in parallel when the stream is made of data thats all in memory.
Conversely, accessing the stream only once could be a winning choice when this
involves some expensive i/o operations, such as when the source of the stream is a
huge file; so (as usual) the only meaningful rule when optimizing the performance of
your application is to just measure it!
This example demonstrates how it can be possible to execute multiple operations
on the same stream in one shot. More importantly, we believe this proves that even
when a specific feature isnt provided by the native java api, the flexibility of lambda
expressions and a bit of creativity in reusing and combining whats already available
can let you implement the missing feature on your own.
Appendix d
lambdas and
jvm bytecode
you may wonder how the java compiler implements lambda expressions and how
the java virtual machine (jvm) deals with it. If you think lambda expressions can
simply be translated to anonymous classes, you should read on. This appendix
briefly discusses how lambda expressions are compiled, by examining the gener-
ated class files.

D.1 anonymous classes
we showed in chapter 2 that anonymous classes can be used to declare and instan-
tiate a class at the same time. As a result, just like lambda expressions, they can be
used to provide the implementation for a functional interface.
Because a lambda expression provides the implementation for the abstract
method of a functional interface, it would seem straightforward to ask the java
compiler to translate a lambda expression into an anonymous class during the
compilation process. But anonymous classes have some undesirable characteristics
that impact the performance of applications:
 the compiler generates a new class file for each anonymous class. The filename usu-
ally looks like classname$1, where classname is the name of the class in
which the anonymous class appears, followed by a dollar sign and a number.
The generation of many class files is undesirable, because each class file
needs to be loaded and verified before being used, which impacts the startup
performance of the application. If lambdas were translated to anonymous
classes, youd have one new class file for each lambda.
 Each new anonymous class introduces a new subtype for a class or interface. If you
had a hundred different lambdas for expressing a comparator, that would

538
bytecode generation 539

mean a hundred different subtypes of comparator. In certain situations, this
can make it harder to improve runtime performance by the jvm.

D.2 bytecode generation
a java source file is compiled to java bytecode by the java compiler. The jvm can then
execute the generated bytecode and run the application. Anonymous classes and
lambda expressions use different bytecode instructions when theyre compiled. You
can inspect the bytecode and constant pool of any class file using the command
javap -c -v classname

lets try to implement an instance of the function interface using the old java 7 syn-
tax, as an anonymous inner class, as shown in the following listing.

Listing d.1 a function implemented as an anonymous inner class

import java. Util. Function. Function;
public class innerclass {
function<object, string> f = new function<object, string>() {
@override
public string apply(object obj) {
return obj. Tostring();
}
};
}

doing this, the corresponding generated bytecode for the function created as an
anonymous inner class will be something along the lines of this:
0: aload_0
1: invokespecial #1 // method java/lang/object. "<init>": ()v
4: aload_0
5: new #2 // class innerclass$1
8: dup
9: aload_0
10: invokespecial #3 // method innerclass$1. "<init>": (linnerclass; )v
13: putfield #4 // field f: ljava/util/function/function;
16: return

this code shows the following:
 an object of type innerclass$1 is instantiated using the byte code operation
new. A reference to the newly created object is pushed on the stack at the
same time.
 The operation dup duplicates that reference on the stack.
 This value then gets consumed by the instruction invokespecial, which initial-
izes the object.
 The top of the stack now still contains a reference to the object, which is stored
in the f1 field of the lambdabytecode class using the putfield instruction.
540 appendix d lambdas and jvm bytecode

innerclass$1 is the name generated by the compiler for the anonymous class. If you
want to reassure yourself, you can inspect the innerclass$1 class file as well, and
youll find the code for the implementation of the function interface:
class innerclass$1 implements
java. Util. Function. Function<java. Lang. Object, java. Lang. String> {
final innerclass this$0;
public java. Lang. String apply(java. Lang. Object);
code:
0: aload_1
1: invokevirtual #3 //method
java/lang/object. Tostring: ()ljava/lang/string;
4: areturn
}

d.3 invokedynamic to the rescue
now lets try to do the same using the new java 8 syntax as a lambda expression.
Inspect the generated class file of the code in the following listing.

Listing d.2 a function implemented with a lambda expression
import java. Util. Function. Function;
public class lambda {
function<object, string> f = obj -> obj. Tostring();
}

youll find the following bytecode instructions:
0: aload_0
1: invokespecial #1 // method java/lang/object. "<init>": ()v
4: aload_0
5: invokedynamic #2, 0 // invokedynamic
#0: apply: ()ljava/util/function/function;
10: putfield #3 // field f: ljava/util/function/function;
13: return

we explained the drawbacks in translating a lambda expression in an anonymous
inner class, and indeed you can see that the result is very different. The creation of an
extra class has been replaced with an invokedynamic instruction.

The invokedynamic instruction
the bytecode instruction invokedynamic was introduced in jdk7 to support dynam-
ically typed languages on the jvm. Invokedynamic adds a further level of indirection
when invoking a method, to let some logic dependent on the specific dynamic lan-
guage determine the call target. The typical use for this instruction is something like
the following:
def add(a, b) { a + b }

here the types of a and b arent known at compile time and can change from time to
time. For this reason, when the jvm executes an invokedynamic for the first time,
code-generation strategies 541

it consults a bootstrap method, implementing the language-dependent logic that
determines the actual method to be called. The bootstrap method returns a linked
call site. Theres a good chance that if the add method is called with two ints, the
subsequent call will also be with two ints. As a result, its not necessary to redis-
cover the method to be called at each invocation. The call site itself can contain the
logic defining under which conditions it needs to be relinked.

In listing d.2, the features of the invokedynamic instruction have been used for a
slightly different purpose than the one for which they were originally introduced. In
fact, here it used to delay the strategy used to translate lambda expressions in byte-
code until runtime. In other words, using invokedynamic in this way allows deferring
code generation for implementing the lambda expression until runtime. This design
choice has positive consequences:
 the strategy used to translate the lambda expression body to bytecode becomes
a pure implementation detail. It could also be changed dynamically, or opti-
mized and modified in future jvm implementations, preserving the bytecodes
backward compatibility.
 Theres no overhead, such as additional fields or static initializer, if the lambda
is never used.
 For stateless (noncapturing) lambdas its possible to create one instance of the
lambda object, cache it, and always return the same. This is a common use case,
and people were used to doing this explicitly before java 8; for example, declar-
ing a specific comparator instance in a static final variable.
 Theres no additional performance cost because this translation has to be per-
formed, and its result linked, only when the lambda is invoked for the first time.
All subsequent invocations can skip this slow path and call the formerly linked
implementation.

D.4 code-generation strategies
a lambda expression is translated into bytecode by putting its body into one of a static
method created at runtime. A stateless lambda, one that captures no state from its
enclosing scope, like the one we defined in listing d.2, is the simplest type of lambda
to be translated. In this case the compiler can generate a method having the same sig-
nature of the lambda expression, so the result of this translation process can be logi-
cally seen as follows:
public class lambda {
function<object, string> f = [dynamic invocation of lambda$1]

static string lambda$1(object obj) {
return obj. Tostring();
}
}
542 appendix d lambdas and jvm bytecode

the case of a lambda expression capturing final (or effectively final) local variables or
fields, as in the following example, is a bit more complex:
public class lambda {
string header = "this is a ";
function<object, string> f = obj -> header + obj. Tostring();
}

in this case the signature of the generated method cant be the same as the lambda
expression, because its necessary to add extra arguments to carry the additional state
of the enclosed context. The simplest solution to achieve this is to prepend the argu-
ments of the lambda expression with an additional argument for each of the captured
variables, so the method generated to implement the former lambda expression will
be something like this:
public class lambda {
string header = "this is a ";
function<object, string> f = [dynamic invocation of lambda$1]

static string lambda$1(string header, object obj) {
return obj -> header + obj. Tostring();
}
}